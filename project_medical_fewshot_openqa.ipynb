{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G0qZfMf4Yreh"
   },
   "source": [
    "# Medical Few-shot OpenQA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dKB9zXRBYrel"
   },
   "source": [
    "## Set-up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sFM54iO7Yren"
   },
   "source": [
    "### General set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "hL9AAtTzYren"
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "from contextlib import nullcontext\n",
    "from collections import namedtuple\n",
    "from datasets import load_dataset\n",
    "import json\n",
    "import numpy as np\n",
    "import random\n",
    "import re \n",
    "import string\n",
    "import torch\n",
    "from typing import List\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "MIvsYoIpYreo"
   },
   "outputs": [],
   "source": [
    "seed = 1\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-mp1C-oyYreq"
   },
   "source": [
    "### Language model set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "TKQEIYGDYrer"
   },
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "transformers.logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "axUPfnNmYrer"
   },
   "source": [
    "### ColBERT set-up\n",
    "The following will clone the ColBERTv2 repository for use in this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L5wP933JYrer",
    "outputId": "743b9a8c-b982-4db8-af18-08d43dee9e17"
   },
   "outputs": [],
   "source": [
    "# Clone the repo\n",
    "# !git clone -b cpu_inference https://github.com/stanford-futuredata/ColBERT.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "6LWz4f-3Yres"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, 'ColBERT/')\n",
    "\n",
    "from colbert.infra import Run, RunConfig, ColBERTConfig\n",
    "from colbert.data import Collection\n",
    "from colbert.searcher import Searcher\n",
    "from utility.utils.dpr import has_answer, DPR_normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U5ganiuNYres"
   },
   "source": [
    "## Language models\n",
    "\n",
    "In few-shot OpenQA, the language model (LM) must read in a prompt and answer the question posed somewhere in the prompt. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fca8-RXjYres"
   },
   "source": [
    "### Answerhood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "U6KepnjTYret"
   },
   "outputs": [],
   "source": [
    "def _find_generated_answer(tokens, newline=\"\\n\" ): \n",
    "    \"\"\"Our LMs tend to insert initial newline characters before\n",
    "    they begin generating text. This function ensures that we \n",
    "    properly capture the true first line as the answer while\n",
    "    also ensuring that token probabilities are aligned.\"\"\"        \n",
    "    answer_token_indices = []\n",
    "    char_seen = False            \n",
    "    for i, tok in enumerate(tokens):\n",
    "        # This is the main condition: a newline that isn't an initial\n",
    "        # string of newlines:\n",
    "        if tok == newline and char_seen:\n",
    "            break\n",
    "        # Keep the initial newlines for consistency:\n",
    "        elif tok == newline and not char_seen:\n",
    "            answer_token_indices.append(i)\n",
    "        # Proper tokens:\n",
    "        elif tok != newline:\n",
    "            char_seen = True\n",
    "            answer_token_indices.append(i)\n",
    "    return answer_token_indices "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "klW12GkAYret"
   },
   "source": [
    "### Eleuther models from Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "1-FkbTaUYret"
   },
   "outputs": [],
   "source": [
    "# \"gpt-neo-125M\" \"gpt-neo-1.3B\" \"gpt-neo-2.7B\" \"gpt-j-6B\"\n",
    "eleuther_model_name = \"gpt-neo-125M\"\n",
    "\n",
    "eleuther_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    f\"EleutherAI/{eleuther_model_name}\", \n",
    "    padding_side=\"left\", \n",
    "    padding='longest', \n",
    "    truncation='longest_first', max_length=2000)\n",
    "eleuther_tokenizer.pad_token = eleuther_tokenizer.eos_token\n",
    "\n",
    "eleuther_model = AutoModelForCausalLM.from_pretrained(\n",
    "    f\"EleutherAI/{eleuther_model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "rEo9YKwNYret"
   },
   "outputs": [],
   "source": [
    "def run_eleuther(prompts, temperature=0.1, top_p=0.95, **generate_kwargs): \n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    prompts : iterable of str\n",
    "    temperature : float\n",
    "        It seems best to set it low for this task!\n",
    "    top_p : float\n",
    "       \n",
    "    For options for `generate_kwargs`, see:\n",
    "    \n",
    "    https://huggingface.co/docs/transformers/master/en/main_classes/text_generation#transformers.generation_utils.GenerationMixin.generate\n",
    "    \n",
    "    Options that are likely to be especially relevant include \n",
    "    `temperature`, `length_penalty`, and the parameters that\n",
    "    determine the decoding strategy. With `num_return_sequences > 1`,\n",
    "    the default parameters in this function do multinomial sampling.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    list of dicts\n",
    "    \n",
    "    {\"prompt\": str, \n",
    "     \"generated_text\": str, \"generated_tokens\": list of str, \"generated_probs\": list of float,\n",
    "     \"answer\": str, \"answer_tokens\": list of str, \"answer_probs\": list of float\n",
    "    }\n",
    "         \n",
    "    \"\"\"\n",
    "    prompt_ids = eleuther_tokenizer(\n",
    "        prompts, return_tensors=\"pt\", padding=True).input_ids\n",
    "        \n",
    "    with torch.inference_mode():\n",
    "        # Automatic mixed precision if possible.\n",
    "        with torch.cuda.amp.autocast() if torch.cuda.is_available() else nullcontext():\n",
    "            model_output = eleuther_model.generate(\n",
    "                prompt_ids,\n",
    "                temperature=temperature,\n",
    "                do_sample=True,\n",
    "                top_p=top_p,           \n",
    "                max_new_tokens=16,\n",
    "                num_return_sequences=1,                \n",
    "                pad_token_id=eleuther_tokenizer.eos_token_id, \n",
    "                return_dict_in_generate=True,\n",
    "                output_scores=True,\n",
    "                **generate_kwargs)\n",
    "        \n",
    "    # Converting output scores using the helpful recipe here:\n",
    "    # https://discuss.huggingface.co/t/generation-probabilities-how-to-compute-probabilities-of-output-scores-for-gpt2/3175\n",
    "    gen_ids = model_output.sequences[:, prompt_ids.shape[-1] :]\n",
    "    gen_probs = torch.stack(model_output.scores, dim=1).softmax(-1)\n",
    "    gen_probs = torch.gather(gen_probs, 2, gen_ids[:, :, None]).squeeze(-1)\n",
    "    \n",
    "    # Generated texts, including the prompts:\n",
    "    gen_texts = eleuther_tokenizer.batch_decode(\n",
    "        model_output.sequences, skip_special_tokens=True)\n",
    "    \n",
    "    data = []     \n",
    "    iterator = zip(prompts, gen_ids, gen_texts, gen_probs)    \n",
    "    for prompt, gen_id, gen_text, gen_prob in iterator:       \n",
    "        gen_tokens = eleuther_tokenizer.convert_ids_to_tokens(gen_id)\n",
    "        generated_text = gen_text[len(prompt): ]\n",
    "        gen_prob = [float(x) for x in gen_prob.numpy()] # float for JSON storage\n",
    "        ans_indices = _find_generated_answer(gen_tokens, newline=\"Ċ\")\n",
    "        answer_tokens = [gen_tokens[i] for i in ans_indices]\n",
    "        answer_probs = [gen_prob[i] for i in ans_indices]\n",
    "        answer = \"\".join(answer_tokens).replace(\"Ġ\", \" \").replace(\"Ċ\", \"\\n\")                                       \n",
    "        data.append({\n",
    "            \"prompt\": prompt,\n",
    "            \"generated_text\": generated_text,\n",
    "            \"generated_tokens\": gen_tokens,\n",
    "            \"generated_probs\": gen_prob,\n",
    "            \"generated_answer\": answer,\n",
    "            \"generated_answer_probs\": answer_probs,\n",
    "            \"generated_answer_tokens\": answer_tokens})                        \n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7fIfuTEGYreu",
    "outputId": "ec7c8ead-7980-4a4e-b6c1-10aef49bd625"
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# ## test run\n",
    "\n",
    "# eleuther_ex = run_eleuther([    \n",
    "#     \"What year was Stanford University founded?\", \n",
    "#     \"In which year did Stanford first enroll students?\"])\n",
    "\n",
    "# eleuther_ex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XGK3hCs9Yrev"
   },
   "source": [
    "## Dataset Loading\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### SQuAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0,
     "referenced_widgets": [
      "29084cffd32b495ba5c29910a483b41a",
      "cb331d5edf364e19af8f45fd3726a9f8",
      "dcb183bf0a85498d8bb2bd360c64255b",
      "83ab942f9ce14cd6b82da28b36dd1377",
      "19838b224425453988f4ea47c50c048e",
      "bdcd068db1ad4c55ba282295b91aaeec",
      "f61e1dd551d74455b834735b4aa816df",
      "5054cd44686b4f649724445b581ad979",
      "76b125c6b6cf426cbf3ad638e443cbd8",
      "eda083d3daeb4c59b66beb142f12063a",
      "adaf48a176ad439f89883761b3027232"
     ]
    },
    "id": "YQ_do58EYrev",
    "outputId": "39d8b916-1d33-412e-dbde-8e5cb3085951"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset squad (/home/zhanj289/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49c3df12874b4ff0bba68fd909b8bec2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "squad = load_dataset(\"squad\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VbCUz66GYrev"
   },
   "source": [
    "The following utility just reads a SQuAD split in as a list of `SquadExample` instances:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "B9-0hkxgYrew"
   },
   "outputs": [],
   "source": [
    "SquadExample = namedtuple(\"SquadExample\",  \"id title context question answers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "g2gt0dkeYrew"
   },
   "outputs": [],
   "source": [
    "def get_squad_split(squad, split=\"validation\"):\n",
    "    \"\"\"\n",
    "    Use `split='train'` for the train split.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    list of SquadExample named tuples with attributes\n",
    "    id, title, context, question, answers\n",
    "    \n",
    "    \"\"\"    \n",
    "    fields = squad[split].features\n",
    "    data = zip(*[squad[split][field] for field in fields])\n",
    "    return [SquadExample(eid, title, context, question, answers[\"text\"]) \n",
    "            for eid, title, context, question, answers in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Split Dev and Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = squad['validation'].features\n",
    "data = zip(*[squad['validation'][field] for field in fields])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "NatnOLsDYrew"
   },
   "outputs": [],
   "source": [
    "squad_dev = get_squad_split(squad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jb-ZrSzoYrew",
    "outputId": "4ac4149e-0f96-4c0f-e0ca-aed1d12c796c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SquadExample(id='56d602631c85041400946edb', title='Super_Bowl_50', context='CBS broadcast Super Bowl 50 in the U.S., and charged an average of $5 million for a 30-second commercial during the game. The Super Bowl 50 halftime show was headlined by the British rock group Coldplay with special guest performers Beyoncé and Bruno Mars, who headlined the Super Bowl XLVII and Super Bowl XLVIII halftime shows, respectively. It was the third-most watched U.S. broadcast ever.', question='Who were special guests for the Super Bowl halftime show?', answers=['Beyoncé and Bruno Mars', 'Beyoncé and Bruno Mars', 'Beyoncé and Bruno Mars'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squad_dev[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "rw3OGISgzTil"
   },
   "outputs": [],
   "source": [
    "dev_exs = sorted(squad_dev, key=lambda x: hash(x.id))[: 200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "YwwjQiF8zTim"
   },
   "outputs": [],
   "source": [
    "squad_train = get_squad_split(squad, \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "    num_rows: 87599\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squad['train']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BioASQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('./data/bioasq/squad.json', 'r') as f:\n",
    "#     squad_test = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick all factoid questions but ignore all else"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/bioasq/training10b.json', 'r') as f:\n",
    "    bioasq_json = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bioasq_json['questions'][0]['snippets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_dict = {}\n",
    "\n",
    "# for snip in bioasq_json['questions'][0]['snippets']:\n",
    "#     if snip['beginSection'] == 'abstract':\n",
    "#         for k in range(snip['offsetInBeginSection'], snip['offsetInEndSection']):\n",
    "#             text_dict[k] = snip['text'][k- snip['offsetInBeginSection']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recon_text = ''\n",
    "# for key in sorted(text_dict.keys()):\n",
    "#     recon_text += text_dict[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recon_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bioasq_json['questions'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we have 1252 factoid questions, 816 list questions, 1018 summary questions, 1148 yesno qquestions\n",
      "total is 4234\n"
     ]
    }
   ],
   "source": [
    "### Construct dataset\n",
    "count_factoid = 0\n",
    "count_list =0\n",
    "count_summary=0\n",
    "count_yesno =0\n",
    "\n",
    "bioasq_list= []\n",
    "\n",
    "for i in range(len(bioasq_json['questions'])):\n",
    "    \n",
    "    sample = bioasq_json['questions'][i]\n",
    "    \n",
    "    if sample['type'] == 'summary':\n",
    "            count_summary += 1\n",
    "    if sample['type'] == 'yesno':\n",
    "            count_yesno += 1\n",
    "    \n",
    "    if sample['type'] in ['factoid', 'list']:\n",
    "        \n",
    "    #  Context\n",
    "    ## flatten all the snippet, conccatenate and use as context\n",
    "        context = '' \n",
    "        for snip in [ele['text'].strip() for ele in sample['snippets']]:\n",
    "            snip += ' '\n",
    "            context += snip\n",
    "            \n",
    "        context = context.replace('\\n', ' ')\n",
    "        \n",
    "        ## limit the length of context\n",
    "        ### Max: 4096 (for eleuther model)\n",
    "        context = context[:1024]\n",
    "        \n",
    "        # question\n",
    "        question = sample['body']\n",
    "        question = question.replace('\\n', ' ')\n",
    "        \n",
    "        # answer:\n",
    "        ## deal with factoid question and list question differently\n",
    "        if sample['type'] == 'factoid':\n",
    "            answer = sample['exact_answer']\n",
    "            count_factoid += 1\n",
    "        \n",
    "        if sample['type'] == 'list':\n",
    "            answer = [x for y in sample['exact_answer'] for x in y]\n",
    "            count_list += 1\n",
    "        \n",
    "\n",
    "        # construct a QA pairs like SQUAD\n",
    "        bioasq_list.append({\n",
    "            'id': i,\n",
    "            'context': context,\n",
    "            'question': sample['body'],\n",
    "            'answers': answer,\n",
    "            'type': sample['type']\n",
    "        }) \n",
    "\n",
    "print(f'we have {count_factoid} factoid questions, {count_list} list questions, {count_summary} summary questions, {count_yesno} yesno qquestions')   \n",
    "\n",
    "print(f'total is {count_factoid +count_list+ count_summary +count_yesno}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2068"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bioasq_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "def get_bioasq_split(bioasq_list, random_state):\n",
    "    \"\"\"\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    list of example named tuples with attributes\n",
    "    id, title, context, question, answers\n",
    "    \n",
    "    \"\"\"\n",
    "    BioasqExample = namedtuple(\"BioasqExample\",  \"id context question answers\")\n",
    "    \n",
    "    bioasq_data = [BioasqExample(ele['id'], ele['context'], ele['question'], ele['answers']) for ele in bioasq_list]\n",
    "    \n",
    "    bioasq_train, _ = train_test_split(bioasq_data, test_size=0.9, random_state=random_state)\n",
    "\n",
    "    bioasq_dev, bioasq_test = train_test_split(_, test_size=0.8888, random_state=random_state)\n",
    "    \n",
    "    return bioasq_train, bioasq_dev, bioasq_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## split dev and test\n",
    "\n",
    "bioasq_train, bioasq_dev, bioasq_test = get_bioasq_split(bioasq_list, random_state=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206, 207, 1655 \n"
     ]
    }
   ],
   "source": [
    "print(f\"{len(bioasq_train)}, {len(bioasq_dev)}, {len(bioasq_test)} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## pick 10 just for sanity check\n",
    "dev_exs = bioasq_dev[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "829"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_exs[0].id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DZpXMk-0Yrew",
    "tags": []
   },
   "source": [
    "## Evaluation\n",
    "\n",
    "Our evaluation protocols are the standard ones for SQuAD and related tasks: exact match of the answer (EM) and token-level F1.\n",
    "\n",
    "We say further that the predicted answer is the first line of generated text after the prompt.\n",
    "\n",
    "The following evaluation code is taken from the [apple/ml-qrecc](https://github.com/apple/ml-qrecc/blob/main/utils/evaluate_qa.py) repository. It performs very basic string normalization before doing the core comparisons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "nHHntDSSYrew"
   },
   "outputs": [],
   "source": [
    "def normalize_answer(s: str) -> str:\n",
    "    \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n",
    "\n",
    "    def remove_articles(text):\n",
    "        regex = re.compile(r'\\b(a|an|the)\\b', re.UNICODE)\n",
    "        return re.sub(regex, ' ', text)\n",
    "\n",
    "    def white_space_fix(text):\n",
    "        return ' '.join(text.split())\n",
    "\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return ''.join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "\n",
    "def get_tokens(s: str) -> List[str]:\n",
    "    \"\"\"Normalize string and split string into tokens.\"\"\"\n",
    "    if not s:\n",
    "        return []\n",
    "    return normalize_answer(s).split()\n",
    "\n",
    "\n",
    "def compute_exact(a_gold: str, a_pred: str) -> int:\n",
    "    \"\"\"Compute the Exact Match score.\"\"\"\n",
    "    return int(normalize_answer(a_gold) == normalize_answer(a_pred))\n",
    "\n",
    "\n",
    "def compute_f1_from_tokens(gold_toks: List[str], pred_toks: List[str]) -> float:\n",
    "    \"\"\"Compute the F1 score from tokenized gold answer and prediction.\"\"\"\n",
    "    common = collections.Counter(gold_toks) & collections.Counter(pred_toks)\n",
    "    num_same = sum(common.values())\n",
    "\n",
    "    if len(gold_toks) == 0 or len(pred_toks) == 0:\n",
    "        # If either is no-answer, then F1 is 1 if they agree, 0 otherwise\n",
    "        f1= int(gold_toks == pred_toks)\n",
    "        precision = int(gold_toks == pred_toks)\n",
    "        recall = int(gold_toks == pred_toks)\n",
    "        \n",
    "     # if no token overlap at all, all metrics is 0\n",
    "    if num_same == 0: \n",
    "        f1= int(gold_toks == pred_toks)\n",
    "        precision = int(gold_toks == pred_toks)\n",
    "        recall = int(gold_toks == pred_toks)\n",
    "    \n",
    "    else:\n",
    "        precision = 1.0 * num_same / len(pred_toks)\n",
    "        recall = 1.0 * num_same / len(gold_toks)\n",
    "        f1 = (2 * precision * recall) / (precision + recall)\n",
    "    return f1, precision, recall\n",
    "\n",
    "\n",
    "def compute_f1(a_gold: str, a_pred: str) -> float:\n",
    "    \"\"\"Compute the F1 score.\"\"\"\n",
    "    gold_toks = get_tokens(a_gold)\n",
    "    pred_toks = get_tokens(a_pred)\n",
    "    return compute_f1_from_tokens(gold_toks, pred_toks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lJo6C7pgYrex"
   },
   "source": [
    "The following is our general evaluation function. We will make extensive use of it to evaluate different systems:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "bJHSxnA6Yrex"
   },
   "outputs": [],
   "source": [
    "def evaluate(examples, prompts, gens):\n",
    "    \"\"\"Generic evalution function.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    examples: iterable of `SquadExample` instances\n",
    "    prompts: list of str\n",
    "    preds: list of LM-generated texts to evaluate as answers\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    dict with keys \"em_per\", \"macro_f1\", \"examples\", where\n",
    "    each \"examples\" value is a dict\n",
    "    \n",
    "    \"\"\"        \n",
    "    results = []\n",
    "    for ex, prompt, gen in zip(examples, prompts, gens):\n",
    "        answers = ex.answers\n",
    "        pred = gen['generated_answer']\n",
    "        # The result is the highest EM from the available answer strings:\n",
    "        em = max([compute_exact(ans, pred) for ans in answers])\n",
    "        \n",
    "        # adding precision and recall\n",
    "        # print([compute_f1(ans, pred) for ans in answers])\n",
    "        f1 = max([compute_f1(ans, pred)[0] for ans in answers])\n",
    "        precision = max([compute_f1(ans, pred)[1] for ans in answers])\n",
    "        recall = max([compute_f1(ans, pred)[2] for ans in answers])\n",
    "        \n",
    "        gen.update({\n",
    "            \"id\": ex.id, \n",
    "            \"question\": ex.question, \n",
    "            \"prediction\": pred, \n",
    "            \"answers\": answers, \n",
    "            \"em\": em,\n",
    "            \"f1\": f1,\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall\n",
    "        })\n",
    "        results.append(gen)\n",
    "    data = {}        \n",
    "    data[\"macro_f1\"] = np.mean([d['f1'] for d in results])\n",
    "    data[\"macro_precision\"] = np.mean([d['precision'] for d in results])\n",
    "    data[\"macro_recall\"] = np.mean([d['recall'] for d in results])\n",
    "    data[\"em_per\"] = sum([d['em'] for d in results]) / len(results)\n",
    "    data[\"examples\"] = results\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fj170C1PYrex"
   },
   "source": [
    "Here is a highly simplified example to help make the logic behind `evaluate` clearer:    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0bgFXLK3Yrex",
    "outputId": "40c6678c-d600-4a2e-b192-84ce765be7d6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'macro_f1': 0.5,\n",
       " 'macro_precision': 0.3333333333333333,\n",
       " 'macro_recall': 1.0,\n",
       " 'em_per': 0.0,\n",
       " 'examples': [{'generated_answer': 'course on NLU',\n",
       "   'generated_text': 'NLU\\nWho am I?',\n",
       "   'id': '0',\n",
       "   'question': 'What is the course to take?',\n",
       "   'prediction': 'course on NLU',\n",
       "   'answers': ['NLU', 'CS224u'],\n",
       "   'em': 0,\n",
       "   'f1': 0.5,\n",
       "   'precision': 0.3333333333333333,\n",
       "   'recall': 1.0}]}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex = namedtuple(\"SquadExample\",  \"id title context question answers\")\n",
    "\n",
    "examples = [\n",
    "    ex(\"0\", \"CS224u\", \n",
    "       \"The course to take is NLU!\", \n",
    "       \"What is the course to take?\", \n",
    "       [\"NLU\", \"CS224u\"])]\n",
    "\n",
    "prompts = [\"Dear model, Please answer this question!\\n\\nQ: What is the course to take?\\n\\nA:\"]\n",
    "\n",
    "gens = [{\"generated_answer\": \"course on NLU\", \"generated_text\": \"NLU\\nWho am I?\"}]\n",
    "\n",
    "evaluate(examples, prompts, gens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MHZHve9NYrex"
   },
   "source": [
    "The bake-off uses `macro_f1` as the primary metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WfvVdsGrYre0"
   },
   "source": [
    "## ColBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "VCt6-Oe2zTio"
   },
   "outputs": [],
   "source": [
    "index_home = os.path.join(\"experiments\", \"notebook\", \"indexes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NFYxJPpuYre0"
   },
   "source": [
    "### ColBERT parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "5tnUU2UHYre0"
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(os.path.join(\"data\", \"openqa\", \"colbertv2.0.tar.gz\")):\n",
    "    !mkdir -p data/openqa\n",
    "    # ColBERTv2 checkpoint trained on MS MARCO Passage Ranking (388MB compressed)\n",
    "    !wget https://downloads.cs.stanford.edu/nlp/data/colbert/colbertv2/colbertv2.0.tar.gz -P data/openqa/\n",
    "    !tar -xvzf data/openqa/colbertv2.0.tar.gz -C data/openqa/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cjZ1hZJnYre0"
   },
   "source": [
    "If something went wrong with the above, you can just download the file https://downloads.cs.stanford.edu/nlp/data/colbert/colbertv2/colbertv2.0.tar.gz, unarchive it, and move the resulting `colbertv2.0` directory into the `data/openqa` directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QzRjL61eYre0"
   },
   "source": [
    "### ColBERT index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we use our created index for bioasq passages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "id": "XtEGC6MyYre0",
    "outputId": "c6553c0f-598f-4d37-c96b-0766aaef1dfe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Jun 05, 01:40:54] #> Loading collection...\n",
      "0M \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Loaded 2,068 passages'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_home = './experiments/bioasq/indexes'\n",
    "\n",
    "collection = os.path.join(index_home, \"bioasq.all.2bits\", \"bioasq_passage.tsv\")\n",
    "\n",
    "collection = Collection(path=collection)\n",
    "\n",
    "f'Loaded {len(collection):,} passages'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "BixmAYizYre0"
   },
   "outputs": [],
   "source": [
    "index_name = \"bioasq.all.2bits\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "83oO75Z1zTio"
   },
   "source": [
    "Now we create our `searcher`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N6wYpChzYre1",
    "outputId": "4436fe5f-55fa-43d0-ca4b-1d3a70c51d7e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Jun 05, 01:40:55] #> Loading collection...\n",
      "0M \n",
      "[Jun 05, 01:41:04] #> Building the emb2pid mapping..\n",
      "[Jun 05, 01:41:04] len(self.emb2pid) = 378124\n"
     ]
    }
   ],
   "source": [
    "with Run().context(RunConfig(experiment='bioasq')):\n",
    "    searcher = Searcher(index=index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2068"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(searcher.collection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w9n0_xwdYre1"
   },
   "source": [
    "### Retrieval evaluation\n",
    "\n",
    "For more rigorous evaluations of the retriever alone, we can use Sucess@`k` defined relative to the SQuAD passages and answers. We say that we have a \"success\" if a passage in the top `k` retrieved passages contains any of the answers substrings, and Sucess@`k` is the percentage of such success cases. This is very heuristic (perhaps the answer string happens to occur somewhere in a completely irrelevant passage), but it can still be good guidance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "id": "fsNayHzdYre1"
   },
   "outputs": [],
   "source": [
    "def success_at_k(examples, k=20):\n",
    "    scores = []\n",
    "    for ex in examples: \n",
    "        scores.append(evaluate_retrieval_example(ex, k=5))\n",
    "    return sum(scores) / len(scores)\n",
    "        \n",
    "    \n",
    "def evaluate_retrieval_example(ex, k=20):    \n",
    "    results = searcher.search(ex.question, k=k)\n",
    "    for passage_id, passage_rank, passage_score in zip(*results):\n",
    "        passage = searcher.collection[passage_id]\n",
    "        score = has_answer([DPR_normalize(ans) for ans in ex.answers], passage)\n",
    "        if score:\n",
    "            return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k2U4v58dYre1"
   },
   "source": [
    "Here is Sucess@20 for the SQuAD dev set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "J2oEmspeYre1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#> QueryTokenizer.tensorize(batch_text[0], batch_background[0], bsize) ==\n",
      "#> Input: . Which gene harbors the mutation T790M?, \t\t True, \t\t None\n",
      "#> Output IDs: torch.Size([32]), tensor([  101,     1,  2029,  4962,  6496,  2015,  1996, 16221,  1056,  2581,\n",
      "        21057,  2213,  1029,   102,   103,   103,   103,   103,   103,   103,\n",
      "          103,   103,   103,   103,   103,   103,   103,   103,   103,   103,\n",
      "          103,   103])\n",
      "#> Output Mask: torch.Size([32]), tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])\n",
      "\n",
      "0.7681159420289855\n",
      "CPU times: user 35.4 s, sys: 1.45 s, total: 36.9 s\n",
      "Wall time: 6.95 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if torch.cuda.is_available():\n",
    "    # This will take a few hours on a CPU:\n",
    "    print(success_at_k(bioasq_dev))\n",
    "else:\n",
    "    # This should be reasonably fast and yields the\n",
    "    # same kind of result:\n",
    "    print(success_at_k(bioasq_dev))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w2mx3Z4HYre2"
   },
   "source": [
    "### Few-shot OpenQA \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "HUuZ5l3gYre2"
   },
   "outputs": [],
   "source": [
    "def build_few_shot_open_qa_prompt(question, passage, train_exs, joiner=\"\\n\\n\"):\n",
    "    \"\"\"Few-shot OpenQA prompts.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    question : str\n",
    "    passage : str\n",
    "        Presumably something retrieved via search.\n",
    "    train_exs : iterable of SQuAD train examples\n",
    "        These can be obtained via a random sample from \n",
    "        `squad_train` as defined above.\n",
    "    joiner : str\n",
    "        The character to use to join pieces of the prompt \n",
    "        into a single str.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str, the prompt\n",
    "\n",
    "    \"\"\"\n",
    "    ##### YOUR CODE HERE\n",
    "    passage_context = passage\n",
    "    \n",
    "    segs = []\n",
    "\n",
    "    for t in train_exs:\n",
    "        segs += [\n",
    "            # f\"Title: {t.title}\",\n",
    "            f\"Background: {t.context}\",\n",
    "            f\"Q: {t.question}\",\n",
    "            f\"A: {t.answers[0]}\"\n",
    "        ]\n",
    "    segs += [\n",
    "            # f\"Title: {passage_title}\",\n",
    "            f\"Background: {passage_context}\",\n",
    "            f\"Q: {question}\",\n",
    "            f\"A:\"\n",
    "    ]\n",
    "    return joiner.join(segs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "DRhoMeEGYre3"
   },
   "outputs": [],
   "source": [
    "def evaluate_few_shot_open_qa(\n",
    "        examples,\n",
    "        squad_train,\n",
    "        batch_size=20,\n",
    "        n_context=2,\n",
    "        joiner=\"\\n\\n\",\n",
    "        gen_func=run_eleuther):\n",
    "    \"\"\"Evaluate a few-shot OpenQA approach defined by \n",
    "    `build_few_shot_open_qa_prompt` and `gen_func`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    examples : iterable of SQuAD train examples\n",
    "        Presumably a subset of `squad_dev` as defined above.\n",
    "    squad_train : iterable of SQuAD train examples\n",
    "    batch_size : int\n",
    "        Number of examples to send to `gen_func` at once.\n",
    "    joiner : str\n",
    "        Used by `build_few_shot_open_qa_prompt` to join segments\n",
    "        of the prompt into a single str.\n",
    "    gen_func : either `run_eleuther` or `run_gpt3`\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict as determined by `evaluate` above.\n",
    "\n",
    "    \"\"\"\n",
    "    # A list of strings that you build and feed into `gen_func`.\n",
    "    prompts = []\n",
    "\n",
    "    # A list of dicts that you get from `gen_func`.\n",
    "    gens = []\n",
    "\n",
    "    # Iterate through the examples in batches:\n",
    "    for i in range(0, len(examples), batch_size):\n",
    "        # Use the `searcher` defined above to get passages\n",
    "        # using `ex.question` as the query, and use your\n",
    "        # `build_few_shot_open_qa_prompt` to build prompts.\n",
    "\n",
    "        ##### YOUR CODE HERE\n",
    "        \n",
    "        batch = examples[i: i+batch_size]\n",
    "\n",
    "        # sample training from squad_train\n",
    "        train_exs = random.sample(squad_train, k=n_context)\n",
    "\n",
    "        ## get a passage for each example in the dev batch\n",
    "        # get search results (passage index)\n",
    "        results = [searcher.search(ex.question, k=1) for ex in batch]\n",
    "\n",
    "        # from passage index to get the passage 'title | passage'\n",
    "        passages = [searcher.collection[r[0][0]] for r in results]\n",
    " \n",
    "        ps = []\n",
    "\n",
    "        # for every question, combine the find passage and generate the prompt\n",
    "        # append all prompt into a list\n",
    "        for ex, psg in zip(batch, passages):\n",
    "            ps.append(build_few_shot_open_qa_prompt(ex.question, psg, train_exs, joiner=joiner))  \n",
    "\n",
    "        # feed prompt to gen_func\n",
    "        gs = gen_func(ps)       \n",
    "\n",
    "        # add the prompt to prompt list\n",
    "        prompts += ps\n",
    "        # add generated txt to gen list\n",
    "        gens += gs\n",
    "\n",
    "\n",
    "    # Return value from a call to `evalaute`, with `examples`\n",
    "    # as provided by the user and the `prompts` and `gens`\n",
    "    # you built:\n",
    "    return evaluate(examples, prompts, gens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oXeQzplkYre3"
   },
   "source": [
    "### Answer scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "m7PhfMNsYre3"
   },
   "outputs": [],
   "source": [
    "def get_passages_with_scores(question, k=5):\n",
    "    \"\"\"Pseudo-probabilities from the retriever.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    question : str\n",
    "    k : int\n",
    "        Number of passages to retrieve.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    passages (list of str), passage_probs (np.array)\n",
    "\n",
    "    \"\"\"\n",
    "    # Use the `searcher` to get `k` passages for `questions`:\n",
    "    ##### YOUR CODE HERE\n",
    "    search_score = searcher.search(question, k = k)[2]\n",
    "    passage_index = searcher.search(question, k = k)[0]\n",
    "\n",
    "    # Softmax normalize the scores and convert the list to\n",
    "    # a NumPy array:\n",
    "    ##### YOUR CODE HERE\n",
    "    exp_score = np.exp(search_score)\n",
    "    sum_score = np.sum(exp_score) \n",
    "    passage_probs = np.array([score/sum_score for score in exp_score] )\n",
    "\n",
    "    # Get the passages as a list of texts:\n",
    "    ##### YOUR CODE HERE\n",
    "\n",
    "    passages = [searcher.collection[idx] for idx in passage_index]\n",
    "\n",
    "    return passages, passage_probs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "SwQVdCb6Yre4"
   },
   "outputs": [],
   "source": [
    "from types import GeneratorType\n",
    "def answer_scoring(passages, passage_probs, prompts, gen_func=run_eleuther):\n",
    "    \"\"\"Implements our basic scoring strategy.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    passages : list of str\n",
    "    passage_probs : list of float\n",
    "    prompts : list of str\n",
    "    gen_func : either `run_eleuther` or `run_gpt3`\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list of pairs (score, dict), sorted with the largest score first.\n",
    "    `dict` should be the return value of `gen_func` for an example.\n",
    "\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    for passage, passage_prob, prompt in zip(passages, passage_probs, prompts):\n",
    "        # Run `gen_func` on [prompt] (crucially, the singleton list here),\n",
    "        # and get the dictionary `gen` from the singleton list `gen_func`\n",
    "        # returns, and then use the values to score `gen` according to our\n",
    "        # scoring method.\n",
    "        #\n",
    "        # Be sure to use \"generated_answer_probs\" for the scores.\n",
    "        ##### YOUR CODE HERE\n",
    "\n",
    "        gen = gen_func([prompt])\n",
    "\n",
    "        # print(gen)\n",
    "        \n",
    "        answer_score = np.prod(gen[0]['generated_answer_probs'])\n",
    "\n",
    "        final_score = passage_prob*answer_score\n",
    "        \n",
    "        data.append((final_score, gen[0]))\n",
    "\n",
    "\n",
    "    # Return `data`, sorted with the highest scoring `(score, gen)`\n",
    "    # pair given first.\n",
    "    ##### YOUR CODE HERE\n",
    "    data.sort(key = lambda x: x[0], reverse=True)\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline System (ColBERT straight output + Eleuther)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "          temperature 0.05 \n",
      "          Macro F1 is: 0.11406956518107561， \n",
      "          Exact Match: 0.024773413897280966， \n",
      "          Macro Precision is: 0.10246440534960173,\n",
      "          Macro Recall is: 0.20589445145434757,\n",
      "          \n",
      "CPU times: user 1h 50min 6s, sys: 4min 15s, total: 1h 54min 21s\n",
      "Wall time: 15min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "working_dataset = bioasq_test # bioasq_test vs dev_exs\n",
    "\n",
    "batch_size = 5\n",
    "joiner = '\\n\\n'\n",
    "# number of prompts\n",
    "n_context = 2\n",
    "\n",
    "# temperatures = [0.01, 0.025, 0.05, 0.075]\n",
    "temperatures = [0.025]\n",
    "# bioasq_dev\n",
    "# bioasq_train\n",
    "\n",
    "for temperature in temperatures:\n",
    "    \n",
    "    prompts = []\n",
    "\n",
    "    gens = []\n",
    "\n",
    "    for i in range(0, len(working_dataset), batch_size):\n",
    "        # Use the `searcher` defined above to get passages\n",
    "        # using `ex.question` as the query, and use your\n",
    "        # `build_few_shot_open_qa_prompt` to build prompts.\n",
    "        \n",
    "        # get a batch from bioasq dev (to replace dev_exs)\n",
    "        batch = working_dataset[i: i+batch_size]\n",
    "\n",
    "        train_exs = random.sample(bioasq_train, k=n_context)\n",
    "\n",
    "        ## get a passage for each example in the dev batch\n",
    "        # get search results (passage index) for all examples in the batch\n",
    "        # k = 1 because we choose the top result\n",
    "        results = [searcher.search(ex.question, k=1) for ex in batch]\n",
    "\n",
    "        # from passage index to get the passage 'title | passage'\n",
    "        passages = [searcher.collection[r[0][0]] for r in results]\n",
    "\n",
    "        # re-initiating prompt\n",
    "        ps = []\n",
    "\n",
    "        # for every question in the batch, combine the train_exs (background + q +a) + found passage + question and generate the prompt\n",
    "        # append all prompt into a list\n",
    "        \n",
    "        for ex, psg in zip(batch, passages):\n",
    "            ps.append(build_few_shot_open_qa_prompt(ex.question, psg, train_exs, joiner=joiner))  \n",
    "\n",
    "        # feed prompts (in list of prompts) to gen_func\n",
    "        gs = run_eleuther(ps)       \n",
    "\n",
    "        # add the prompt to prompt list\n",
    "        prompts += ps\n",
    "        # add generated txt to gen list\n",
    "        gens += gs\n",
    "    \n",
    "    eva = evaluate(working_dataset, prompts, gens)\n",
    "    # print(eva)\n",
    "    print(f\"\"\"\n",
    "          temperature {temperature} \n",
    "          Macro F1 is: {eva['macro_f1']}， \n",
    "          Exact Match: {eva['em_per']}， \n",
    "          Macro Precision is: {eva['macro_precision']},\n",
    "          Macro Recall is: {eva['macro_recall']},\n",
    "          \"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ColBERT improvement (normalization/answer scoring) + Eleuther"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "id": "D9pfDIrPYre4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "          temperature 0.025 \n",
      "          Macro F1 is: 0.1641240799601087， \n",
      "          Exact Match: 0.07734138972809668， \n",
      "          Macro Precision is: 0.17187570066724142,\n",
      "          Macro Recall is: 0.22683778416752035,\n",
      "          \n",
      "CPU times: user 12h 8min 59s, sys: 8min 27s, total: 12h 17min 26s\n",
      "Wall time: 1h 34min 8s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "######## This part is functional modules ############\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "#### enhanced squad training example searching\n",
    "\n",
    "def train_tf_idf(bioasq_train):\n",
    "    tfidfvectorizer = TfidfVectorizer(analyzer='word',stop_words= 'english', ngram_range=(1, 3))\n",
    "\n",
    "  # append all context\n",
    "    train_context = [x.context for x in bioasq_train]\n",
    "\n",
    "    tfidfvectorizer.fit_transform(train_context)\n",
    "\n",
    "    context_tfidf = tfidfvectorizer.transform(train_context)\n",
    "\n",
    "    return tfidfvectorizer, context_tfidf\n",
    "\n",
    "def sample_bioasq_train(tfidfvectorizer, context_tfidf, question, n_context):\n",
    "    '''\n",
    "    This is using tf-idf and consine similarity to sample \"related to question\" bioasq example to build the prompt\n",
    "    '''\n",
    "    question_tfidf = tfidfvectorizer.transform([question])\n",
    "\n",
    "    cosine_sim = cosine_similarity(context_tfidf, question_tfidf).flatten()\n",
    "\n",
    "    related_index = cosine_sim.argsort()[-n_context:][::-1]\n",
    "\n",
    "    train_exs = [bioasq_train[i] for i in related_index]\n",
    "\n",
    "    return train_exs\n",
    "\n",
    "### revised answer scoring by normalizing the score by length\n",
    "from types import GeneratorType\n",
    "## added temperature arg to allow change\n",
    "def answer_scoring_normalized(passages, passage_probs, prompts, temperature, gen_func=run_eleuther):\n",
    "    \"\"\"Implements our basic scoring strategy.\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  passages : list of str\n",
    "  passage_probs : list of float\n",
    "  prompts : list of str\n",
    "  gen_func : either `run_eleuther` or `run_gpt3`\n",
    "\n",
    "  Returns\n",
    "  -------\n",
    "  list of pairs (score, dict), sorted with the largest score first.\n",
    "  `dict` should be the return value of `gen_func` for an example.\n",
    "\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    length_sum = 0\n",
    "    gen_list = []\n",
    "\n",
    "    for passage, passage_prob, prompt in zip(passages, passage_probs, prompts):\n",
    "        gen = gen_func([prompt], temperature = temperature)\n",
    "\n",
    "        gen_list.append(gen)\n",
    "        # calculate the total length of answers\n",
    "        length_sum += len(gen[0]['generated_answer'].split(' '))\n",
    "\n",
    "    for passage_prob, gen in zip(passage_probs, gen_list):\n",
    "\n",
    "        answer_score = np.prod(gen[0]['generated_answer_probs'])\n",
    "\n",
    "        length_of_answer = len(gen[0]['generated_answer'].split(' '))\n",
    "\n",
    "        # give more weight to longer answers, as its product of per-token probabiliyy is underdog\n",
    "        weight = length_of_answer/length_sum\n",
    "\n",
    "        final_score = passage_prob*answer_score*weight\n",
    "\n",
    "        data.append((final_score, gen[0]))\n",
    "\n",
    "    data.sort(key = lambda x: x[0], reverse=True)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "######## This part is system development ############\n",
    "\n",
    "batch_size = 5\n",
    "joiner = '\\n\\n'\n",
    "n_context = 2\n",
    "\n",
    "# temperatures = [0.01, 0.025, 0.05, 0.075]\n",
    "temperatures = [0.025]\n",
    "\n",
    "\n",
    "working_dataset = bioasq_test # bioasq_test vs dev_exs\n",
    "\n",
    "for temperature in temperatures:\n",
    "    prompts = []\n",
    "\n",
    "    gens = []\n",
    "\n",
    "    # use tf-idf to find \"related few shot in bioasq to build the prompt\n",
    "    # train tf-idf on all bioasq examples\n",
    "    tfidfvectorizer, context_tfidf = train_tf_idf(bioasq_train)\n",
    "\n",
    "    for i in range(0, len(working_dataset), batch_size):\n",
    "        # Use the `searcher` defined above to get passages\n",
    "        # using `ex.question` as the query, and use your\n",
    "        # `build_few_shot_open_qa_prompt` to build prompts.\n",
    "\n",
    "        # get a batch from bioasq dev (to replace dev_exs)\n",
    "        batch = working_dataset[i: i+batch_size]\n",
    "\n",
    "        # train_exs = random.sample(bioasq_train, k=n_context)\n",
    "\n",
    "        ## score for answer-passage pair\n",
    "        for ex in batch:\n",
    "\n",
    "          # use tf idf to sample training exs, instead of just random sampling bioasq training\n",
    "            train_exs = sample_bioasq_train(tfidfvectorizer, context_tfidf, ex.question, n_context)\n",
    "\n",
    "            passages, passage_probs = get_passages_with_scores(ex.question)\n",
    "\n",
    "            # re-initiating prompt\n",
    "            ps = []\n",
    "            # iterate through each passage in the top k (5) passages\n",
    "            for psg in passages:\n",
    "            # build the prompt based on question, that specific passge, and training examples\n",
    "            # say we have passage, then ps will be ['prompt1', 'prompt2', 'prompt3', 'prompt4', 'prompt5']\n",
    "                ps.append(build_few_shot_open_qa_prompt(ex.question, psg, train_exs, joiner=joiner)) \n",
    "\n",
    "          # calculate the answering score for the highest passage-answer pair                 \n",
    "          # data = answer_scoring(passages,       # only related to question, same length as ps\n",
    "          #                       passage_probs,  # only related to question, same length as ps\n",
    "          #                       ps,             # k prompts\n",
    "          #                       run_eleuther)\n",
    "\n",
    "            data = answer_scoring_normalized(passages,       # only related to question, same length as ps\n",
    "                                passage_probs,  # only related to question, same length as ps\n",
    "                                ps,             # k prompts\n",
    "                                temperature,\n",
    "                                run_eleuther)\n",
    "\n",
    "            # pick highest score answer-prompt pair (note: in)\n",
    "            highest_gs = [data[0][1]]\n",
    "            highest_ps = [data[0][1]['prompt']]\n",
    "\n",
    "            # add the prompt to prompt list\n",
    "            prompts += highest_ps\n",
    "\n",
    "            # add generated txt to gen list\n",
    "            gens += highest_gs\n",
    " \n",
    "    eva = evaluate(working_dataset, prompts, gens)\n",
    "    \n",
    "    print(f\"\"\"\n",
    "          temperature {temperature} \n",
    "          Macro F1 is: {eva['macro_f1']}， \n",
    "          Exact Match: {eva['em_per']}， \n",
    "          Macro Precision is: {eva['macro_precision']},\n",
    "          Macro Recall is: {eva['macro_recall']},\n",
    "          \"\"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nWqnZmNeYG6S"
   },
   "source": [
    "#### DPR Model for retrieval + LM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DPRContextEncoder, AutoModel\n",
    "\n",
    "context_tokenizer = AutoTokenizer.from_pretrained(\"facebook/dpr-ctx_encoder-single-nq-base\")\n",
    "\n",
    "encode_context_model = DPRContextEncoder.from_pretrained(\"facebook/dpr-ctx_encoder-single-nq-base\")\n",
    "\n",
    "question_tokenizer = AutoTokenizer.from_pretrained(\"facebook/dpr-question_encoder-single-nq-base\")\n",
    "\n",
    "encode_question_model = AutoModel.from_pretrained(\"facebook/dpr-question_encoder-single-nq-base\")\n",
    "\n",
    "encode_context_model.to(device)\n",
    "encode_question_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# create passage embedding for the following analysis\n",
    "\n",
    "## make all passage embeddings and get their id\n",
    "context_all = []\n",
    "\n",
    "encode_context_model.eval()\n",
    "\n",
    "\n",
    "for i in range(len(bioasq_list)):\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        context_input_ids = context_tokenizer(bioasq_list[i]['context'], return_tensors=\"pt\")[\"input_ids\"]\n",
    "        context_embeddings = encode_context_model(context_input_ids).pooler_output.squeeze()\n",
    "\n",
    "        context_all.append(context_embeddings)\n",
    "\n",
    "        torch.cuda.empty_cache() # probably redundant\n",
    "    \n",
    "    if i % 100 == 0:\n",
    "        print(i)\n",
    "\n",
    "context_all_tensor = torch.stack(context_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "          temperature 0.025 \n",
      "          Macro F1 is: 0.0806630306815693， \n",
      "          Exact Match: 0.009667673716012085， \n",
      "          Macro Precision is: 0.0759503906633816,\n",
      "          Macro Recall is: 0.14150972153460456,\n",
      "          \n",
      "CPU times: user 2h 33min 52s, sys: 32min 50s, total: 3h 6min 43s\n",
      "Wall time: 46min 40s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "working_dataset = bioasq_test # bioasq_test vs dev_exs\n",
    "\n",
    "batch_size = 5\n",
    "joiner = '\\n\\n'\n",
    "# number of prompts\n",
    "n_context = 2\n",
    "\n",
    "# temperatures = [0.01, 0.025, 0.05, 0.075]\n",
    "temperatures = [0.025]\n",
    "\n",
    "encode_question_model.eval()\n",
    "\n",
    "for temperature in temperatures:\n",
    "    \n",
    "    prompts = []\n",
    "\n",
    "    gens = []\n",
    "\n",
    "    for i in range(0, len(working_dataset), batch_size):\n",
    "        # Use the `searcher` defined above to get passages\n",
    "        # using `ex.question` as the query, and use your\n",
    "        # `build_few_shot_open_qa_prompt` to build prompts.\n",
    "        \n",
    "        # get a batch from bioasq dev (to replace dev_exs)\n",
    "        batch = working_dataset[i: i+batch_size]\n",
    "\n",
    "        train_exs = random.sample(bioasq_train, k=n_context)\n",
    "\n",
    "        ## get a passage for each example in the dev batch\n",
    "        # get search results (passage index) for all examples in the batch\n",
    "        # k = 1 because we choose the top result\n",
    "        \n",
    "        passages = []\n",
    "        \n",
    "        for ex in batch:\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                \n",
    "                # encode question\n",
    "                question_input_ids = question_tokenizer(ex.question, return_tensors=\"pt\")[\"input_ids\"]\n",
    "                question_embeddings = encode_question_model(question_input_ids).pooler_output.squeeze()\n",
    "                \n",
    "                # get the dot product (score and sort it)\n",
    "                dot_products = torch.sum(context_all_tensor * question_embeddings, -1)\n",
    "                new_dot_products = torch.sort(dot_products, dim=- 1, descending=True)\n",
    "                \n",
    "                # retrieve the passage\n",
    "                passages.append(bioasq_list[new_dot_products.indices[0]]['context'])\n",
    "\n",
    "        # re-initiating prompt\n",
    "        ps = []\n",
    "\n",
    "        # for every question in the batch, combine the train_exs (background + q +a) + found passage + question and generate the prompt\n",
    "        # append all prompt into a list\n",
    "        \n",
    "        for ex, psg in zip(batch, passages):\n",
    "            ps.append(build_few_shot_open_qa_prompt(ex.question, psg, train_exs, joiner=joiner))  \n",
    "\n",
    "        # feed prompts (in list of prompts) to gen_func\n",
    "        gs = run_eleuther(ps)       \n",
    "\n",
    "        # add the prompt to prompt list\n",
    "        prompts += ps\n",
    "        # add generated txt to gen list\n",
    "        gens += gs\n",
    "    \n",
    "    eva = evaluate(working_dataset, prompts, gens)\n",
    "    # print(eva)\n",
    "    print(f\"\"\"\n",
    "          temperature {temperature} \n",
    "          Macro F1 is: {eva['macro_f1']}， \n",
    "          Exact Match: {eva['em_per']}， \n",
    "          Macro Precision is: {eva['macro_precision']},\n",
    "          Macro Recall is: {eva['macro_recall']},\n",
    "          \"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Doc2Query Augumentation + BM25\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "d2q_tokenizer = T5Tokenizer.from_pretrained('castorini/doc2query-t5-base-msmarco')\n",
    "d2q_model = T5ForConditionalGeneration.from_pretrained('castorini/doc2query-t5-base-msmarco')\n",
    "d2q_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "bioasq_list_copy = copy.deepcopy(bioasq_list)\n",
    "# bioasq_list_copy = bioasq_list.copy()\n",
    "# bioasq_list_copy = bioasq_list_copy[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15min 58s, sys: 35.1 s, total: 16min 33s\n",
      "Wall time: 16min 33s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "## this is to use doc2query to find additional question from MS Macro and append the top 10 \n",
    "## questions back to the context\n",
    "d2q_model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    \n",
    "    for i in range(len(bioasq_list_copy)):\n",
    "        \n",
    "        doc_text = bioasq_list_copy[i]['context']\n",
    "\n",
    "        input_ids = d2q_tokenizer.encode(doc_text, return_tensors='pt').to(device)\n",
    "        \n",
    "        number_of_q = 10\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = d2q_model.generate(\n",
    "                input_ids=input_ids,\n",
    "                max_length=64,\n",
    "                do_sample=True,\n",
    "                top_k=10,\n",
    "                num_return_sequences=number_of_q)\n",
    "    \n",
    "        query_to_append = ' '\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for j in range(number_of_q):\n",
    "                query_to_append += d2q_tokenizer.decode(outputs[j], skip_special_tokens=True) + ' '\n",
    "            \n",
    "        # append the query to the passage\n",
    "        \n",
    "        bioasq_list_copy[i]['context'] = bioasq_list_copy[i]['context'] + query_to_append\n",
    "        \n",
    "        torch.cuda.empty_cache() # probably redundant\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "207"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bioasq_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "          temperature 0.025 \n",
      "          Macro F1 is: 0.08072085112218676， \n",
      "          Exact Match: 0.015105740181268883， \n",
      "          Macro Precision is: 0.07526040424831965,\n",
      "          Macro Recall is: 0.14252709122749127,\n",
      "          \n",
      "CPU times: user 3h 1min 42s, sys: 27min 50s, total: 3h 29min 32s\n",
      "Wall time: 52min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "working_dataset = bioasq_test # bioasq_test vs dev_exs\n",
    "\n",
    "batch_size = 5\n",
    "joiner = '\\n\\n'\n",
    "# number of prompts\n",
    "n_context = 2\n",
    "\n",
    "# temperatures = [0.01, 0.025, 0.05, 0.075]\n",
    "temperatures = [0.025]\n",
    "\n",
    "## prepare BM25\n",
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "tokenized_corpus = [example['context'].split(\" \") for example in bioasq_list_copy]\n",
    "\n",
    "bm25 = BM25Okapi(tokenized_corpus)\n",
    "\n",
    "for temperature in temperatures:\n",
    "    \n",
    "    prompts = []\n",
    "\n",
    "    gens = []\n",
    "\n",
    "    for i in range(0, len(working_dataset), batch_size):\n",
    "        # Use the `searcher` defined above to get passages\n",
    "        # using `ex.question` as the query, and use your\n",
    "        # `build_few_shot_open_qa_prompt` to build prompts.\n",
    "        \n",
    "        # get a batch from bioasq dev (to replace dev_exs)\n",
    "        batch = working_dataset[i: i+batch_size]\n",
    "\n",
    "        train_exs = random.sample(bioasq_train, k=n_context)\n",
    "\n",
    "        ## get a passage for each example in the dev batch\n",
    "        # get search results (passage index) for all examples in the batch\n",
    "        # k = 1 because we choose the top result\n",
    "        \n",
    "        passages = []\n",
    "        \n",
    "        for ex in batch:\n",
    "                \n",
    "            tokenized_query = ex.question.split(\" \")\n",
    "            \n",
    "            # retrieve the top one passage with question added\n",
    "            # doc_scores = bm25.get_scores(tokenized_query)\n",
    "            passage = bm25.get_top_n(tokenized_query, bioasq_list_copy, n=1)[0]['context']\n",
    "\n",
    "            # retrieve the passage\n",
    "            passages.append(passage)\n",
    "\n",
    "        # re-initiating prompt\n",
    "        ps = []\n",
    "\n",
    "        # for every question in the batch, combine the train_exs (background + q +a) + found passage + question and generate the prompt\n",
    "        # append all prompt into a list\n",
    "        \n",
    "        for ex, psg in zip(batch, passages):\n",
    "            ps.append(build_few_shot_open_qa_prompt(ex.question, psg, train_exs, joiner=joiner))  \n",
    "\n",
    "        # feed prompts (in list of prompts) to gen_func\n",
    "        gs = run_eleuther(ps)       \n",
    "\n",
    "        # add the prompt to prompt list\n",
    "        prompts += ps\n",
    "        # add generated txt to gen list\n",
    "        gens += gs\n",
    "    \n",
    "    eva = evaluate(working_dataset, prompts, gens)\n",
    "    # print(eva)\n",
    "    print(f\"\"\"\n",
    "          temperature {temperature} \n",
    "          Macro F1 is: {eva['macro_f1']}， \n",
    "          Exact Match: {eva['em_per']}， \n",
    "          Macro Precision is: {eva['macro_precision']},\n",
    "          Macro Recall is: {eva['macro_recall']},\n",
    "          \"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT BASE Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rank Fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "dKB9zXRBYrel",
    "-mp1C-oyYreq",
    "Fca8-RXjYres",
    "XGK3hCs9Yrev",
    "DZpXMk-0Yrew",
    "NFYxJPpuYre0",
    "w9n0_xwdYre1",
    "teKobQM8Yre1",
    "R55lgmnTzTip",
    "w2mx3Z4HYre2"
   ],
   "machine_shape": "hm",
   "name": "hw_openqa.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "a99ac6d2deb03d0b7ced3594556c328848678d7cea021ae1b9990e15d3ad5c49"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "19838b224425453988f4ea47c50c048e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "29084cffd32b495ba5c29910a483b41a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_cb331d5edf364e19af8f45fd3726a9f8",
       "IPY_MODEL_dcb183bf0a85498d8bb2bd360c64255b",
       "IPY_MODEL_83ab942f9ce14cd6b82da28b36dd1377"
      ],
      "layout": "IPY_MODEL_19838b224425453988f4ea47c50c048e"
     }
    },
    "5054cd44686b4f649724445b581ad979": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "76b125c6b6cf426cbf3ad638e443cbd8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "83ab942f9ce14cd6b82da28b36dd1377": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_eda083d3daeb4c59b66beb142f12063a",
      "placeholder": "​",
      "style": "IPY_MODEL_adaf48a176ad439f89883761b3027232",
      "value": " 2/2 [00:00&lt;00:00,  7.29it/s]"
     }
    },
    "adaf48a176ad439f89883761b3027232": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "bdcd068db1ad4c55ba282295b91aaeec": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cb331d5edf364e19af8f45fd3726a9f8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_bdcd068db1ad4c55ba282295b91aaeec",
      "placeholder": "​",
      "style": "IPY_MODEL_f61e1dd551d74455b834735b4aa816df",
      "value": "100%"
     }
    },
    "dcb183bf0a85498d8bb2bd360c64255b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5054cd44686b4f649724445b581ad979",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_76b125c6b6cf426cbf3ad638e443cbd8",
      "value": 2
     }
    },
    "eda083d3daeb4c59b66beb142f12063a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f61e1dd551d74455b834735b4aa816df": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
