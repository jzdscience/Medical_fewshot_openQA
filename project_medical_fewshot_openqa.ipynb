{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G0qZfMf4Yreh"
   },
   "source": [
    "# Medical Few-shot OpenQA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dKB9zXRBYrel"
   },
   "source": [
    "## Set-up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sFM54iO7Yren"
   },
   "source": [
    "### General set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "hL9AAtTzYren"
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "from contextlib import nullcontext\n",
    "from collections import namedtuple\n",
    "from datasets import load_dataset\n",
    "import json\n",
    "import numpy as np\n",
    "import random\n",
    "import re \n",
    "import string\n",
    "import torch\n",
    "from typing import List\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "MIvsYoIpYreo"
   },
   "outputs": [],
   "source": [
    "seed = 1\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-mp1C-oyYreq"
   },
   "source": [
    "### Language model set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "TKQEIYGDYrer"
   },
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "transformers.logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### ColBERT set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the repo\n",
    "# !git clone -b cpu_inference https://github.com/stanford-futuredata/ColBERT.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, 'ColBERT/')\n",
    "\n",
    "from colbert.infra import Run, RunConfig, ColBERTConfig\n",
    "from colbert.data import Collection\n",
    "from colbert.searcher import Searcher\n",
    "from utility.utils.dpr import has_answer, DPR_normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NFYxJPpuYre0"
   },
   "source": [
    "##### ColBERT parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "5tnUU2UHYre0"
   },
   "outputs": [],
   "source": [
    "index_home = os.path.join(\"experiments\", \"notebook\", \"indexes\")\n",
    "\n",
    "if not os.path.exists(os.path.join(\"data\", \"openqa\", \"colbertv2.0.tar.gz\")):\n",
    "    !mkdir -p data/openqa\n",
    "    # ColBERTv2 checkpoint trained on MS MARCO Passage Ranking (388MB compressed)\n",
    "    !wget https://downloads.cs.stanford.edu/nlp/data/colbert/colbertv2/colbertv2.0.tar.gz -P data/openqa/\n",
    "    !tar -xvzf data/openqa/colbertv2.0.tar.gz -C data/openqa/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U5ganiuNYres",
    "tags": []
   },
   "source": [
    "## Language model Loading\n",
    "\n",
    "In few-shot OpenQA, the language model (LM) must read in a prompt and answer the question posed somewhere in the prompt. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fca8-RXjYres"
   },
   "source": [
    "### Answerhood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "U6KepnjTYret"
   },
   "outputs": [],
   "source": [
    "def _find_generated_answer(tokens, newline=\"\\n\" ): \n",
    "    \"\"\"Our LMs tend to insert initial newline characters before\n",
    "    they begin generating text. This function ensures that we \n",
    "    properly capture the true first line as the answer while\n",
    "    also ensuring that token probabilities are aligned.\"\"\"        \n",
    "    answer_token_indices = []\n",
    "    char_seen = False            \n",
    "    for i, tok in enumerate(tokens):\n",
    "        # This is the main condition: a newline that isn't an initial\n",
    "        # string of newlines:\n",
    "        if tok == newline and char_seen:\n",
    "            break\n",
    "        # Keep the initial newlines for consistency:\n",
    "        elif tok == newline and not char_seen:\n",
    "            answer_token_indices.append(i)\n",
    "        # Proper tokens:\n",
    "        elif tok != newline:\n",
    "            char_seen = True\n",
    "            answer_token_indices.append(i)\n",
    "    return answer_token_indices "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "klW12GkAYret"
   },
   "source": [
    "### Eleuther models from Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "1-FkbTaUYret"
   },
   "outputs": [],
   "source": [
    "# \"gpt-neo-125M\" \"gpt-neo-1.3B\" \"gpt-neo-2.7B\" \"gpt-j-6B\"\n",
    "eleuther_model_name = \"gpt-neo-125M\"\n",
    "\n",
    "eleuther_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    f\"EleutherAI/{eleuther_model_name}\", \n",
    "    padding_side=\"left\", \n",
    "    padding='longest', \n",
    "    truncation='longest_first', max_length=2000)\n",
    "eleuther_tokenizer.pad_token = eleuther_tokenizer.eos_token\n",
    "\n",
    "eleuther_model = AutoModelForCausalLM.from_pretrained(\n",
    "    f\"EleutherAI/{eleuther_model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "rEo9YKwNYret"
   },
   "outputs": [],
   "source": [
    "def run_eleuther(prompts, temperature=0.1, top_p=0.95, **generate_kwargs): \n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    prompts : iterable of str\n",
    "    temperature : float\n",
    "        It seems best to set it low for this task!\n",
    "    top_p : float\n",
    "       \n",
    "    For options for `generate_kwargs`, see:\n",
    "    \n",
    "    https://huggingface.co/docs/transformers/master/en/main_classes/text_generation#transformers.generation_utils.GenerationMixin.generate\n",
    "    \n",
    "    Options that are likely to be especially relevant include \n",
    "    `temperature`, `length_penalty`, and the parameters that\n",
    "    determine the decoding strategy. With `num_return_sequences > 1`,\n",
    "    the default parameters in this function do multinomial sampling.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    list of dicts\n",
    "    \n",
    "    {\"prompt\": str, \n",
    "     \"generated_text\": str, \"generated_tokens\": list of str, \"generated_probs\": list of float,\n",
    "     \"answer\": str, \"answer_tokens\": list of str, \"answer_probs\": list of float\n",
    "    }\n",
    "         \n",
    "    \"\"\"\n",
    "    prompt_ids = eleuther_tokenizer(\n",
    "        prompts, return_tensors=\"pt\", padding=True).input_ids\n",
    "        \n",
    "    with torch.inference_mode():\n",
    "        # Automatic mixed precision if possible.\n",
    "        with torch.cuda.amp.autocast() if torch.cuda.is_available() else nullcontext():\n",
    "            model_output = eleuther_model.generate(\n",
    "                prompt_ids,\n",
    "                temperature=temperature,\n",
    "                do_sample=True,\n",
    "                top_p=top_p,           \n",
    "                max_new_tokens=16,\n",
    "                num_return_sequences=1,                \n",
    "                pad_token_id=eleuther_tokenizer.eos_token_id, \n",
    "                return_dict_in_generate=True,\n",
    "                output_scores=True,\n",
    "                **generate_kwargs)\n",
    "        \n",
    "    # Converting output scores using the helpful recipe here:\n",
    "    # https://discuss.huggingface.co/t/generation-probabilities-how-to-compute-probabilities-of-output-scores-for-gpt2/3175\n",
    "    gen_ids = model_output.sequences[:, prompt_ids.shape[-1] :]\n",
    "    gen_probs = torch.stack(model_output.scores, dim=1).softmax(-1)\n",
    "    gen_probs = torch.gather(gen_probs, 2, gen_ids[:, :, None]).squeeze(-1)\n",
    "    \n",
    "    # Generated texts, including the prompts:\n",
    "    gen_texts = eleuther_tokenizer.batch_decode(\n",
    "        model_output.sequences, skip_special_tokens=True)\n",
    "    \n",
    "    data = []     \n",
    "    iterator = zip(prompts, gen_ids, gen_texts, gen_probs)    \n",
    "    for prompt, gen_id, gen_text, gen_prob in iterator:       \n",
    "        gen_tokens = eleuther_tokenizer.convert_ids_to_tokens(gen_id)\n",
    "        generated_text = gen_text[len(prompt): ]\n",
    "        gen_prob = [float(x) for x in gen_prob.numpy()] # float for JSON storage\n",
    "        ans_indices = _find_generated_answer(gen_tokens, newline=\"Ċ\")\n",
    "        answer_tokens = [gen_tokens[i] for i in ans_indices]\n",
    "        answer_probs = [gen_prob[i] for i in ans_indices]\n",
    "        answer = \"\".join(answer_tokens).replace(\"Ġ\", \" \").replace(\"Ċ\", \"\\n\")                                       \n",
    "        data.append({\n",
    "            \"prompt\": prompt,\n",
    "            \"generated_text\": generated_text,\n",
    "            \"generated_tokens\": gen_tokens,\n",
    "            \"generated_probs\": gen_prob,\n",
    "            \"generated_answer\": answer,\n",
    "            \"generated_answer_probs\": answer_probs,\n",
    "            \"generated_answer_tokens\": answer_tokens})                        \n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7fIfuTEGYreu",
    "outputId": "ec7c8ead-7980-4a4e-b6c1-10aef49bd625"
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# ## test run\n",
    "\n",
    "# eleuther_ex = run_eleuther([    \n",
    "#     \"What year was Stanford University founded?\", \n",
    "#     \"In which year did Stanford first enroll students?\"])\n",
    "\n",
    "# eleuther_ex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XGK3hCs9Yrev",
    "tags": []
   },
   "source": [
    "## Dataset Loading\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### SQuAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0,
     "referenced_widgets": [
      "29084cffd32b495ba5c29910a483b41a",
      "cb331d5edf364e19af8f45fd3726a9f8",
      "dcb183bf0a85498d8bb2bd360c64255b",
      "83ab942f9ce14cd6b82da28b36dd1377",
      "19838b224425453988f4ea47c50c048e",
      "bdcd068db1ad4c55ba282295b91aaeec",
      "f61e1dd551d74455b834735b4aa816df",
      "5054cd44686b4f649724445b581ad979",
      "76b125c6b6cf426cbf3ad638e443cbd8",
      "eda083d3daeb4c59b66beb142f12063a",
      "adaf48a176ad439f89883761b3027232"
     ]
    },
    "id": "YQ_do58EYrev",
    "outputId": "39d8b916-1d33-412e-dbde-8e5cb3085951"
   },
   "outputs": [],
   "source": [
    "squad = load_dataset(\"squad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "B9-0hkxgYrew"
   },
   "outputs": [],
   "source": [
    "SquadExample = namedtuple(\"SquadExample\",  \"id title context question answers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "g2gt0dkeYrew"
   },
   "outputs": [],
   "source": [
    "def get_squad_split(squad, split=\"validation\"):\n",
    "    \"\"\"\n",
    "    Use `split='train'` for the train split.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    list of SquadExample named tuples with attributes\n",
    "    id, title, context, question, answers\n",
    "    \n",
    "    \"\"\"    \n",
    "    fields = squad[split].features\n",
    "    data = zip(*[squad[split][field] for field in fields])\n",
    "    return [SquadExample(eid, title, context, question, answers[\"text\"]) \n",
    "            for eid, title, context, question, answers in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Split Dev and Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = squad['validation'].features\n",
    "data = zip(*[squad['validation'][field] for field in fields])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "NatnOLsDYrew"
   },
   "outputs": [],
   "source": [
    "squad_dev = get_squad_split(squad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jb-ZrSzoYrew",
    "outputId": "4ac4149e-0f96-4c0f-e0ca-aed1d12c796c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SquadExample(id='56d602631c85041400946edb', title='Super_Bowl_50', context='CBS broadcast Super Bowl 50 in the U.S., and charged an average of $5 million for a 30-second commercial during the game. The Super Bowl 50 halftime show was headlined by the British rock group Coldplay with special guest performers Beyoncé and Bruno Mars, who headlined the Super Bowl XLVII and Super Bowl XLVIII halftime shows, respectively. It was the third-most watched U.S. broadcast ever.', question='Who were special guests for the Super Bowl halftime show?', answers=['Beyoncé and Bruno Mars', 'Beyoncé and Bruno Mars', 'Beyoncé and Bruno Mars'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squad_dev[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "rw3OGISgzTil"
   },
   "outputs": [],
   "source": [
    "dev_exs = sorted(squad_dev, key=lambda x: hash(x.id))[: 200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "YwwjQiF8zTim"
   },
   "outputs": [],
   "source": [
    "squad_train = get_squad_split(squad, \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "    num_rows: 87599\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squad['train']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### BioASQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('./data/bioasq/squad.json', 'r') as f:\n",
    "#     squad_test = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick all factoid questions but ignore all else"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/bioasq/training10b.json', 'r') as f:\n",
    "    bioasq_json = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bioasq_json['questions'][0]['snippets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_dict = {}\n",
    "\n",
    "# for snip in bioasq_json['questions'][0]['snippets']:\n",
    "#     if snip['beginSection'] == 'abstract':\n",
    "#         for k in range(snip['offsetInBeginSection'], snip['offsetInEndSection']):\n",
    "#             text_dict[k] = snip['text'][k- snip['offsetInBeginSection']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recon_text = ''\n",
    "# for key in sorted(text_dict.keys()):\n",
    "#     recon_text += text_dict[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recon_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bioasq_json['questions'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we have 1252 factoid questions, 816 list questions, 1018 summary questions, 1148 yesno qquestions\n",
      "total is 4234\n"
     ]
    }
   ],
   "source": [
    "### Construct dataset\n",
    "count_factoid = 0\n",
    "count_list =0\n",
    "count_summary=0\n",
    "count_yesno =0\n",
    "\n",
    "bioasq_list= []\n",
    "\n",
    "for i in range(len(bioasq_json['questions'])):\n",
    "    \n",
    "    sample = bioasq_json['questions'][i]\n",
    "    \n",
    "    if sample['type'] == 'summary':\n",
    "            count_summary += 1\n",
    "    if sample['type'] == 'yesno':\n",
    "            count_yesno += 1\n",
    "    \n",
    "    if sample['type'] in ['factoid', 'list']:\n",
    "        \n",
    "    #  Context\n",
    "    ## flatten all the snippet, conccatenate and use as context\n",
    "        context = '' \n",
    "        for snip in [ele['text'].strip() for ele in sample['snippets']]:\n",
    "            snip += ' '\n",
    "            context += snip\n",
    "            \n",
    "        context = context.replace('\\n', ' ')\n",
    "        \n",
    "        ## limit the length of context\n",
    "        ### Max: 4096 (for eleuther model)\n",
    "        context = context[:1024]\n",
    "        \n",
    "        # question\n",
    "        question = sample['body']\n",
    "        question = question.replace('\\n', ' ')\n",
    "        \n",
    "        # answer:\n",
    "        ## deal with factoid question and list question differently\n",
    "        if sample['type'] == 'factoid':\n",
    "            answer = sample['exact_answer']\n",
    "            count_factoid += 1\n",
    "        \n",
    "        if sample['type'] == 'list':\n",
    "            answer = [x for y in sample['exact_answer'] for x in y]\n",
    "            count_list += 1\n",
    "        \n",
    "\n",
    "        # construct a QA pairs like SQUAD\n",
    "        bioasq_list.append({\n",
    "            'id': i,\n",
    "            'context': context,\n",
    "            'question': sample['body'],\n",
    "            'answers': answer,\n",
    "            'type': sample['type']\n",
    "        }) \n",
    "\n",
    "print(f'we have {count_factoid} factoid questions, {count_list} list questions, {count_summary} summary questions, {count_yesno} yesno qquestions')   \n",
    "\n",
    "print(f'total is {count_factoid +count_list+ count_summary +count_yesno}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2068"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bioasq_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "def get_bioasq_split(bioasq_list, random_state):\n",
    "    \"\"\"\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    list of example named tuples with attributes\n",
    "    id, title, context, question, answers\n",
    "    \n",
    "    \"\"\"\n",
    "    BioasqExample = namedtuple(\"BioasqExample\",  \"id context question answers\")\n",
    "    \n",
    "    bioasq_data = [BioasqExample(ele['id'], ele['context'], ele['question'], ele['answers']) for ele in bioasq_list]\n",
    "    \n",
    "    bioasq_train, _ = train_test_split(bioasq_data, test_size=0.9, random_state=random_state)\n",
    "\n",
    "    bioasq_dev, bioasq_test = train_test_split(_, test_size=0.8888, random_state=random_state)\n",
    "    \n",
    "    return bioasq_train, bioasq_dev, bioasq_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## split dev and test\n",
    "\n",
    "bioasq_train, bioasq_dev, bioasq_test = get_bioasq_split(bioasq_list, random_state=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206, 207, 1655 \n"
     ]
    }
   ],
   "source": [
    "print(f\"{len(bioasq_train)}, {len(bioasq_dev)}, {len(bioasq_test)} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "## pick 10 just for sanity check\n",
    "dev_exs = bioasq_dev[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "829"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_exs[0].id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Util Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DZpXMk-0Yrew",
    "tags": []
   },
   "source": [
    "#### Evaluation\n",
    "\n",
    "Our evaluation protocols are the standard ones for SQuAD and related tasks: exact match of the answer (EM) and token-level F1.\n",
    "\n",
    "We say further that the predicted answer is the first line of generated text after the prompt.\n",
    "\n",
    "The following evaluation code is taken from the [apple/ml-qrecc](https://github.com/apple/ml-qrecc/blob/main/utils/evaluate_qa.py) repository. It performs very basic string normalization before doing the core comparisons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "nHHntDSSYrew"
   },
   "outputs": [],
   "source": [
    "def normalize_answer(s: str) -> str:\n",
    "    \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n",
    "\n",
    "    def remove_articles(text):\n",
    "        regex = re.compile(r'\\b(a|an|the)\\b', re.UNICODE)\n",
    "        return re.sub(regex, ' ', text)\n",
    "\n",
    "    def white_space_fix(text):\n",
    "        return ' '.join(text.split())\n",
    "\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return ''.join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "\n",
    "def get_tokens(s: str) -> List[str]:\n",
    "    \"\"\"Normalize string and split string into tokens.\"\"\"\n",
    "    if not s:\n",
    "        return []\n",
    "    return normalize_answer(s).split()\n",
    "\n",
    "\n",
    "def compute_exact(a_gold: str, a_pred: str) -> int:\n",
    "    \"\"\"Compute the Exact Match score.\"\"\"\n",
    "    return int(normalize_answer(a_gold) == normalize_answer(a_pred))\n",
    "\n",
    "\n",
    "def compute_f1_from_tokens(gold_toks: List[str], pred_toks: List[str]) -> float:\n",
    "    \"\"\"Compute the F1 score from tokenized gold answer and prediction.\"\"\"\n",
    "    common = collections.Counter(gold_toks) & collections.Counter(pred_toks)\n",
    "    num_same = sum(common.values())\n",
    "\n",
    "    if len(gold_toks) == 0 or len(pred_toks) == 0:\n",
    "        # If either is no-answer, then F1 is 1 if they agree, 0 otherwise\n",
    "        f1= int(gold_toks == pred_toks)\n",
    "        precision = int(gold_toks == pred_toks)\n",
    "        recall = int(gold_toks == pred_toks)\n",
    "        \n",
    "     # if no token overlap at all, all metrics is 0\n",
    "    if num_same == 0: \n",
    "        f1= int(gold_toks == pred_toks)\n",
    "        precision = int(gold_toks == pred_toks)\n",
    "        recall = int(gold_toks == pred_toks)\n",
    "    \n",
    "    else:\n",
    "        precision = 1.0 * num_same / len(pred_toks)\n",
    "        recall = 1.0 * num_same / len(gold_toks)\n",
    "        f1 = (2 * precision * recall) / (precision + recall)\n",
    "    return f1, precision, recall\n",
    "\n",
    "\n",
    "def compute_f1(a_gold: str, a_pred: str) -> float:\n",
    "    \"\"\"Compute the F1 score.\"\"\"\n",
    "    gold_toks = get_tokens(a_gold)\n",
    "    pred_toks = get_tokens(a_pred)\n",
    "    return compute_f1_from_tokens(gold_toks, pred_toks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lJo6C7pgYrex"
   },
   "source": [
    "The following is our general evaluation function. We will make extensive use of it to evaluate different systems:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "bJHSxnA6Yrex"
   },
   "outputs": [],
   "source": [
    "def evaluate(examples, prompts, gens):\n",
    "    \"\"\"Generic evalution function.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    examples: iterable of `SquadExample` instances\n",
    "    prompts: list of str\n",
    "    preds: list of LM-generated texts to evaluate as answers\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    dict with keys \"em_per\", \"macro_f1\", \"examples\", where\n",
    "    each \"examples\" value is a dict\n",
    "    \n",
    "    \"\"\"        \n",
    "    results = []\n",
    "    for ex, prompt, gen in zip(examples, prompts, gens):\n",
    "        answers = ex.answers\n",
    "        pred = gen['generated_answer']\n",
    "        # The result is the highest EM from the available answer strings:\n",
    "        em = max([compute_exact(ans, pred) for ans in answers])\n",
    "        \n",
    "        # adding precision and recall\n",
    "        # print([compute_f1(ans, pred) for ans in answers])\n",
    "        f1 = max([compute_f1(ans, pred)[0] for ans in answers])\n",
    "        precision = max([compute_f1(ans, pred)[1] for ans in answers])\n",
    "        recall = max([compute_f1(ans, pred)[2] for ans in answers])\n",
    "        \n",
    "        gen.update({\n",
    "            \"id\": ex.id, \n",
    "            \"question\": ex.question, \n",
    "            \"prediction\": pred, \n",
    "            \"answers\": answers, \n",
    "            \"em\": em,\n",
    "            \"f1\": f1,\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall\n",
    "        })\n",
    "        results.append(gen)\n",
    "    data = {}        \n",
    "    data[\"macro_f1\"] = np.mean([d['f1'] for d in results])\n",
    "    data[\"macro_precision\"] = np.mean([d['precision'] for d in results])\n",
    "    data[\"macro_recall\"] = np.mean([d['recall'] for d in results])\n",
    "    data[\"em_per\"] = sum([d['em'] for d in results]) / len(results)\n",
    "    data[\"examples\"] = results\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fj170C1PYrex"
   },
   "source": [
    "Here is a highly simplified example to help make the logic behind `evaluate` clearer:    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0bgFXLK3Yrex",
    "outputId": "40c6678c-d600-4a2e-b192-84ce765be7d6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'macro_f1': 0.5,\n",
       " 'macro_precision': 0.3333333333333333,\n",
       " 'macro_recall': 1.0,\n",
       " 'em_per': 0.0,\n",
       " 'examples': [{'generated_answer': 'course on NLU',\n",
       "   'generated_text': 'NLU\\nWho am I?',\n",
       "   'id': '0',\n",
       "   'question': 'What is the course to take?',\n",
       "   'prediction': 'course on NLU',\n",
       "   'answers': ['NLU', 'CS224u'],\n",
       "   'em': 0,\n",
       "   'f1': 0.5,\n",
       "   'precision': 0.3333333333333333,\n",
       "   'recall': 1.0}]}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex = namedtuple(\"SquadExample\",  \"id title context question answers\")\n",
    "\n",
    "examples = [\n",
    "    ex(\"0\", \"CS224u\", \n",
    "       \"The course to take is NLU!\", \n",
    "       \"What is the course to take?\", \n",
    "       [\"NLU\", \"CS224u\"])]\n",
    "\n",
    "prompts = [\"Dear model, Please answer this question!\\n\\nQ: What is the course to take?\\n\\nA:\"]\n",
    "\n",
    "gens = [{\"generated_answer\": \"course on NLU\", \"generated_text\": \"NLU\\nWho am I?\"}]\n",
    "\n",
    "evaluate(examples, prompts, gens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MHZHve9NYrex"
   },
   "source": [
    "The bake-off uses `macro_f1` as the primary metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w9n0_xwdYre1"
   },
   "source": [
    "#### Retrieval evaluation\n",
    "\n",
    "For more rigorous evaluations of the retriever alone, we can use Sucess@`k` defined relative to the SQuAD passages and answers. We say that we have a \"success\" if a passage in the top `k` retrieved passages contains any of the answers substrings, and Sucess@`k` is the percentage of such success cases. This is very heuristic (perhaps the answer string happens to occur somewhere in a completely irrelevant passage), but it can still be good guidance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "fsNayHzdYre1"
   },
   "outputs": [],
   "source": [
    "## this is actually success at 5, bug from the teaching team!\n",
    "def success_at_k(examples, k=20):\n",
    "    scores = []\n",
    "    for ex in examples: \n",
    "        scores.append(evaluate_retrieval_example(ex, k=5))\n",
    "    return sum(scores) / len(scores)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w2mx3Z4HYre2",
    "tags": []
   },
   "source": [
    "#### Few-shot OpenQA \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "HUuZ5l3gYre2"
   },
   "outputs": [],
   "source": [
    "def build_few_shot_open_qa_prompt(question, passage, train_exs, joiner=\"\\n\\n\"):\n",
    "    \"\"\"Few-shot OpenQA prompts.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    question : str\n",
    "    passage : str\n",
    "        Presumably something retrieved via search.\n",
    "    train_exs : iterable of SQuAD train examples\n",
    "        These can be obtained via a random sample from \n",
    "        `squad_train` as defined above.\n",
    "    joiner : str\n",
    "        The character to use to join pieces of the prompt \n",
    "        into a single str.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str, the prompt\n",
    "\n",
    "    \"\"\"\n",
    "    ##### YOUR CODE HERE\n",
    "    passage_context = passage\n",
    "    \n",
    "    segs = []\n",
    "\n",
    "    for t in train_exs:\n",
    "        segs += [\n",
    "            # f\"Title: {t.title}\",\n",
    "            f\"Background: {t.context}\",\n",
    "            f\"Q: {t.question}\",\n",
    "            f\"A: {t.answers[0]}\"\n",
    "        ]\n",
    "    segs += [\n",
    "            # f\"Title: {passage_title}\",\n",
    "            f\"Background: {passage_context}\",\n",
    "            f\"Q: {question}\",\n",
    "            f\"A:\"\n",
    "    ]\n",
    "    return joiner.join(segs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "DRhoMeEGYre3"
   },
   "outputs": [],
   "source": [
    "def evaluate_few_shot_open_qa(\n",
    "        examples,\n",
    "        squad_train,\n",
    "        batch_size=20,\n",
    "        n_context=2,\n",
    "        joiner=\"\\n\\n\",\n",
    "        gen_func=run_eleuther):\n",
    "    \"\"\"Evaluate a few-shot OpenQA approach defined by \n",
    "    `build_few_shot_open_qa_prompt` and `gen_func`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    examples : iterable of SQuAD train examples\n",
    "        Presumably a subset of `squad_dev` as defined above.\n",
    "    squad_train : iterable of SQuAD train examples\n",
    "    batch_size : int\n",
    "        Number of examples to send to `gen_func` at once.\n",
    "    joiner : str\n",
    "        Used by `build_few_shot_open_qa_prompt` to join segments\n",
    "        of the prompt into a single str.\n",
    "    gen_func : either `run_eleuther` or `run_gpt3`\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict as determined by `evaluate` above.\n",
    "\n",
    "    \"\"\"\n",
    "    # A list of strings that you build and feed into `gen_func`.\n",
    "    prompts = []\n",
    "\n",
    "    # A list of dicts that you get from `gen_func`.\n",
    "    gens = []\n",
    "\n",
    "    # Iterate through the examples in batches:\n",
    "    for i in range(0, len(examples), batch_size):\n",
    "        # Use the `searcher` defined above to get passages\n",
    "        # using `ex.question` as the query, and use your\n",
    "        # `build_few_shot_open_qa_prompt` to build prompts.\n",
    "\n",
    "        ##### YOUR CODE HERE\n",
    "        \n",
    "        batch = examples[i: i+batch_size]\n",
    "\n",
    "        # sample training from squad_train\n",
    "        train_exs = random.sample(squad_train, k=n_context)\n",
    "\n",
    "        ## get a passage for each example in the dev batch\n",
    "        # get search results (passage index)\n",
    "        results = [searcher.search(ex.question, k=1) for ex in batch]\n",
    "\n",
    "        # from passage index to get the passage 'title | passage'\n",
    "        passages = [searcher.collection[r[0][0]] for r in results]\n",
    " \n",
    "        ps = []\n",
    "\n",
    "        # for every question, combine the find passage and generate the prompt\n",
    "        # append all prompt into a list\n",
    "        for ex, psg in zip(batch, passages):\n",
    "            ps.append(build_few_shot_open_qa_prompt(ex.question, psg, train_exs, joiner=joiner))  \n",
    "\n",
    "        # feed prompt to gen_func\n",
    "        gs = gen_func(ps)       \n",
    "\n",
    "        # add the prompt to prompt list\n",
    "        prompts += ps\n",
    "        # add generated txt to gen list\n",
    "        gens += gs\n",
    "\n",
    "\n",
    "    # Return value from a call to `evalaute`, with `examples`\n",
    "    # as provided by the user and the `prompts` and `gens`\n",
    "    # you built:\n",
    "    return evaluate(examples, prompts, gens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oXeQzplkYre3",
    "tags": []
   },
   "source": [
    "#### Answer scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "SwQVdCb6Yre4"
   },
   "outputs": [],
   "source": [
    "def get_passages_with_scores(question, searcher, k=5):\n",
    "    \"\"\"Pseudo-probabilities from the retriever.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    question : str\n",
    "    k : int\n",
    "        Number of passages to retrieve.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    passages (list of str), passage_probs (np.array)\n",
    "\n",
    "    \"\"\"\n",
    "    # Use the `searcher` to get `k` passages for `questions`:\n",
    "    ##### YOUR CODE HERE\n",
    "    search_score = searcher.search(question, k = k)[2]\n",
    "    passage_index = searcher.search(question, k = k)[0]\n",
    "\n",
    "    # Softmax normalize the scores and convert the list to\n",
    "    # a NumPy array:\n",
    "    ##### YOUR CODE HERE\n",
    "    exp_score = np.exp(search_score)\n",
    "    sum_score = np.sum(exp_score) \n",
    "    passage_probs = np.array([score/sum_score for score in exp_score] )\n",
    "\n",
    "    # Get the passages as a list of texts:\n",
    "    ##### YOUR CODE HERE\n",
    "\n",
    "    passages = [searcher.collection[idx] for idx in passage_index]\n",
    "\n",
    "    return passages, passage_probs\n",
    "\n",
    "\n",
    "from types import GeneratorType\n",
    "def answer_scoring(passages, passage_probs, prompts, gen_func=run_eleuther):\n",
    "    \"\"\"Implements our basic scoring strategy.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    passages : list of str\n",
    "    passage_probs : list of float\n",
    "    prompts : list of str\n",
    "    gen_func : either `run_eleuther` or `run_gpt3`\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list of pairs (score, dict), sorted with the largest score first.\n",
    "    `dict` should be the return value of `gen_func` for an example.\n",
    "\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    for passage, passage_prob, prompt in zip(passages, passage_probs, prompts):\n",
    "        # Run `gen_func` on [prompt] (crucially, the singleton list here),\n",
    "        # and get the dictionary `gen` from the singleton list `gen_func`\n",
    "        # returns, and then use the values to score `gen` according to our\n",
    "        # scoring method.\n",
    "        #\n",
    "        # Be sure to use \"generated_answer_probs\" for the scores.\n",
    "        ##### YOUR CODE HERE\n",
    "\n",
    "        gen = gen_func([prompt])\n",
    "\n",
    "        # print(gen)\n",
    "        \n",
    "        answer_score = np.prod(gen[0]['generated_answer_probs'])\n",
    "\n",
    "        final_score = passage_prob*answer_score\n",
    "        \n",
    "        data.append((final_score, gen[0]))\n",
    "\n",
    "\n",
    "    # Return `data`, sorted with the highest scoring `(score, gen)`\n",
    "    # pair given first.\n",
    "    ##### YOUR CODE HERE\n",
    "    data.sort(key = lambda x: x[0], reverse=True)\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### I. Baseline System (ColBERT straight output + Eleuther)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QzRjL61eYre0",
    "tags": []
   },
   "source": [
    "##### Load ColBERT index and initializing the searcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "id": "XtEGC6MyYre0",
    "outputId": "c6553c0f-598f-4d37-c96b-0766aaef1dfe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Jun 05, 22:47:02] #> Loading collection...\n",
      "0M \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Loaded 2,068 passages'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collection = os.path.join(index_home, \"bioasq.all.2bits\", \"bioasq_passage.tsv\")\n",
    "\n",
    "collection = Collection(path=collection)\n",
    "\n",
    "f'Loaded {len(collection):,} passages'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "BixmAYizYre0"
   },
   "outputs": [],
   "source": [
    "index_name = \"bioasq.all.2bits\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "83oO75Z1zTio"
   },
   "source": [
    "Now we create our `searcher`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N6wYpChzYre1",
    "outputId": "4436fe5f-55fa-43d0-ca4b-1d3a70c51d7e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Jun 05, 22:47:06] #> Loading collection...\n",
      "0M \n",
      "[Jun 05, 22:47:16] #> Building the emb2pid mapping..\n",
      "[Jun 05, 22:47:16] len(self.emb2pid) = 378124\n"
     ]
    }
   ],
   "source": [
    "with Run().context(RunConfig(experiment='bioasq')):\n",
    "    searcher = Searcher(index=index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2068"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(searcher.collection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Retiever evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## define the method-specific evaluation method\n",
    "def evaluate_retrieval_example(ex, k=20):    \n",
    "    results = searcher.search(ex.question, k=k)\n",
    "    for passage_id, passage_rank, passage_score in zip(*results):\n",
    "        passage = searcher.collection[passage_id]\n",
    "        score = has_answer([DPR_normalize(ans) for ans in ex.answers], passage)\n",
    "        if score:\n",
    "            return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#> QueryTokenizer.tensorize(batch_text[0], batch_background[0], bsize) ==\n",
      "#> Input: . Which is the enzymatic activity of OTULIN?, \t\t True, \t\t None\n",
      "#> Output IDs: torch.Size([32]), tensor([  101,     1,  2029,  2003,  1996,  4372,  9096, 12644,  4023,  1997,\n",
      "        27178, 18639,  1029,   102,   103,   103,   103,   103,   103,   103,\n",
      "          103,   103,   103,   103,   103,   103,   103,   103,   103,   103,\n",
      "          103,   103])\n",
      "#> Output Mask: torch.Size([32]), tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])\n",
      "\n",
      "0.7969788519637462\n",
      "CPU times: user 2min 7s, sys: 5.52 s, total: 2min 12s\n",
      "Wall time: 46.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if torch.cuda.is_available():\n",
    "    # This will take a few hours on a CPU:\n",
    "    print(success_at_k(bioasq_test))\n",
    "else:\n",
    "    # This should be reasonably fast and yields the\n",
    "    # same kind of result:\n",
    "    print(success_at_k(bioasq_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Run system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "          temperature 0.05 \n",
      "          Macro F1 is: 0.11406956518107561， \n",
      "          Exact Match: 0.024773413897280966， \n",
      "          Macro Precision is: 0.10246440534960173,\n",
      "          Macro Recall is: 0.20589445145434757,\n",
      "          \n",
      "CPU times: user 1h 50min 6s, sys: 4min 15s, total: 1h 54min 21s\n",
      "Wall time: 15min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "working_dataset = bioasq_test # bioasq_test vs dev_exs\n",
    "\n",
    "batch_size = 5\n",
    "joiner = '\\n\\n'\n",
    "# number of prompts\n",
    "n_context = 2\n",
    "\n",
    "# temperatures = [0.01, 0.025, 0.05, 0.075]\n",
    "temperatures = [0.025]\n",
    "# bioasq_dev\n",
    "# bioasq_train\n",
    "\n",
    "for temperature in temperatures:\n",
    "    \n",
    "    prompts = []\n",
    "\n",
    "    gens = []\n",
    "\n",
    "    for i in range(0, len(working_dataset), batch_size):\n",
    "        # Use the `searcher` defined above to get passages\n",
    "        # using `ex.question` as the query, and use your\n",
    "        # `build_few_shot_open_qa_prompt` to build prompts.\n",
    "        \n",
    "        # get a batch from bioasq dev (to replace dev_exs)\n",
    "        batch = working_dataset[i: i+batch_size]\n",
    "\n",
    "        train_exs = random.sample(bioasq_train, k=n_context)\n",
    "\n",
    "        ## get a passage for each example in the dev batch\n",
    "        # get search results (passage index) for all examples in the batch\n",
    "        # k = 1 because we choose the top result\n",
    "        results = [searcher.search(ex.question, k=1) for ex in batch]\n",
    "\n",
    "        # from passage index to get the passage 'title | passage'\n",
    "        ## collection contains all the passage avaiable (train+ test + dev)\n",
    "        passages = [searcher.collection[r[0][0]] for r in results]\n",
    "\n",
    "        # re-initiating prompt\n",
    "        ps = []\n",
    "\n",
    "        # for every question in the batch, combine the train_exs (background + q +a) + found passage + question and generate the prompt\n",
    "        # append all prompt into a list\n",
    "        \n",
    "        for ex, psg in zip(batch, passages):\n",
    "            ps.append(build_few_shot_open_qa_prompt(ex.question, psg, train_exs, joiner=joiner))  \n",
    "\n",
    "        # feed prompts (in list of prompts) to gen_func\n",
    "        gs = run_eleuther(ps)       \n",
    "\n",
    "        # add the prompt to prompt list\n",
    "        prompts += ps\n",
    "        # add generated txt to gen list\n",
    "        gens += gs\n",
    "    \n",
    "    eva = evaluate(working_dataset, prompts, gens)\n",
    "    # print(eva)\n",
    "    print(f\"\"\"\n",
    "          temperature {temperature} \n",
    "          Macro F1 is: {eva['macro_f1']}， \n",
    "          Exact Match: {eva['em_per']}， \n",
    "          Macro Precision is: {eva['macro_precision']},\n",
    "          Macro Recall is: {eva['macro_recall']},\n",
    "          \"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### II. ColBERT improvement (normalization/answer scoring) + Eleuther"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "id": "D9pfDIrPYre4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "          temperature 0.025 \n",
      "          Macro F1 is: 0.1641240799601087， \n",
      "          Exact Match: 0.07734138972809668， \n",
      "          Macro Precision is: 0.17187570066724142,\n",
      "          Macro Recall is: 0.22683778416752035,\n",
      "          \n",
      "CPU times: user 12h 8min 59s, sys: 8min 27s, total: 12h 17min 26s\n",
      "Wall time: 1h 34min 8s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "######## This part is functional modules ############\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "#### enhanced squad training example searching\n",
    "\n",
    "def train_tf_idf(bioasq_train):\n",
    "    tfidfvectorizer = TfidfVectorizer(analyzer='word',stop_words= 'english', ngram_range=(1, 3))\n",
    "\n",
    "  # append all context\n",
    "    train_context = [x.context for x in bioasq_train]\n",
    "\n",
    "    tfidfvectorizer.fit_transform(train_context)\n",
    "\n",
    "    context_tfidf = tfidfvectorizer.transform(train_context)\n",
    "\n",
    "    return tfidfvectorizer, context_tfidf\n",
    "\n",
    "def sample_bioasq_train(tfidfvectorizer, context_tfidf, question, n_context):\n",
    "    '''\n",
    "    This is using tf-idf and consine similarity to sample \"related to question\" bioasq example to build the prompt\n",
    "    '''\n",
    "    question_tfidf = tfidfvectorizer.transform([question])\n",
    "\n",
    "    cosine_sim = cosine_similarity(context_tfidf, question_tfidf).flatten()\n",
    "\n",
    "    related_index = cosine_sim.argsort()[-n_context:][::-1]\n",
    "\n",
    "    train_exs = [bioasq_train[i] for i in related_index]\n",
    "\n",
    "    return train_exs\n",
    "\n",
    "### revised answer scoring by normalizing the score by length\n",
    "from types import GeneratorType\n",
    "## added temperature arg to allow change\n",
    "def answer_scoring_normalized(passages, passage_probs, prompts, temperature, gen_func=run_eleuther):\n",
    "    \"\"\"Implements our basic scoring strategy.\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  passages : list of str\n",
    "  passage_probs : list of float\n",
    "  prompts : list of str\n",
    "  gen_func : either `run_eleuther` or `run_gpt3`\n",
    "\n",
    "  Returns\n",
    "  -------\n",
    "  list of pairs (score, dict), sorted with the largest score first.\n",
    "  `dict` should be the return value of `gen_func` for an example.\n",
    "\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    length_sum = 0\n",
    "    gen_list = []\n",
    "\n",
    "    for passage, passage_prob, prompt in zip(passages, passage_probs, prompts):\n",
    "        gen = gen_func([prompt], temperature = temperature)\n",
    "\n",
    "        gen_list.append(gen)\n",
    "        # calculate the total length of answers\n",
    "        length_sum += len(gen[0]['generated_answer'].split(' '))\n",
    "\n",
    "    for passage_prob, gen in zip(passage_probs, gen_list):\n",
    "\n",
    "        answer_score = np.prod(gen[0]['generated_answer_probs'])\n",
    "\n",
    "        length_of_answer = len(gen[0]['generated_answer'].split(' '))\n",
    "\n",
    "        # give more weight to longer answers, as its product of per-token probabiliyy is underdog\n",
    "        weight = length_of_answer/length_sum\n",
    "\n",
    "        final_score = passage_prob*answer_score*weight\n",
    "\n",
    "        data.append((final_score, gen[0]))\n",
    "\n",
    "    data.sort(key = lambda x: x[0], reverse=True)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "######## This part is system development ############\n",
    "\n",
    "batch_size = 5\n",
    "joiner = '\\n\\n'\n",
    "n_context = 2\n",
    "\n",
    "# temperatures = [0.01, 0.025, 0.05, 0.075]\n",
    "temperatures = [0.025]\n",
    "\n",
    "\n",
    "working_dataset = bioasq_test # bioasq_test vs dev_exs\n",
    "\n",
    "for temperature in temperatures:\n",
    "    prompts = []\n",
    "\n",
    "    gens = []\n",
    "\n",
    "    # use tf-idf to find \"related few shot in bioasq to build the prompt\n",
    "    # train tf-idf on all bioasq examples\n",
    "    tfidfvectorizer, context_tfidf = train_tf_idf(bioasq_train)\n",
    "\n",
    "    for i in range(0, len(working_dataset), batch_size):\n",
    "        # Use the `searcher` defined above to get passages\n",
    "        # using `ex.question` as the query, and use your\n",
    "        # `build_few_shot_open_qa_prompt` to build prompts.\n",
    "\n",
    "        # get a batch from bioasq dev (to replace dev_exs)\n",
    "        batch = working_dataset[i: i+batch_size]\n",
    "\n",
    "        # train_exs = random.sample(bioasq_train, k=n_context)\n",
    "\n",
    "        ## score for answer-passage pair\n",
    "        for ex in batch:\n",
    "\n",
    "          # use tf idf to sample training exs, instead of just random sampling bioasq training\n",
    "            train_exs = sample_bioasq_train(tfidfvectorizer, context_tfidf, ex.question, n_context)\n",
    "\n",
    "            passages, passage_probs = get_passages_with_scores(ex.question)\n",
    "\n",
    "            # re-initiating prompt\n",
    "            ps = []\n",
    "            # iterate through each passage in the top k (5) passages\n",
    "            for psg in passages:\n",
    "            # build the prompt based on question, that specific passge, and training examples\n",
    "            # say we have passage, then ps will be ['prompt1', 'prompt2', 'prompt3', 'prompt4', 'prompt5']\n",
    "                ps.append(build_few_shot_open_qa_prompt(ex.question, psg, train_exs, joiner=joiner)) \n",
    "\n",
    "          # calculate the answering score for the highest passage-answer pair                 \n",
    "          # data = answer_scoring(passages,       # only related to question, same length as ps\n",
    "          #                       passage_probs,  # only related to question, same length as ps\n",
    "          #                       ps,             # k prompts\n",
    "          #                       run_eleuther)\n",
    "\n",
    "            data = answer_scoring_normalized(passages,       # only related to question, same length as ps\n",
    "                                passage_probs,  # only related to question, same length as ps\n",
    "                                ps,             # k prompts\n",
    "                                temperature,\n",
    "                                run_eleuther)\n",
    "\n",
    "            # pick highest score answer-prompt pair (note: in)\n",
    "            highest_gs = [data[0][1]]\n",
    "            highest_ps = [data[0][1]['prompt']]\n",
    "\n",
    "            # add the prompt to prompt list\n",
    "            prompts += highest_ps\n",
    "\n",
    "            # add generated txt to gen list\n",
    "            gens += highest_gs\n",
    " \n",
    "    eva = evaluate(working_dataset, prompts, gens)\n",
    "    \n",
    "    print(f\"\"\"\n",
    "          temperature {temperature} \n",
    "          Macro F1 is: {eva['macro_f1']}， \n",
    "          Exact Match: {eva['em_per']}， \n",
    "          Macro Precision is: {eva['macro_precision']},\n",
    "          Macro Recall is: {eva['macro_recall']},\n",
    "          \"\"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nWqnZmNeYG6S",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### III. DPR Model for retrieval + LM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DPRContextEncoder, AutoModel\n",
    "\n",
    "context_tokenizer = AutoTokenizer.from_pretrained(\"facebook/dpr-ctx_encoder-single-nq-base\")\n",
    "\n",
    "encode_context_model = DPRContextEncoder.from_pretrained(\"facebook/dpr-ctx_encoder-single-nq-base\")\n",
    "\n",
    "question_tokenizer = AutoTokenizer.from_pretrained(\"facebook/dpr-question_encoder-single-nq-base\")\n",
    "\n",
    "encode_question_model = AutoModel.from_pretrained(\"facebook/dpr-question_encoder-single-nq-base\")\n",
    "\n",
    "encode_context_model.to(device)\n",
    "encode_question_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# create passage embedding for the following analysis\n",
    "\n",
    "## make all passage embeddings and get their id\n",
    "context_all = []\n",
    "\n",
    "encode_context_model.eval()\n",
    "\n",
    "\n",
    "for i in range(len(bioasq_list)):\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        context_input_ids = context_tokenizer(bioasq_list[i]['context'], return_tensors=\"pt\")[\"input_ids\"].to(device)\n",
    "        context_embeddings = encode_context_model(context_input_ids).pooler_output.squeeze()\n",
    "\n",
    "        context_all.append(context_embeddings)\n",
    "\n",
    "        torch.cuda.empty_cache() # probably redundant\n",
    "    \n",
    "    if i % 100 == 0:\n",
    "        print(i)\n",
    "\n",
    "context_all_tensor = torch.stack(context_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "          temperature 0.025 \n",
      "          Macro F1 is: 0.0806630306815693， \n",
      "          Exact Match: 0.009667673716012085， \n",
      "          Macro Precision is: 0.0759503906633816,\n",
      "          Macro Recall is: 0.14150972153460456,\n",
      "          \n",
      "CPU times: user 2h 33min 52s, sys: 32min 50s, total: 3h 6min 43s\n",
      "Wall time: 46min 40s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "working_dataset = bioasq_test # bioasq_test vs dev_exs\n",
    "\n",
    "batch_size = 5\n",
    "joiner = '\\n\\n'\n",
    "# number of prompts\n",
    "n_context = 2\n",
    "\n",
    "# temperatures = [0.01, 0.025, 0.05, 0.075]\n",
    "temperatures = [0.025]\n",
    "\n",
    "encode_question_model.eval()\n",
    "\n",
    "for temperature in temperatures:\n",
    "    \n",
    "    prompts = []\n",
    "\n",
    "    gens = []\n",
    "\n",
    "    for i in range(0, len(working_dataset), batch_size):\n",
    "        # Use the `searcher` defined above to get passages\n",
    "        # using `ex.question` as the query, and use your\n",
    "        # `build_few_shot_open_qa_prompt` to build prompts.\n",
    "        \n",
    "        # get a batch from bioasq dev (to replace dev_exs)\n",
    "        batch = working_dataset[i: i+batch_size]\n",
    "\n",
    "        train_exs = random.sample(bioasq_train, k=n_context)\n",
    "\n",
    "        ## get a passage for each example in the dev batch\n",
    "        # get search results (passage index) for all examples in the batch\n",
    "        # k = 1 because we choose the top result\n",
    "        \n",
    "        passages = []\n",
    "        \n",
    "        for ex in batch:\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                \n",
    "                # encode question\n",
    "                question_input_ids = question_tokenizer(ex.question, return_tensors=\"pt\")[\"input_ids\"]\n",
    "                question_embeddings = encode_question_model(question_input_ids).pooler_output.squeeze()\n",
    "                \n",
    "                # get the dot product (score and sort it)\n",
    "                dot_products = torch.sum(context_all_tensor * question_embeddings, -1)\n",
    "                new_dot_products = torch.sort(dot_products, dim=- 1, descending=True)\n",
    "                \n",
    "                # retrieve the passage from all evaialbe passages\n",
    "                passages.append(bioasq_list[new_dot_products.indices[0]]['context'])\n",
    "\n",
    "        # re-initiating prompt\n",
    "        ps = []\n",
    "\n",
    "        # for every question in the batch, combine the train_exs (background + q +a) + found passage + question and generate the prompt\n",
    "        # append all prompt into a list\n",
    "        \n",
    "        for ex, psg in zip(batch, passages):\n",
    "            ps.append(build_few_shot_open_qa_prompt(ex.question, psg, train_exs, joiner=joiner))  \n",
    "\n",
    "        # feed prompts (in list of prompts) to gen_func\n",
    "        gs = run_eleuther(ps)       \n",
    "\n",
    "        # add the prompt to prompt list\n",
    "        prompts += ps\n",
    "        # add generated txt to gen list\n",
    "        gens += gs\n",
    "    \n",
    "    eva = evaluate(working_dataset, prompts, gens)\n",
    "    # print(eva)\n",
    "    print(f\"\"\"\n",
    "          temperature {temperature} \n",
    "          Macro F1 is: {eva['macro_f1']}， \n",
    "          Exact Match: {eva['em_per']}， \n",
    "          Macro Precision is: {eva['macro_precision']},\n",
    "          Macro Recall is: {eva['macro_recall']},\n",
    "          \"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Retrieval Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "def success_at_k(examples, k=20):\n",
    "    scores = []\n",
    "    for ex in examples: \n",
    "        scores.append(evaluate_retrieval_example(ex, k=5))\n",
    "    return sum(scores) / len(scores)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_retrieval_example(ex, k=20): \n",
    "        \n",
    "    with torch.no_grad():\n",
    "\n",
    "        # encode question\n",
    "        question_input_ids = question_tokenizer(ex.question, return_tensors=\"pt\")[\"input_ids\"].to(device)\n",
    "        question_embeddings = encode_question_model(question_input_ids).pooler_output.squeeze()\n",
    "\n",
    "        # get the dot product (score and sort it)\n",
    "        dot_products = torch.sum(context_all_tensor * question_embeddings, -1)\n",
    "        new_dot_products = torch.sort(dot_products, dim=- 1, descending=True)\n",
    "\n",
    "        # retrieve the passage from all evaialbe passages\n",
    "        inx_for_retrieval = new_dot_products.indices[:k]\n",
    "        \n",
    "        passages = [bioasq_list[i]['context'] for i in inx_for_retrieval]\n",
    "            \n",
    "        # print(ex.question)\n",
    "        # print(ex.answers)\n",
    "        # print(passages)\n",
    "        for passage in passages:\n",
    "            score = has_answer([DPR_normalize(ans) for ans in ex.answers], passage)\n",
    "\n",
    "            if score:\n",
    "                return 1\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_retrieval_example(dev_exs[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.518429003021148\n",
      "CPU times: user 22.7 s, sys: 14.3 ms, total: 22.7 s\n",
      "Wall time: 22.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if torch.cuda.is_available():\n",
    "    # This will take a few hours on a CPU:\n",
    "    print(success_at_k(bioasq_test))\n",
    "else:\n",
    "    # This should be reasonably fast and yields the\n",
    "    # same kind of result:\n",
    "    print(success_at_k(bioasq_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### IV. BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "          temperature 0.025 \n",
      "          Macro F1 is: 0.09281831278566667， \n",
      "          Exact Match: 0.021148036253776436， \n",
      "          Macro Precision is: 0.08666789571623408,\n",
      "          Macro Recall is: 0.16364468951668765,\n",
      "          \n",
      "CPU times: user 1h 48min 39s, sys: 33min 12s, total: 2h 21min 51s\n",
      "Wall time: 18min 38s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "working_dataset = bioasq_test # bioasq_test vs dev_exs\n",
    "\n",
    "batch_size = 5\n",
    "joiner = '\\n\\n'\n",
    "# number of prompts\n",
    "n_context = 2\n",
    "\n",
    "# temperatures = [0.01, 0.025, 0.05, 0.075]\n",
    "temperatures = [0.025]\n",
    "\n",
    "## prepare BM25\n",
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "tokenized_corpus = [example['context'].split(\" \") for example in bioasq_list]\n",
    "\n",
    "bm25 = BM25Okapi(tokenized_corpus)\n",
    "\n",
    "for temperature in temperatures:\n",
    "    \n",
    "    prompts = []\n",
    "\n",
    "    gens = []\n",
    "\n",
    "    for i in range(0, len(working_dataset), batch_size):\n",
    "        # Use the `searcher` defined above to get passages\n",
    "        # using `ex.question` as the query, and use your\n",
    "        # `build_few_shot_open_qa_prompt` to build prompts.\n",
    "        \n",
    "        # get a batch from bioasq dev (to replace dev_exs)\n",
    "        batch = working_dataset[i: i+batch_size]\n",
    "\n",
    "        train_exs = random.sample(bioasq_train, k=n_context)\n",
    "\n",
    "        ## get a passage for each example in the dev batch\n",
    "        # get search results (passage index) for all examples in the batch\n",
    "        # k = 1 because we choose the top result\n",
    "        \n",
    "        passages = []\n",
    "        \n",
    "        for ex in batch:\n",
    "                \n",
    "            tokenized_query = ex.question.split(\" \")\n",
    "            \n",
    "            # retrieve the top one passage with question added\n",
    "            # doc_scores = bm25.get_scores(tokenized_query)\n",
    "            passage = bm25.get_top_n(tokenized_query, bioasq_list, n=1)[0]['context']\n",
    "\n",
    "            # retrieve the passage\n",
    "            passages.append(passage)\n",
    "\n",
    "        # re-initiating prompt\n",
    "        ps = []\n",
    "\n",
    "        # for every question in the batch, combine the train_exs (background + q +a) + found passage + question and generate the prompt\n",
    "        # append all prompt into a list\n",
    "        \n",
    "        for ex, psg in zip(batch, passages):\n",
    "            ps.append(build_few_shot_open_qa_prompt(ex.question, psg, train_exs, joiner=joiner))  \n",
    "\n",
    "        # feed prompts (in list of prompts) to gen_func\n",
    "        gs = run_eleuther(ps)       \n",
    "\n",
    "        # add the prompt to prompt list\n",
    "        prompts += ps\n",
    "        # add generated txt to gen list\n",
    "        gens += gs\n",
    "    \n",
    "    eva = evaluate(working_dataset, prompts, gens)\n",
    "    # print(eva)\n",
    "    print(f\"\"\"\n",
    "          temperature {temperature} \n",
    "          Macro F1 is: {eva['macro_f1']}， \n",
    "          Exact Match: {eva['em_per']}， \n",
    "          Macro Precision is: {eva['macro_precision']},\n",
    "          Macro Recall is: {eva['macro_recall']},\n",
    "          \"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##### Retriever's Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "tokenized_corpus = [example['context'].split(\" \") for example in bioasq_list]\n",
    "\n",
    "bm25 = BM25Okapi(tokenized_corpus)\n",
    "\n",
    "def evaluate_retrieval_example(ex, k=20): \n",
    "        \n",
    "    tokenized_query = ex.question.split(\" \")\n",
    "            \n",
    "    # retrieve the top one passage with question added\n",
    "    # doc_scores = bm25.get_scores(tokenized_query)\n",
    "    \n",
    "    passages = [ example['context'] for example in bm25.get_top_n(tokenized_query, bioasq_list, n=k)]\n",
    "        \n",
    "\n",
    "    # print(ex.question)\n",
    "    # print(ex.answers)\n",
    "    # print(passages)\n",
    "    for passage in passages:\n",
    "        score = has_answer([DPR_normalize(ans) for ans in ex.answers], passage)\n",
    "\n",
    "        if score:\n",
    "            return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_retrieval_example(dev_exs[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5915407854984894\n",
      "CPU times: user 11.2 s, sys: 3.01 ms, total: 11.2 s\n",
      "Wall time: 11.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if torch.cuda.is_available():\n",
    "    # This will take a few hours on a CPU:\n",
    "    print(success_at_k(bioasq_test))\n",
    "else:\n",
    "    # This should be reasonably fast and yields the\n",
    "    # same kind of result:\n",
    "    print(success_at_k(bioasq_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Doc2Query Augumentation Group\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Enhance the context by Doc2Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11.8 ms, sys: 3.9 ms, total: 15.7 ms\n",
      "Wall time: 28.3 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import torch\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "import copy\n",
    "import pickle\n",
    "\n",
    "if not os.path.exists('experiments/bioasq/enhnaced_all_data.pickle'):\n",
    "    \n",
    "    d2q_tokenizer = T5Tokenizer.from_pretrained('castorini/doc2query-t5-base-msmarco')\n",
    "    d2q_model = T5ForConditionalGeneration.from_pretrained('castorini/doc2query-t5-base-msmarco')\n",
    "    d2q_model.to(device)\n",
    "\n",
    "    bioasq_list_copy = copy.deepcopy(bioasq_list)\n",
    "\n",
    "    ## this is to use doc2query to find additional question from MS Macro and append the top 10 \n",
    "    ## questions back to the context\n",
    "    d2q_model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for i in range(len(bioasq_list_copy)):\n",
    "\n",
    "            doc_text = bioasq_list_copy[i]['context']\n",
    "\n",
    "            input_ids = d2q_tokenizer.encode(doc_text, return_tensors='pt').to(device)\n",
    "\n",
    "            number_of_q = 10\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = d2q_model.generate(\n",
    "                    input_ids=input_ids,\n",
    "                    max_length=64,\n",
    "                    do_sample=True,\n",
    "                    top_k=10,\n",
    "                    num_return_sequences=number_of_q)\n",
    "\n",
    "            query_to_append = ' '\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for j in range(number_of_q):\n",
    "                    query_to_append += d2q_tokenizer.decode(outputs[j], skip_special_tokens=True) + ' '\n",
    "\n",
    "            # append the query to the passage\n",
    "\n",
    "            bioasq_list_copy[i]['context'] = bioasq_list_copy[i]['context'] + query_to_append\n",
    "\n",
    "            # torch.cuda.empty_cache() # probably redundant\n",
    "    \n",
    "    # save enhance dataset as pickle\n",
    "    with open('experiments/bioasq/enhnaced_all_data.pickle', 'wb') as handle:\n",
    "        pickle.dump(bioasq_list_copy, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "            \n",
    "else:\n",
    "    # load enhanced dataset\n",
    "    with open('experiments/bioasq/enhnaced_all_data.pickle', 'rb') as handle:\n",
    "        bioasq_list_copy = pickle.load(handle)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "## split dev and test\n",
    "\n",
    "bioasq_train_enh, bioasq_dev_enh, bioasq_test_enh = get_bioasq_split(\n",
    "    bioasq_list_copy, \n",
    "    random_state=40\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206, 207, 1655 \n"
     ]
    }
   ],
   "source": [
    "print(f\"{len(bioasq_train_enh)}, {len(bioasq_dev_enh)}, {len(bioasq_test_enh)} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# check the split it correct\n",
    "count= 0\n",
    "for i in range(len(bioasq_train_enh)):\n",
    "    count += bioasq_train_enh[i].id != bioasq_train[i].id \n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "## pick 10 just for sanity check\n",
    "dev_exs_enh = bioasq_dev_enh[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_exs_enh[19]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### V. Doc2query+ BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "          temperature 0.025 \n",
      "          Macro F1 is: 0.08072085112218676， \n",
      "          Exact Match: 0.015105740181268883， \n",
      "          Macro Precision is: 0.07526040424831965,\n",
      "          Macro Recall is: 0.14252709122749127,\n",
      "          \n",
      "CPU times: user 3h 1min 42s, sys: 27min 50s, total: 3h 29min 32s\n",
      "Wall time: 52min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "working_dataset = bioasq_test # bioasq_test vs dev_exs\n",
    "\n",
    "batch_size = 5\n",
    "joiner = '\\n\\n'\n",
    "# number of prompts\n",
    "n_context = 2\n",
    "\n",
    "# temperatures = [0.01, 0.025, 0.05, 0.075]\n",
    "temperatures = [0.025]\n",
    "\n",
    "## prepare BM25\n",
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "tokenized_corpus = [example['context'].split(\" \") for example in bioasq_list_copy]\n",
    "\n",
    "bm25 = BM25Okapi(tokenized_corpus)\n",
    "\n",
    "for temperature in temperatures:\n",
    "    \n",
    "    prompts = []\n",
    "\n",
    "    gens = []\n",
    "\n",
    "    for i in range(0, len(working_dataset), batch_size):\n",
    "        # Use the `searcher` defined above to get passages\n",
    "        # using `ex.question` as the query, and use your\n",
    "        # `build_few_shot_open_qa_prompt` to build prompts.\n",
    "        \n",
    "        # get a batch from bioasq dev (to replace dev_exs)\n",
    "        batch = working_dataset[i: i+batch_size]\n",
    "\n",
    "        train_exs = random.sample(bioasq_train, k=n_context)\n",
    "\n",
    "        ## get a passage for each example in the dev batch\n",
    "        # get search results (passage index) for all examples in the batch\n",
    "        # k = 1 because we choose the top result\n",
    "        \n",
    "        passages = []\n",
    "        \n",
    "        for ex in batch:\n",
    "                \n",
    "            tokenized_query = ex.question.split(\" \")\n",
    "            \n",
    "            # retrieve the top one passage with question added\n",
    "            # doc_scores = bm25.get_scores(tokenized_query)\n",
    "            passage = bm25.get_top_n(tokenized_query, bioasq_list_copy, n=1)[0]['context']\n",
    "\n",
    "            # retrieve the passage\n",
    "            passages.append(passage)\n",
    "\n",
    "        # re-initiating prompt\n",
    "        ps = []\n",
    "\n",
    "        # for every question in the batch, combine the train_exs (background + q +a) + found passage + question and generate the prompt\n",
    "        # append all prompt into a list\n",
    "        \n",
    "        for ex, psg in zip(batch, passages):\n",
    "            ps.append(build_few_shot_open_qa_prompt(ex.question, psg, train_exs, joiner=joiner))  \n",
    "\n",
    "        # feed prompts (in list of prompts) to gen_func\n",
    "        gs = run_eleuther(ps)       \n",
    "\n",
    "        # add the prompt to prompt list\n",
    "        prompts += ps\n",
    "        # add generated txt to gen list\n",
    "        gens += gs\n",
    "    \n",
    "    eva = evaluate(working_dataset, prompts, gens)\n",
    "    # print(eva)\n",
    "    print(f\"\"\"\n",
    "          temperature {temperature} \n",
    "          Macro F1 is: {eva['macro_f1']}， \n",
    "          Exact Match: {eva['em_per']}， \n",
    "          Macro Precision is: {eva['macro_precision']},\n",
    "          Macro Recall is: {eva['macro_recall']},\n",
    "          \"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Retriever's Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "tokenized_corpus = [example['context'].split(\" \") for example in bioasq_list_copy]\n",
    "\n",
    "bm25 = BM25Okapi(tokenized_corpus)\n",
    "\n",
    "def evaluate_retrieval_example(ex, k=20): \n",
    "        \n",
    "    tokenized_query = ex.question.split(\" \")\n",
    "            \n",
    "    # retrieve the top one passage with question added\n",
    "    # doc_scores = bm25.get_scores(tokenized_query)\n",
    "    \n",
    "    passages = [ example['context'] for example in bm25.get_top_n(tokenized_query, bioasq_list_copy, n=k)]\n",
    "        \n",
    "\n",
    "    # print(ex.question)\n",
    "    # print(ex.answers)\n",
    "    # print(passages)\n",
    "    for passage in passages:\n",
    "        score = has_answer([DPR_normalize(ans) for ans in ex.answers], passage)\n",
    "\n",
    "        if score:\n",
    "            return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_retrieval_example(dev_exs_enh[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6392749244712991\n",
      "CPU times: user 12.2 s, sys: 2.66 ms, total: 12.2 s\n",
      "Wall time: 12.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if torch.cuda.is_available():\n",
    "    # This will take a few hours on a CPU:\n",
    "    print(success_at_k(bioasq_test_enh))\n",
    "else:\n",
    "    # This should be reasonably fast and yields the\n",
    "    # same kind of result:\n",
    "    print(success_at_k(bioasq_test_enh))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### VI.Doc2query+ DPR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DPRContextEncoder, AutoModel\n",
    "\n",
    "context_tokenizer = AutoTokenizer.from_pretrained(\"facebook/dpr-ctx_encoder-single-nq-base\")\n",
    "\n",
    "encode_context_model = DPRContextEncoder.from_pretrained(\"facebook/dpr-ctx_encoder-single-nq-base\")\n",
    "\n",
    "question_tokenizer = AutoTokenizer.from_pretrained(\"facebook/dpr-question_encoder-single-nq-base\")\n",
    "\n",
    "encode_question_model = AutoModel.from_pretrained(\"facebook/dpr-question_encoder-single-nq-base\")\n",
    "\n",
    "encode_context_model.to(device)\n",
    "encode_question_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# create passage embedding for the following analysis\n",
    "\n",
    "## make all passage embeddings and get their id\n",
    "context_all = []\n",
    "\n",
    "encode_context_model.eval()\n",
    "\n",
    "\n",
    "for i in range(len(bioasq_list_copy)):\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        context_input_ids = context_tokenizer(bioasq_list_copy[i]['context'], return_tensors=\"pt\")[\"input_ids\"].to(device)\n",
    "        context_embeddings = encode_context_model(context_input_ids).pooler_output.squeeze()\n",
    "\n",
    "        context_all.append(context_embeddings)\n",
    "\n",
    "        # torch.cuda.empty_cache() # probably redundant\n",
    "    \n",
    "    if i % 100 == 0:\n",
    "        print(i)\n",
    "\n",
    "context_all_tensor = torch.stack(context_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "          temperature 0.025 \n",
      "          Macro F1 is: 0.08028451415538838， \n",
      "          Exact Match: 0.012084592145015106， \n",
      "          Macro Precision is: 0.07447420823553755,\n",
      "          Macro Recall is: 0.1411964309837065,\n",
      "          \n",
      "CPU times: user 2h 30min 16s, sys: 1h 23s, total: 3h 30min 40s\n",
      "Wall time: 28min 18s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "working_dataset = bioasq_test_enh # bioasq_test vs dev_exs vs bioasq_dev\n",
    "\n",
    "batch_size = 5\n",
    "joiner = '\\n\\n'\n",
    "# number of prompts\n",
    "n_context = 2\n",
    "\n",
    "# temperatures = [0.01, 0.025, 0.05, 0.075]\n",
    "temperatures = [0.025]\n",
    "\n",
    "encode_question_model.eval()\n",
    "\n",
    "for temperature in temperatures:\n",
    "    \n",
    "    prompts = []\n",
    "\n",
    "    gens = []\n",
    "\n",
    "    for i in range(0, len(working_dataset), batch_size):\n",
    "        # Use the `searcher` defined above to get passages\n",
    "        # using `ex.question` as the query, and use your\n",
    "        # `build_few_shot_open_qa_prompt` to build prompts.\n",
    "        \n",
    "        # get a batch from bioasq dev (to replace dev_exs)\n",
    "        batch = working_dataset[i: i+batch_size]\n",
    "\n",
    "        train_exs = random.sample(bioasq_train_enh, k=n_context)\n",
    "\n",
    "        ## get a passage for each example in the dev batch\n",
    "        # get search results (passage index) for all examples in the batch\n",
    "        # k = 1 because we choose the top result\n",
    "        \n",
    "        passages = []\n",
    "        \n",
    "        for ex in batch:\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                \n",
    "                # encode question\n",
    "                question_input_ids = question_tokenizer(ex.question, return_tensors=\"pt\")[\"input_ids\"].to(device)\n",
    "                question_embeddings = encode_question_model(question_input_ids).pooler_output.squeeze()\n",
    "                \n",
    "                # get the dot product (score and sort it)\n",
    "                dot_products = torch.sum(context_all_tensor * question_embeddings, -1)\n",
    "                new_dot_products = torch.sort(dot_products, dim=- 1, descending=True)\n",
    "                \n",
    "                # retrieve the passage from all evaialbe passages\n",
    "                passages.append(bioasq_list_copy[new_dot_products.indices[0]]['context'])\n",
    "\n",
    "        # re-initiating prompt\n",
    "        ps = []\n",
    "\n",
    "        # for every question in the batch, combine the train_exs (background + q +a) + found passage + question and generate the prompt\n",
    "        # append all prompt into a list\n",
    "        \n",
    "        for ex, psg in zip(batch, passages):\n",
    "            ps.append(build_few_shot_open_qa_prompt(ex.question, psg, train_exs, joiner=joiner))  \n",
    "\n",
    "        # feed prompts (in list of prompts) to gen_func\n",
    "        gs = run_eleuther(ps)       \n",
    "\n",
    "        # add the prompt to prompt list\n",
    "        prompts += ps\n",
    "        # add generated txt to gen list\n",
    "        gens += gs\n",
    "    \n",
    "    eva = evaluate(working_dataset, prompts, gens)\n",
    "    # print(eva)\n",
    "    print(f\"\"\"\n",
    "          temperature {temperature} \n",
    "          Macro F1 is: {eva['macro_f1']}， \n",
    "          Exact Match: {eva['em_per']}， \n",
    "          Macro Precision is: {eva['macro_precision']},\n",
    "          Macro Recall is: {eva['macro_recall']},\n",
    "          \"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Retrieval Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "def success_at_k(examples, k=20):\n",
    "    scores = []\n",
    "    for ex in examples: \n",
    "        scores.append(evaluate_retrieval_example(ex, k=5))\n",
    "    return sum(scores) / len(scores)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_retrieval_example(ex, k=20): \n",
    "        \n",
    "    with torch.no_grad():\n",
    "\n",
    "        # encode question\n",
    "        question_input_ids = question_tokenizer(ex.question, return_tensors=\"pt\")[\"input_ids\"].to(device)\n",
    "        question_embeddings = encode_question_model(question_input_ids).pooler_output.squeeze()\n",
    "\n",
    "        # get the dot product (score and sort it)\n",
    "        dot_products = torch.sum(context_all_tensor * question_embeddings, -1)\n",
    "        new_dot_products = torch.sort(dot_products, dim=- 1, descending=True)\n",
    "\n",
    "        # retrieve the passage from all evaialbe passages\n",
    "        inx_for_retrieval = new_dot_products.indices[:k]\n",
    "        \n",
    "        passages = [bioasq_list_copy[i]['context'] for i in inx_for_retrieval]\n",
    "            \n",
    "        # print(ex.question)\n",
    "        # print(ex.answers)\n",
    "        # print(passages)\n",
    "        for passage in passages:\n",
    "            score = has_answer([DPR_normalize(ans) for ans in ex.answers], passage)\n",
    "\n",
    "            if score:\n",
    "                return 1\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_retrieval_example(dev_exs[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5444108761329305\n",
      "CPU times: user 23.8 s, sys: 22.7 ms, total: 23.8 s\n",
      "Wall time: 23.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if torch.cuda.is_available():\n",
    "    # This will take a few hours on a CPU:\n",
    "    print(success_at_k(bioasq_test_enh))\n",
    "else:\n",
    "    # This should be reasonably fast and yields the\n",
    "    # same kind of result:\n",
    "    print(success_at_k(bioasq_test_enh))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### VIII. Doc2query + ColBERT Base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##### Creating new indexer (one time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "passages_enh = []\n",
    "for i in range(len(bioasq_list_copy)):\n",
    "    passages_enh.append((i, bioasq_list_copy[i]['context']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "## need to use a new index that is based on the ehnanced dataset\n",
    "import csv\n",
    "with open('experiments/bioasq_passage_enh.tsv', 'w+', newline='') as f_output:\n",
    "    csv_output = csv.writer(f_output, delimiter='\\t')\n",
    "\n",
    "    csv_output.writerows(passages_enh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from colbert.infra import Run, RunConfig, ColBERTConfig\n",
    "from colbert.data import Queries, Collection\n",
    "from colbert import Indexer, Searcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Jun 05, 21:45:11] #> Loading the queries from experiments/bioasq_query.tsv ...\n",
      "[Jun 05, 21:45:11] #> Got 2068 queries. All QIDs are unique.\n",
      "\n",
      "[Jun 05, 21:45:11] #> Loading collection...\n",
      "0M \n"
     ]
    }
   ],
   "source": [
    "queries = Queries(path='experiments/bioasq_query.tsv')\n",
    "collection = Collection(path='experiments/bioasq_passage_enh.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "## indexing\n",
    "nbits = 2   # encode each dimension with 2 bits\n",
    "doc_maxlen = 300   # truncate passages at 300 tokens\n",
    "\n",
    "checkpoint = 'ColBERT/docs/downloads/colbertv2.0'\n",
    "index_name = \"bioasq_enh.all.2bits\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "id": "XtEGC6MyYre0",
    "outputId": "c6553c0f-598f-4d37-c96b-0766aaef1dfe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Jun 05, 21:49:57] #> Loading collection...\n",
      "0M \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Loaded 2,068 passages'"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collection = './experiments/bioasq_passage_enh.tsv'\n",
    "\n",
    "collection = Collection(path=collection)\n",
    "\n",
    "f'Loaded {len(collection):,} passages'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BixmAYizYre0"
   },
   "outputs": [],
   "source": [
    "## indexing\n",
    "with Run().context(RunConfig(nranks=1, experiment='bioasq')):  # nranks specifies the number of GPUs to use.\n",
    "    config = ColBERTConfig(doc_maxlen=doc_maxlen, nbits=nbits)\n",
    "\n",
    "    indexer = Indexer(checkpoint=checkpoint, config=config)\n",
    "    indexer.index(name=index_name, collection=collection, overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##### Initializing Searcher\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "id": "XtEGC6MyYre0",
    "outputId": "c6553c0f-598f-4d37-c96b-0766aaef1dfe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Jun 05, 23:16:20] #> Loading collection...\n",
      "0M \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Loaded 2,068 passages'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collection = './experiments/bioasq_passage_enh.tsv'\n",
    "\n",
    "collection = Collection(path=collection)\n",
    "\n",
    "f'Loaded {len(collection):,} passages'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "BixmAYizYre0"
   },
   "outputs": [],
   "source": [
    "index_name = \"bioasq_enh.all.2bits\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N6wYpChzYre1",
    "outputId": "4436fe5f-55fa-43d0-ca4b-1d3a70c51d7e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Jun 05, 23:16:23] #> Loading collection...\n",
      "0M \n",
      "[Jun 05, 23:16:24] #> Building the emb2pid mapping..\n",
      "[Jun 05, 23:16:24] len(self.emb2pid) = 498813\n"
     ]
    }
   ],
   "source": [
    "with Run().context(RunConfig(experiment='bioasq')):\n",
    "    searcher = Searcher(index=index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2068"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(searcher.collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Pyridostigmine is the most widely used acetylcholinesterase inhibitor. For more than 50 years the acetylcholinesterase inhibitor pyridostigmine bromide has been the drug of choice in the symptomatic therapy for myasthenia gravis. The switch to SR-Pyr ameliorated the total quantified myasthenia gravis (QMG) score from 0.9 ± 0.5 to 0.6 ± 0.4 (p<0.001) in all patients and in the younger subgroup. This was accompanied by a significant improvement in the quality of life parameters. The health status valued by EuroQoL questionnaire improved from 0.626 ± 0.286 to 0.782 ± 0.186 (p<0.001). Our results support the usefulness of SR-Pyr in an individualized therapeutic regimen to improve quality of life regardless of the patient's age in myasthenia gravis. This review focuses on treatment of MG, mainly on the use of the AChE inhibitor pyridostigmine. Despite a lack of data from well controlled clinical trials to support their use, AChE inhibitors, of which pyridostigmine is the most commonly used, are recommended as firs what is pyridostigmine bromide what is pyridostigmine? what is sr pyr what medications are used for myasthenia gravis what is pyridostigmine? is pyridostigmine acetylcholinesterase inhibitor what is pyridostigmine in a chemo which medication to use for myasthenia gravis what is pyridostigmine bromide is pyridostigmine an acetylcholinesterase inhibitor?\""
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "searcher.collection[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Run system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "          temperature 0.025 \n",
      "          Macro F1 is: 0.11306637037290883， \n",
      "          Exact Match: 0.027794561933534745， \n",
      "          Macro Precision is: 0.10250745897271576,\n",
      "          Macro Recall is: 0.2037761426962686,\n",
      "          \n",
      "CPU times: user 4h 3min 21s, sys: 1h 5min 54s, total: 5h 9min 15s\n",
      "Wall time: 1h 18min 36s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "working_dataset = bioasq_test_enh # bioasq_test vs dev_exs\n",
    "\n",
    "batch_size = 5\n",
    "joiner = '\\n\\n'\n",
    "# number of prompts\n",
    "n_context = 2\n",
    "\n",
    "# temperatures = [0.01, 0.025, 0.05, 0.075]\n",
    "temperatures = [0.025]\n",
    "# bioasq_dev\n",
    "# bioasq_train\n",
    "\n",
    "for temperature in temperatures:\n",
    "    \n",
    "    prompts = []\n",
    "\n",
    "    gens = []\n",
    "\n",
    "    for i in range(0, len(working_dataset), batch_size):\n",
    "        # Use the `searcher` defined above to get passages\n",
    "        # using `ex.question` as the query, and use your\n",
    "        # `build_few_shot_open_qa_prompt` to build prompts.\n",
    "        \n",
    "        # get a batch from bioasq dev (to replace dev_exs)\n",
    "        batch = working_dataset[i: i+batch_size]\n",
    "\n",
    "        train_exs = random.sample(bioasq_train_enh, k=n_context)\n",
    "\n",
    "        ## get a passage for each example in the dev batch\n",
    "        # get search results (passage index) for all examples in the batch\n",
    "        # k = 1 because we choose the top result\n",
    "        results = [searcher.search(ex.question, k=1) for ex in batch]\n",
    "\n",
    "        # from passage index to get the passage 'title | passage'\n",
    "        ## collection contains all the passage avaiable (train+ test + dev)\n",
    "        passages = [searcher.collection[r[0][0]] for r in results]\n",
    "\n",
    "        # re-initiating prompt\n",
    "        ps = []\n",
    "\n",
    "        # for every question in the batch, combine the train_exs (background + q +a) + found passage + question and generate the prompt\n",
    "        # append all prompt into a list\n",
    "        \n",
    "        for ex, psg in zip(batch, passages):\n",
    "            ps.append(build_few_shot_open_qa_prompt(ex.question, psg, train_exs, joiner=joiner))  \n",
    "\n",
    "        # feed prompts (in list of prompts) to gen_func\n",
    "        gs = run_eleuther(ps)       \n",
    "\n",
    "        # add the prompt to prompt list\n",
    "        prompts += ps\n",
    "        # add generated txt to gen list\n",
    "        gens += gs\n",
    "    \n",
    "    eva = evaluate(working_dataset, prompts, gens)\n",
    "    # print(eva)\n",
    "    print(f\"\"\"\n",
    "          temperature {temperature} \n",
    "          Macro F1 is: {eva['macro_f1']}， \n",
    "          Exact Match: {eva['em_per']}， \n",
    "          Macro Precision is: {eva['macro_precision']},\n",
    "          Macro Recall is: {eva['macro_recall']},\n",
    "          \"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### VII.Doc2query+ ColBERT (with improvements)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##### Creating new indexer (one time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "passages_enh = []\n",
    "for i in range(len(bioasq_list_copy)):\n",
    "    passages_enh.append((i, bioasq_list_copy[i]['context']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "## need to use a new index that is based on the ehnanced dataset\n",
    "import csv\n",
    "with open('experiments/bioasq_passage_enh.tsv', 'w+', newline='') as f_output:\n",
    "    csv_output = csv.writer(f_output, delimiter='\\t')\n",
    "\n",
    "    csv_output.writerows(passages_enh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from colbert.infra import Run, RunConfig, ColBERTConfig\n",
    "from colbert.data import Queries, Collection\n",
    "from colbert import Indexer, Searcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Jun 05, 21:45:11] #> Loading the queries from experiments/bioasq_query.tsv ...\n",
      "[Jun 05, 21:45:11] #> Got 2068 queries. All QIDs are unique.\n",
      "\n",
      "[Jun 05, 21:45:11] #> Loading collection...\n",
      "0M \n"
     ]
    }
   ],
   "source": [
    "queries = Queries(path='experiments/bioasq_query.tsv')\n",
    "collection = Collection(path='experiments/bioasq_passage_enh.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "## indexing\n",
    "nbits = 2   # encode each dimension with 2 bits\n",
    "doc_maxlen = 300   # truncate passages at 300 tokens\n",
    "\n",
    "checkpoint = 'ColBERT/docs/downloads/colbertv2.0'\n",
    "index_name = \"bioasq_enh.all.2bits\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "id": "XtEGC6MyYre0",
    "outputId": "c6553c0f-598f-4d37-c96b-0766aaef1dfe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Jun 05, 21:49:57] #> Loading collection...\n",
      "0M \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Loaded 2,068 passages'"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collection = './experiments/bioasq_passage_enh.tsv'\n",
    "\n",
    "collection = Collection(path=collection)\n",
    "\n",
    "f'Loaded {len(collection):,} passages'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BixmAYizYre0"
   },
   "outputs": [],
   "source": [
    "## indexing\n",
    "with Run().context(RunConfig(nranks=1, experiment='bioasq')):  # nranks specifies the number of GPUs to use.\n",
    "    config = ColBERTConfig(doc_maxlen=doc_maxlen, nbits=nbits)\n",
    "\n",
    "    indexer = Indexer(checkpoint=checkpoint, config=config)\n",
    "    indexer.index(name=index_name, collection=collection, overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Initializing Searcher\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "id": "XtEGC6MyYre0",
    "outputId": "c6553c0f-598f-4d37-c96b-0766aaef1dfe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Jun 05, 22:08:20] #> Loading collection...\n",
      "0M \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Loaded 2,068 passages'"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collection = './experiments/bioasq_passage_enh.tsv'\n",
    "\n",
    "collection = Collection(path=collection)\n",
    "\n",
    "f'Loaded {len(collection):,} passages'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "id": "BixmAYizYre0"
   },
   "outputs": [],
   "source": [
    "index_name = \"bioasq_enh.all.2bits\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N6wYpChzYre1",
    "outputId": "4436fe5f-55fa-43d0-ca4b-1d3a70c51d7e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Jun 05, 22:08:32] #> Loading collection...\n",
      "0M \n",
      "[Jun 05, 22:08:33] #> Building the emb2pid mapping..\n",
      "[Jun 05, 22:08:33] len(self.emb2pid) = 498813\n"
     ]
    }
   ],
   "source": [
    "with Run().context(RunConfig(experiment='bioasq')):\n",
    "    searcher = Searcher(index=index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2068"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(searcher.collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Pyridostigmine is the most widely used acetylcholinesterase inhibitor. For more than 50 years the acetylcholinesterase inhibitor pyridostigmine bromide has been the drug of choice in the symptomatic therapy for myasthenia gravis. The switch to SR-Pyr ameliorated the total quantified myasthenia gravis (QMG) score from 0.9 ± 0.5 to 0.6 ± 0.4 (p<0.001) in all patients and in the younger subgroup. This was accompanied by a significant improvement in the quality of life parameters. The health status valued by EuroQoL questionnaire improved from 0.626 ± 0.286 to 0.782 ± 0.186 (p<0.001). Our results support the usefulness of SR-Pyr in an individualized therapeutic regimen to improve quality of life regardless of the patient's age in myasthenia gravis. This review focuses on treatment of MG, mainly on the use of the AChE inhibitor pyridostigmine. Despite a lack of data from well controlled clinical trials to support their use, AChE inhibitors, of which pyridostigmine is the most commonly used, are recommended as firs what is pyridostigmine bromide what is pyridostigmine? what is sr pyr what medications are used for myasthenia gravis what is pyridostigmine? is pyridostigmine acetylcholinesterase inhibitor what is pyridostigmine in a chemo which medication to use for myasthenia gravis what is pyridostigmine bromide is pyridostigmine an acetylcholinesterase inhibitor?\""
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "searcher.collection[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Run "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "          temperature 0.025 \n",
      "          Macro F1 is: 0.09191770647653001， \n",
      "          Exact Match: 0.0， \n",
      "          Macro Precision is: 0.1404166666666667,\n",
      "          Macro Recall is: 0.16006944444444443,\n",
      "          \n",
      "CPU times: user 10min 5s, sys: 20.7 s, total: 10min 26s\n",
      "Wall time: 1min 21s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "######## This part is functional modules ############\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "#### enhanced squad training example searching\n",
    "\n",
    "def train_tf_idf(training_data):\n",
    "    tfidfvectorizer = TfidfVectorizer(analyzer='word',stop_words= 'english', ngram_range=(1, 3))\n",
    "\n",
    "  # append all context\n",
    "    train_context = [x.context for x in training_data]\n",
    "\n",
    "    tfidfvectorizer.fit_transform(train_context)\n",
    "\n",
    "    context_tfidf = tfidfvectorizer.transform(train_context)\n",
    "\n",
    "    return tfidfvectorizer, context_tfidf\n",
    "\n",
    "def sample_bioasq_train(tfidfvectorizer, context_tfidf, question, n_context, training_data):\n",
    "    '''\n",
    "    This is using tf-idf and consine similarity to sample \"related to question\" bioasq example to build the prompt\n",
    "    '''\n",
    "    question_tfidf = tfidfvectorizer.transform([question])\n",
    "\n",
    "    cosine_sim = cosine_similarity(context_tfidf, question_tfidf).flatten()\n",
    "\n",
    "    related_index = cosine_sim.argsort()[-n_context:][::-1]\n",
    "\n",
    "    train_exs = [training_data[i] for i in related_index]\n",
    "\n",
    "    return train_exs\n",
    "\n",
    "### revised answer scoring by normalizing the score by length\n",
    "from types import GeneratorType\n",
    "## added temperature arg to allow change\n",
    "def answer_scoring_normalized(passages, passage_probs, prompts, temperature, gen_func=run_eleuther):\n",
    "    \"\"\"Implements our basic scoring strategy.\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  passages : list of str\n",
    "  passage_probs : list of float\n",
    "  prompts : list of str\n",
    "  gen_func : either `run_eleuther` or `run_gpt3`\n",
    "\n",
    "  Returns\n",
    "  -------\n",
    "  list of pairs (score, dict), sorted with the largest score first.\n",
    "  `dict` should be the return value of `gen_func` for an example.\n",
    "\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    length_sum = 0\n",
    "    gen_list = []\n",
    "\n",
    "    for passage, passage_prob, prompt in zip(passages, passage_probs, prompts):\n",
    "        gen = gen_func([prompt], temperature = temperature)\n",
    "\n",
    "        gen_list.append(gen)\n",
    "        # calculate the total length of answers\n",
    "        length_sum += len(gen[0]['generated_answer'].split(' '))\n",
    "\n",
    "    for passage_prob, gen in zip(passage_probs, gen_list):\n",
    "\n",
    "        answer_score = np.prod(gen[0]['generated_answer_probs'])\n",
    "\n",
    "        length_of_answer = len(gen[0]['generated_answer'].split(' '))\n",
    "\n",
    "        # give more weight to longer answers, as its product of per-token probabiliyy is underdog\n",
    "        weight = length_of_answer/length_sum\n",
    "\n",
    "        final_score = passage_prob*answer_score*weight\n",
    "\n",
    "        data.append((final_score, gen[0]))\n",
    "\n",
    "    data.sort(key = lambda x: x[0], reverse=True)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "######## This part is system development ############\n",
    "\n",
    "batch_size = 5\n",
    "joiner = '\\n\\n'\n",
    "n_context = 2\n",
    "\n",
    "# temperatures = [0.01, 0.025, 0.05, 0.075]\n",
    "temperatures = [0.025]\n",
    "\n",
    "\n",
    "working_dataset = dev_exs_enh # bioasq_test vs dev_exs\n",
    "\n",
    "for temperature in temperatures:\n",
    "    prompts = []\n",
    "\n",
    "    gens = []\n",
    "\n",
    "    # use tf-idf to find \"related few shot in bioasq to build the prompt\n",
    "    # train tf-idf on all bioasq examples\n",
    "    tfidfvectorizer, context_tfidf = train_tf_idf(bioasq_train_enh)\n",
    "\n",
    "    for i in range(0, len(working_dataset), batch_size):\n",
    "        # Use the `searcher` defined above to get passages\n",
    "        # using `ex.question` as the query, and use your\n",
    "        # `build_few_shot_open_qa_prompt` to build prompts.\n",
    "\n",
    "        # get a batch from bioasq dev (to replace dev_exs)\n",
    "        batch = working_dataset[i: i+batch_size]\n",
    "\n",
    "        # train_exs = random.sample(bioasq_train, k=n_context)\n",
    "\n",
    "        ## score for answer-passage pair\n",
    "        for ex in batch:\n",
    "\n",
    "          # use tf idf to sample training exs, instead of just random sampling bioasq training\n",
    "            train_exs = sample_bioasq_train(tfidfvectorizer, \n",
    "                                            context_tfidf,\n",
    "                                            ex.question, \n",
    "                                            n_context, \n",
    "                                            bioasq_train_enh)\n",
    "\n",
    "            passages, passage_probs = get_passages_with_scores(ex.question)\n",
    "\n",
    "            # re-initiating prompt\n",
    "            ps = []\n",
    "            # iterate through each passage in the top k (5) passages\n",
    "            for psg in passages:\n",
    "            # build the prompt based on question, that specific passge, and training examples\n",
    "            # say we have passage, then ps will be ['prompt1', 'prompt2', 'prompt3', 'prompt4', 'prompt5']\n",
    "                ps.append(build_few_shot_open_qa_prompt(ex.question, psg, train_exs, joiner=joiner)) \n",
    "\n",
    "          # calculate the answering score for the highest passage-answer pair                 \n",
    "          # data = answer_scoring(passages,       # only related to question, same length as ps\n",
    "          #                       passage_probs,  # only related to question, same length as ps\n",
    "          #                       ps,             # k prompts\n",
    "          #                       run_eleuther)\n",
    "\n",
    "            data = answer_scoring_normalized(passages,       # only related to question, same length as ps\n",
    "                                passage_probs,  # only related to question, same length as ps\n",
    "                                ps,             # k prompts\n",
    "                                temperature,\n",
    "                                run_eleuther)\n",
    "\n",
    "            # pick highest score answer-prompt pair (note: in)\n",
    "            highest_gs = [data[0][1]]\n",
    "            highest_ps = [data[0][1]['prompt']]\n",
    "\n",
    "            # add the prompt to prompt list\n",
    "            prompts += highest_ps\n",
    "\n",
    "            # add generated txt to gen list\n",
    "            gens += highest_gs\n",
    " \n",
    "    eva = evaluate(working_dataset, prompts, gens)\n",
    "    \n",
    "    print(f\"\"\"\n",
    "          temperature {temperature} \n",
    "          Macro F1 is: {eva['macro_f1']}， \n",
    "          Exact Match: {eva['em_per']}， \n",
    "          Macro Precision is: {eva['macro_precision']},\n",
    "          Macro Recall is: {eva['macro_recall']},\n",
    "          \"\"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Retrieval Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7957703927492447\n",
      "CPU times: user 1min 55s, sys: 2.88 s, total: 1min 58s\n",
      "Wall time: 38.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if torch.cuda.is_available():\n",
    "    # This will take a few hours on a CPU:\n",
    "    print(success_at_k(bioasq_test_enh))\n",
    "else:\n",
    "    # This should be reasonably fast and yields the\n",
    "    # same kind of result:\n",
    "    print(success_at_k(bioasq_test_enh))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### BERT BASE Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f8b03ded8e64ca98b7821776fc431d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/62.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60ee71a64de34d8696aa9aa48971bc43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/696 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6b2888661364c44a84e36ebff98d48b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/851k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d973b92808cd490b89af8d3778bccdc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9749bc6bd1b64c2e9d5ee17cd14a0ba4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/638M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"amberoad/bert-multilingual-passage-reranking-msmarco\")\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"amberoad/bert-multilingual-passage-reranking-msmarco\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    corpus_token = []\n",
    "    for example in bioasq_list:\n",
    "        example_token = tokenizer(example['context'], return_tensors=\"pt\", truncation=True, padding='max_length', max_length = 400)[\"input_ids\"] \n",
    "        corpus_token.append(example_token) \n",
    "    corpus_token = torch.stack(corpus_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2068, 1, 400])"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_token.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = dev_exs[10].question\n",
    "\n",
    "num_corpus = len(bioasq_list)\n",
    "\n",
    "with torch.no_grad():\n",
    "    query_token = bert_tokenizer(question, return_tensors=\"pt\", truncation=True, max_length = 112)[\"input_ids\"] \n",
    "    \n",
    "    # replicate the question token to the number of passages\n",
    "    query_tokens= query_token.expand(num_corpus, query_token.shape[0], query_token.shape[1] )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2068, 1, 18])"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_tensor = torch.cat((query_tokens, corpus_token), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "outputs = bert_model(**concat_tensor)\n",
    "\n",
    "last_hidden_states = outputs.last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    inputs = tokenizer([\"what the fucnt\", 'you tell me'], return_tensors=\"pt\", padding = True).to(device)\n",
    "    outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[ 4.4169, -3.5654],\n",
       "        [ 2.7513, -2.1540]], device='cuda:0'), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 768])"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.hidden_states[-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def bert_encode(corpus, query, tokenizer):\n",
    "    \"\"\"\n",
    "    Function that takes a corpus, a query and a tokenizer and returns the \n",
    "    query and all texts in the corpus concatenated together and separated by\n",
    "    [CLS] flag, then tokenized and ready for BERT.\n",
    "\n",
    "    This function utilizes the previous utility function 'encode_text'.\n",
    "@param corpus: list,\n",
    "        A valid list of string elements where each element is an article in our\n",
    "        corpus. As returned from 'read_corpus' function. As returned from 'read_corpus' function.\n",
    "    @param query: string,\n",
    "        A valid text string which is the query for which answers need to be \n",
    "        retrieved.\n",
    "    @param tokenizer: BERT.tokenization function,\n",
    "        A valid BERT-compatible tokenizer.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    @return inputs: dict,\n",
    "        A dictionary containg three elements: \n",
    "            - input_word_ids: TF.io.tensor, \n",
    "                    The tokenized words ids.\n",
    "            - input_mask: TF.io.tensor,\n",
    "                    Tensor taking values based on whether the element at each \n",
    "                    position is a mask (flag) or not.\n",
    "            - input_type_ids: TF.io.tensor,\n",
    "                    Tensor taking values based on the type of the input element\n",
    "                    at each position.\n",
    "    \"\"\"\n",
    "\n",
    "    # Compute corpus length.\n",
    "    corpus_length = len(corpus)\n",
    "\n",
    "    # Transform each article in the corpus to a TF ragged constant.\n",
    "    tf_corpus = tf.ragged.constant(\n",
    "        [encode_text(article, tokenizer) for article in corpus]\n",
    "        )\n",
    "\n",
    "    # Encode the query, then transform it to a TF ragged constant of same \n",
    "    # length as the corpus.\n",
    "    encoded_query = encode_text(query, tokenizer)\n",
    "    tf_query = tf.ragged.constant(\n",
    "        [encoded_query for i in range(corpus_length)]\n",
    "        )\n",
    "    \n",
    "    # Create as many [CLS] flags as the number of articles in the corpus.\n",
    "    cls = [tokenizer.convert_tokens_to_ids(['[CLS]'])] * tf_corpus.shape[0]\n",
    "    # Concatenate all elements together \n",
    "    input_word_ids = tf.concat([cls, tf_query, tf_corpus], axis = -1)\n",
    "\n",
    "    # Create masks tensor...\n",
    "    input_mask = tf.ones_like(input_word_ids).to_tensor()\n",
    "\n",
    "    # Create types tensors...\n",
    "    type_cls = tf.zeros_like(cls)\n",
    "    type_corpus = tf.zeros_like(tf_corpus)\n",
    "    type_query = tf.zeros_like(tf_query)\n",
    "    # ... and concatenate them together\n",
    "    input_type_ids = tf.concat(\n",
    "        [type_cls, type_query, type_corpus],\n",
    "        axis = -1\n",
    "    ).to_tensor()\n",
    "\n",
    "    # Prepare results dictionary for returning...\n",
    "    inputs = {\n",
    "        'input_word_ids' : input_word_ids.to_tensor(),\n",
    "        'input_mask' : input_mask,\n",
    "        'input_type_ids' : input_type_ids\n",
    "    }\n",
    "\n",
    "    # Return...\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rank Fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "dKB9zXRBYrel",
    "-mp1C-oyYreq",
    "Fca8-RXjYres",
    "XGK3hCs9Yrev",
    "DZpXMk-0Yrew",
    "NFYxJPpuYre0",
    "w9n0_xwdYre1",
    "teKobQM8Yre1",
    "R55lgmnTzTip",
    "w2mx3Z4HYre2"
   ],
   "machine_shape": "hm",
   "name": "hw_openqa.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "a99ac6d2deb03d0b7ced3594556c328848678d7cea021ae1b9990e15d3ad5c49"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "19838b224425453988f4ea47c50c048e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "29084cffd32b495ba5c29910a483b41a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_cb331d5edf364e19af8f45fd3726a9f8",
       "IPY_MODEL_dcb183bf0a85498d8bb2bd360c64255b",
       "IPY_MODEL_83ab942f9ce14cd6b82da28b36dd1377"
      ],
      "layout": "IPY_MODEL_19838b224425453988f4ea47c50c048e"
     }
    },
    "5054cd44686b4f649724445b581ad979": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "76b125c6b6cf426cbf3ad638e443cbd8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "83ab942f9ce14cd6b82da28b36dd1377": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_eda083d3daeb4c59b66beb142f12063a",
      "placeholder": "​",
      "style": "IPY_MODEL_adaf48a176ad439f89883761b3027232",
      "value": " 2/2 [00:00&lt;00:00,  7.29it/s]"
     }
    },
    "adaf48a176ad439f89883761b3027232": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "bdcd068db1ad4c55ba282295b91aaeec": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cb331d5edf364e19af8f45fd3726a9f8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_bdcd068db1ad4c55ba282295b91aaeec",
      "placeholder": "​",
      "style": "IPY_MODEL_f61e1dd551d74455b834735b4aa816df",
      "value": "100%"
     }
    },
    "dcb183bf0a85498d8bb2bd360c64255b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5054cd44686b4f649724445b581ad979",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_76b125c6b6cf426cbf3ad638e443cbd8",
      "value": 2
     }
    },
    "eda083d3daeb4c59b66beb142f12063a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f61e1dd551d74455b834735b4aa816df": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
