{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G0qZfMf4Yreh"
   },
   "source": [
    "# Medical Few-shot OpenQA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dKB9zXRBYrel"
   },
   "source": [
    "## Set-up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sFM54iO7Yren"
   },
   "source": [
    "### General set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "hL9AAtTzYren"
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "from contextlib import nullcontext\n",
    "from collections import namedtuple\n",
    "from datasets import load_dataset\n",
    "import json\n",
    "import numpy as np\n",
    "import random\n",
    "import re \n",
    "import string\n",
    "import torch\n",
    "from typing import List\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vNXANb8fYreo"
   },
   "source": [
    "Try to set all the seeds for reproducibility (won't extend to GPT-3):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "MIvsYoIpYreo"
   },
   "outputs": [],
   "source": [
    "seed = 1\n",
    "\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-mp1C-oyYreq"
   },
   "source": [
    "### Language model set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "TKQEIYGDYrer"
   },
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "transformers.logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "axUPfnNmYrer"
   },
   "source": [
    "### ColBERT set-up\n",
    "\n",
    "Our retriever will be a ColbERT-based model ([Khattab and Zaharia 2020](https://arxiv.org/abs/2004.12832)). ColBERT is a powerful neural information retrieval (Neural IR) model that has proven extremely successful in retrieval applications and as a component in a variety of different systems for OpenQA and other knowledge-intensive tasks (e.g., [Khattab et al. 2021a](https://aclanthology.org/2021.tacl-1.55/); [Khattab et al. 2021b](https://proceedings.neurips.cc/paper/2021/hash/e8b1cbd05f6e6a358a81dee52493dd06-Abstract.html); [Santhanam, Khattab, et al. 2021](https://arxiv.org/abs/2112.01488)).\n",
    "\n",
    "The following will clone the ColBERTv2 repository for use in this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L5wP933JYrer",
    "outputId": "743b9a8c-b982-4db8-af18-08d43dee9e17"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'ColBERT'...\n",
      "remote: Enumerating objects: 764, done.\u001b[K\n",
      "remote: Counting objects: 100% (379/379), done.\u001b[K\n",
      "remote: Compressing objects: 100% (132/132), done.\u001b[K\n",
      "remote: Total 764 (delta 272), reused 318 (delta 247), pack-reused 385\u001b[K\n",
      "Receiving objects: 100% (764/764), 302.54 KiB | 1.11 MiB/s, done.\n",
      "Resolving deltas: 100% (424/424), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone -b cpu_inference https://github.com/stanford-futuredata/ColBERT.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "6LWz4f-3Yres"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, 'ColBERT/')\n",
    "\n",
    "from colbert.infra import Run, RunConfig, ColBERTConfig\n",
    "from colbert.data import Collection\n",
    "from colbert.searcher import Searcher\n",
    "from utility.utils.dpr import has_answer, DPR_normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U5ganiuNYres"
   },
   "source": [
    "## Language models\n",
    "\n",
    "In few-shot OpenQA, the language model (LM) must read in a prompt and answer the question posed somewhere in the prompt. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fca8-RXjYres"
   },
   "source": [
    "### Answerhood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "U6KepnjTYret"
   },
   "outputs": [],
   "source": [
    "def _find_generated_answer(tokens, newline=\"\\n\" ): \n",
    "    \"\"\"Our LMs tend to insert initial newline characters before\n",
    "    they begin generating text. This function ensures that we \n",
    "    properly capture the true first line as the answer while\n",
    "    also ensuring that token probabilities are aligned.\"\"\"        \n",
    "    answer_token_indices = []\n",
    "    char_seen = False            \n",
    "    for i, tok in enumerate(tokens):\n",
    "        # This is the main condition: a newline that isn't an initial\n",
    "        # string of newlines:\n",
    "        if tok == newline and char_seen:\n",
    "            break\n",
    "        # Keep the initial newlines for consistency:\n",
    "        elif tok == newline and not char_seen:\n",
    "            answer_token_indices.append(i)\n",
    "        # Proper tokens:\n",
    "        elif tok != newline:\n",
    "            char_seen = True\n",
    "            answer_token_indices.append(i)\n",
    "    return answer_token_indices "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "klW12GkAYret"
   },
   "source": [
    "### Eleuther models from Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "1-FkbTaUYret"
   },
   "outputs": [],
   "source": [
    "# \"gpt-neo-125M\" \"gpt-neo-1.3B\" \"gpt-neo-2.7B\" \"gpt-j-6B\"\n",
    "eleuther_model_name = \"gpt-neo-125M\"\n",
    "\n",
    "eleuther_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    f\"EleutherAI/{eleuther_model_name}\", \n",
    "    padding_side=\"left\", \n",
    "    padding='longest', \n",
    "    truncation='longest_first', max_length=2000)\n",
    "eleuther_tokenizer.pad_token = eleuther_tokenizer.eos_token\n",
    "\n",
    "eleuther_model = AutoModelForCausalLM.from_pretrained(\n",
    "    f\"EleutherAI/{eleuther_model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "rEo9YKwNYret"
   },
   "outputs": [],
   "source": [
    "def run_eleuther(prompts, temperature=0.1, top_p=0.95, **generate_kwargs): \n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    prompts : iterable of str\n",
    "    temperature : float\n",
    "        It seems best to set it low for this task!\n",
    "    top_p : float\n",
    "       \n",
    "    For options for `generate_kwargs`, see:\n",
    "    \n",
    "    https://huggingface.co/docs/transformers/master/en/main_classes/text_generation#transformers.generation_utils.GenerationMixin.generate\n",
    "    \n",
    "    Options that are likely to be especially relevant include \n",
    "    `temperature`, `length_penalty`, and the parameters that\n",
    "    determine the decoding strategy. With `num_return_sequences > 1`,\n",
    "    the default parameters in this function do multinomial sampling.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    list of dicts\n",
    "    \n",
    "    {\"prompt\": str, \n",
    "     \"generated_text\": str, \"generated_tokens\": list of str, \"generated_probs\": list of float,\n",
    "     \"answer\": str, \"answer_tokens\": list of str, \"answer_probs\": list of float\n",
    "    }\n",
    "         \n",
    "    \"\"\"\n",
    "    prompt_ids = eleuther_tokenizer(\n",
    "        prompts, return_tensors=\"pt\", padding=True).input_ids\n",
    "        \n",
    "    with torch.inference_mode():\n",
    "        # Automatic mixed precision if possible.\n",
    "        with torch.cuda.amp.autocast() if torch.cuda.is_available() else nullcontext():\n",
    "            model_output = eleuther_model.generate(\n",
    "                prompt_ids,\n",
    "                temperature=temperature,\n",
    "                do_sample=True,\n",
    "                top_p=top_p,           \n",
    "                max_new_tokens=16,\n",
    "                num_return_sequences=1,                \n",
    "                pad_token_id=eleuther_tokenizer.eos_token_id, \n",
    "                return_dict_in_generate=True,\n",
    "                output_scores=True,\n",
    "                **generate_kwargs)\n",
    "        \n",
    "    # Converting output scores using the helpful recipe here:\n",
    "    # https://discuss.huggingface.co/t/generation-probabilities-how-to-compute-probabilities-of-output-scores-for-gpt2/3175\n",
    "    gen_ids = model_output.sequences[:, prompt_ids.shape[-1] :]\n",
    "    gen_probs = torch.stack(model_output.scores, dim=1).softmax(-1)\n",
    "    gen_probs = torch.gather(gen_probs, 2, gen_ids[:, :, None]).squeeze(-1)\n",
    "    \n",
    "    # Generated texts, including the prompts:\n",
    "    gen_texts = eleuther_tokenizer.batch_decode(\n",
    "        model_output.sequences, skip_special_tokens=True)\n",
    "    \n",
    "    data = []     \n",
    "    iterator = zip(prompts, gen_ids, gen_texts, gen_probs)    \n",
    "    for prompt, gen_id, gen_text, gen_prob in iterator:       \n",
    "        gen_tokens = eleuther_tokenizer.convert_ids_to_tokens(gen_id)\n",
    "        generated_text = gen_text[len(prompt): ]\n",
    "        gen_prob = [float(x) for x in gen_prob.numpy()] # float for JSON storage\n",
    "        ans_indices = _find_generated_answer(gen_tokens, newline=\"Ċ\")\n",
    "        answer_tokens = [gen_tokens[i] for i in ans_indices]\n",
    "        answer_probs = [gen_prob[i] for i in ans_indices]\n",
    "        answer = \"\".join(answer_tokens).replace(\"Ġ\", \" \").replace(\"Ċ\", \"\\n\")                                       \n",
    "        data.append({\n",
    "            \"prompt\": prompt,\n",
    "            \"generated_text\": generated_text,\n",
    "            \"generated_tokens\": gen_tokens,\n",
    "            \"generated_probs\": gen_prob,\n",
    "            \"generated_answer\": answer,\n",
    "            \"generated_answer_probs\": answer_probs,\n",
    "            \"generated_answer_tokens\": answer_tokens})                        \n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7fIfuTEGYreu",
    "outputId": "ec7c8ead-7980-4a4e-b6c1-10aef49bd625"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "## test run\n",
    "\n",
    "eleuther_ex = run_eleuther([    \n",
    "    \"What year was Stanford University founded?\", \n",
    "    \"In which year did Stanford first enroll students?\"])\n",
    "\n",
    "eleuther_ex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XGK3hCs9Yrev"
   },
   "source": [
    "## Dataset Loading\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### SQuAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0,
     "referenced_widgets": [
      "29084cffd32b495ba5c29910a483b41a",
      "cb331d5edf364e19af8f45fd3726a9f8",
      "dcb183bf0a85498d8bb2bd360c64255b",
      "83ab942f9ce14cd6b82da28b36dd1377",
      "19838b224425453988f4ea47c50c048e",
      "bdcd068db1ad4c55ba282295b91aaeec",
      "f61e1dd551d74455b834735b4aa816df",
      "5054cd44686b4f649724445b581ad979",
      "76b125c6b6cf426cbf3ad638e443cbd8",
      "eda083d3daeb4c59b66beb142f12063a",
      "adaf48a176ad439f89883761b3027232"
     ]
    },
    "id": "YQ_do58EYrev",
    "outputId": "39d8b916-1d33-412e-dbde-8e5cb3085951"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset squad (/home/zhanj289/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eea3de6d3ebb4fb3adaad4fbf2b985fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "squad = load_dataset(\"squad\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VbCUz66GYrev"
   },
   "source": [
    "The following utility just reads a SQuAD split in as a list of `SquadExample` instances:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "B9-0hkxgYrew"
   },
   "outputs": [],
   "source": [
    "SquadExample = namedtuple(\"SquadExample\",  \"id title context question answers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "g2gt0dkeYrew"
   },
   "outputs": [],
   "source": [
    "def get_squad_split(squad, split=\"validation\"):\n",
    "    \"\"\"\n",
    "    Use `split='train'` for the train split.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    list of SquadExample named tuples with attributes\n",
    "    id, title, context, question, answers\n",
    "    \n",
    "    \"\"\"    \n",
    "    fields = squad[split].features\n",
    "    data = zip(*[squad[split][field] for field in fields])\n",
    "    return [SquadExample(eid, title, context, question, answers[\"text\"]) \n",
    "            for eid, title, context, question, answers in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Split Dev and Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = squad['validation'].features\n",
    "data = zip(*[squad['validation'][field] for field in fields])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "NatnOLsDYrew"
   },
   "outputs": [],
   "source": [
    "squad_dev = get_squad_split(squad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jb-ZrSzoYrew",
    "outputId": "4ac4149e-0f96-4c0f-e0ca-aed1d12c796c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SquadExample(id='56d602631c85041400946edb', title='Super_Bowl_50', context='CBS broadcast Super Bowl 50 in the U.S., and charged an average of $5 million for a 30-second commercial during the game. The Super Bowl 50 halftime show was headlined by the British rock group Coldplay with special guest performers Beyoncé and Bruno Mars, who headlined the Super Bowl XLVII and Super Bowl XLVIII halftime shows, respectively. It was the third-most watched U.S. broadcast ever.', question='Who were special guests for the Super Bowl halftime show?', answers=['Beyoncé and Bruno Mars', 'Beyoncé and Bruno Mars', 'Beyoncé and Bruno Mars'])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squad_dev[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "rw3OGISgzTil"
   },
   "outputs": [],
   "source": [
    "dev_exs = sorted(squad_dev, key=lambda x: hash(x.id))[: 200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "YwwjQiF8zTim"
   },
   "outputs": [],
   "source": [
    "squad_train = get_squad_split(squad, \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "    num_rows: 87599\n",
       "})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squad['train']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BioASQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('./data/bioasq/squad.json', 'r') as f:\n",
    "#     squad_test = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/bioasq/training10b.json', 'r') as f:\n",
    "    bioasq_json = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick all factoid questions but ignore all else"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bioasq_json['questions'][0]['snippets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_dict = {}\n",
    "\n",
    "# for snip in bioasq_json['questions'][0]['snippets']:\n",
    "#     if snip['beginSection'] == 'abstract':\n",
    "#         for k in range(snip['offsetInBeginSection'], snip['offsetInEndSection']):\n",
    "#             text_dict[k] = snip['text'][k- snip['offsetInBeginSection']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recon_text = ''\n",
    "# for key in sorted(text_dict.keys()):\n",
    "#     recon_text += text_dict[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"BACKGROUND: RET is the major gene associated to Hirschsprung disease (HSCR) with differential contributions of its rare and common, coding and noncodinIn the etiology of Hirschsprung disease various genes play a role; these are: RET, EDNRB, GDNF, EDN3 and SOX10, NTN3, ECE1, Mutations in these genes may result in dominant, recessive or multifactorial patterns of inheritance.Coding sequence mutaOn the basis of a skewed sex-ratio (M/F = 4/1) and a risk to relatives much higher than the incidence in the general population, HSCR has long been regarded as a sex-modified multifactorial disorderhermore, mutations in the RET gene are responsible for approximately half of the familial and some sThe majority of the identified genes are related to Mendelian syndromic forms of Hirschsprung's diseasee The non-Mendelian inheritance of sporadic non-syndromic Hirschsprung's disease proved to be complex; involvement of multiple loci was demonstrated in a multiplicative model expression in malese HSCR phenotype has been reported, probably due to modifier loci. Therefore, HSCR has become a model for a complex oligo-/polygenic disorder in which the relationship between different genes creating a non-mendelian inheritance pattern still remains to be elucidated\""
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# recon_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bioasq_json['questions'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we have 1252 factoid questions, 816 list questions, 1018 summary questions, 1148 yesno qquestions\n",
      "total is 4234\n"
     ]
    }
   ],
   "source": [
    "### Construct dataset\n",
    "count_factoid = 0\n",
    "count_list =0\n",
    "count_summary=0\n",
    "count_yesno =0\n",
    "\n",
    "bioasq_list= []\n",
    "\n",
    "for i in range(len(bioasq_json['questions'])):\n",
    "    \n",
    "    sample = bioasq_json['questions'][i]\n",
    "    \n",
    "    if sample['type'] == 'summary':\n",
    "            count_summary += 1\n",
    "    if sample['type'] == 'yesno':\n",
    "            count_yesno += 1\n",
    "    \n",
    "    if sample['type'] in ['factoid', 'list']:\n",
    "        \n",
    "    #  Context\n",
    "    ## flatten all the snippet, conccatenate and use as context\n",
    "        context = '' \n",
    "        for snip in [ele['text'].strip() for ele in sample['snippets']]:\n",
    "            snip += ' '\n",
    "            context += snip\n",
    "            \n",
    "        context = context.replace('\\n', ' ')\n",
    "        \n",
    "        ## limit the length of context\n",
    "        ### Max: 4096 (for eleuther model)\n",
    "        context = context[:1024]\n",
    "        \n",
    "        # question\n",
    "        question = sample['body']\n",
    "        question = question.replace('\\n', ' ')\n",
    "        \n",
    "        # answer:\n",
    "        ## deal with factoid question and list question differently\n",
    "        if sample['type'] == 'factoid':\n",
    "            answer = sample['exact_answer']\n",
    "            count_factoid += 1\n",
    "        \n",
    "        if sample['type'] == 'list':\n",
    "            answer = [x for y in sample['exact_answer'] for x in y]\n",
    "            count_list += 1\n",
    "        \n",
    "\n",
    "        # construct a QA pairs like SQUAD\n",
    "        bioasq_list.append({\n",
    "            'id': i,\n",
    "            'context': context,\n",
    "            'question': sample['body'],\n",
    "            'answers': answer,\n",
    "            'type': sample['type']\n",
    "        }) \n",
    "\n",
    "print(f'we have {count_factoid} factoid questions, {count_list} list questions, {count_summary} summary questions, {count_yesno} yesno qquestions')   \n",
    "\n",
    "print(f'total is {count_factoid +count_list+ count_summary +count_yesno}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2068"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bioasq_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "def get_bioasq_split(bioasq_list, random_state):\n",
    "    \"\"\"\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    list of example named tuples with attributes\n",
    "    id, title, context, question, answers\n",
    "    \n",
    "    \"\"\"\n",
    "    BioasqExample = namedtuple(\"BioasqExample\",  \"id context question answers\")\n",
    "    \n",
    "    bioasq_data = [BioasqExample(ele['id'], ele['context'], ele['question'], ele['answers']) for ele in bioasq_list]\n",
    "    \n",
    "    bioasq_train, _ = train_test_split(bioasq_data, test_size=0.9, random_state=random_state)\n",
    "\n",
    "    bioasq_dev, bioasq_test = train_test_split(_, test_size=0.8888, random_state=random_state)\n",
    "    \n",
    "    return bioasq_train, bioasq_dev, bioasq_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "bioasq_train, bioasq_dev, bioasq_test = get_bioasq_split(bioasq_list, random_state=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "## split dev and test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206, 207, 1655 \n"
     ]
    }
   ],
   "source": [
    "print(f\"{len(bioasq_train)}, {len(bioasq_dev)}, {len(bioasq_test)} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "## pick 20 just for sanity check\n",
    "dev_exs = bioasq_dev[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BioasqExample(id=829, context='Nearly one half of all cases of acquired resistance to epidermal growth factor receptor (EGFR) tyrosine kinase inhibitors (TKIs) for non-small-cell lung cancer (NSCLC) are due to the T790M mutation in EGFR exon 20. Two types of epidermal growth factor receptor (EGFR) mutations in exon 19 and exon 21 (ex19del and L858R) are prevalent in lung cancer patients and sensitive to targeted EGFR inhibition. A resistance mutation in exon 20 (T790M) has been found to accompany drug treatment when patients relapse. Acquired EGFR C797S mutation mediates resistance to AZD9291 in non-small cell lung cancer harboring EGFR T790M. However, resistance to the EGFR TKIs develops mostly secondary to T790M mutation in exon 20. The T790M mutation in EGFR accounts for approximately half of all lung cancer cases with acquired resistance to the current clinical EGFR tyrosine kinase inhibitors. In nonsmall cell lung cancer (NSCLC), the threonine(790)-methionine(790) (T790M) point mutation of EGFR kinase is one of the leading causes of a', question='Which gene harbors the mutation T790M?', answers=['EGFR', 'epidermal growth factor receptor'])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_exs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DZpXMk-0Yrew",
    "tags": []
   },
   "source": [
    "## Evaluation\n",
    "\n",
    "Our evaluation protocols are the standard ones for SQuAD and related tasks: exact match of the answer (EM) and token-level F1.\n",
    "\n",
    "We say further that the predicted answer is the first line of generated text after the prompt.\n",
    "\n",
    "The following evaluation code is taken from the [apple/ml-qrecc](https://github.com/apple/ml-qrecc/blob/main/utils/evaluate_qa.py) repository. It performs very basic string normalization before doing the core comparisons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "nHHntDSSYrew"
   },
   "outputs": [],
   "source": [
    "def normalize_answer(s: str) -> str:\n",
    "    \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n",
    "\n",
    "    def remove_articles(text):\n",
    "        regex = re.compile(r'\\b(a|an|the)\\b', re.UNICODE)\n",
    "        return re.sub(regex, ' ', text)\n",
    "\n",
    "    def white_space_fix(text):\n",
    "        return ' '.join(text.split())\n",
    "\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return ''.join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "\n",
    "def get_tokens(s: str) -> List[str]:\n",
    "    \"\"\"Normalize string and split string into tokens.\"\"\"\n",
    "    if not s:\n",
    "        return []\n",
    "    return normalize_answer(s).split()\n",
    "\n",
    "\n",
    "def compute_exact(a_gold: str, a_pred: str) -> int:\n",
    "    \"\"\"Compute the Exact Match score.\"\"\"\n",
    "    return int(normalize_answer(a_gold) == normalize_answer(a_pred))\n",
    "\n",
    "\n",
    "def compute_f1_from_tokens(gold_toks: List[str], pred_toks: List[str]) -> float:\n",
    "    \"\"\"Compute the F1 score from tokenized gold answer and prediction.\"\"\"\n",
    "    common = collections.Counter(gold_toks) & collections.Counter(pred_toks)\n",
    "    num_same = sum(common.values())\n",
    "\n",
    "    if len(gold_toks) == 0 or len(pred_toks) == 0:\n",
    "        # If either is no-answer, then F1 is 1 if they agree, 0 otherwise\n",
    "        return int(gold_toks == pred_toks)\n",
    "\n",
    "    if num_same == 0:\n",
    "        return 0\n",
    "\n",
    "    precision = 1.0 * num_same / len(pred_toks)\n",
    "    recall = 1.0 * num_same / len(gold_toks)\n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "    return f1\n",
    "\n",
    "\n",
    "def compute_f1(a_gold: str, a_pred: str) -> float:\n",
    "    \"\"\"Compute the F1 score.\"\"\"\n",
    "    gold_toks = get_tokens(a_gold)\n",
    "    pred_toks = get_tokens(a_pred)\n",
    "    return compute_f1_from_tokens(gold_toks, pred_toks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lJo6C7pgYrex"
   },
   "source": [
    "The following is our general evaluation function. We will make extensive use of it to evaluate different systems:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "bJHSxnA6Yrex"
   },
   "outputs": [],
   "source": [
    "def evaluate(examples, prompts, gens):\n",
    "    \"\"\"Generic evalution function.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    examples: iterable of `SquadExample` instances\n",
    "    prompts: list of str\n",
    "    preds: list of LM-generated texts to evaluate as answers\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    dict with keys \"em_per\", \"macro_f1\", \"examples\", where\n",
    "    each \"examples\" value is a dict\n",
    "    \n",
    "    \"\"\"        \n",
    "    results = []\n",
    "    for ex, prompt, gen in zip(examples, prompts, gens):\n",
    "        answers = ex.answers\n",
    "        pred = gen['generated_answer']\n",
    "        # The result is the highest EM from the available answer strings:\n",
    "        em = max([compute_exact(ans, pred) for ans in answers])\n",
    "        f1 = max([compute_f1(ans, pred) for ans in answers])\n",
    "        gen.update({\n",
    "            \"id\": ex.id, \n",
    "            \"question\": ex.question, \n",
    "            \"prediction\": pred, \n",
    "            \"answers\": answers, \n",
    "            \"em\": em,\n",
    "            \"f1\": f1\n",
    "        })\n",
    "        results.append(gen)\n",
    "    data = {}        \n",
    "    data[\"macro_f1\"] = np.mean([d['f1'] for d in results])\n",
    "    data[\"em_per\"] = sum([d['em'] for d in results]) / len(results)\n",
    "    data[\"examples\"] = results\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fj170C1PYrex"
   },
   "source": [
    "Here is a highly simplified example to help make the logic behind `evaluate` clearer:    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0bgFXLK3Yrex",
    "outputId": "40c6678c-d600-4a2e-b192-84ce765be7d6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'macro_f1': 1.0,\n",
       " 'em_per': 1.0,\n",
       " 'examples': [{'generated_answer': 'NLU',\n",
       "   'generated_text': 'NLU\\nWho am I?',\n",
       "   'id': '0',\n",
       "   'question': 'What is the course to take?',\n",
       "   'prediction': 'NLU',\n",
       "   'answers': ['NLU', 'CS224u'],\n",
       "   'em': 1,\n",
       "   'f1': 1.0}]}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex = namedtuple(\"SquadExample\",  \"id title context question answers\")\n",
    "\n",
    "examples = [\n",
    "    ex(\"0\", \"CS224u\", \n",
    "       \"The course to take is NLU!\", \n",
    "       \"What is the course to take?\", \n",
    "       [\"NLU\", \"CS224u\"])]\n",
    "\n",
    "prompts = [\"Dear model, Please answer this question!\\n\\nQ: What is the course to take?\\n\\nA:\"]\n",
    "\n",
    "gens = [{\"generated_answer\": \"NLU\", \"generated_text\": \"NLU\\nWho am I?\"}]\n",
    "\n",
    "evaluate(examples, prompts, gens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MHZHve9NYrex"
   },
   "source": [
    "The bake-off uses `macro_f1` as the primary metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_3LQv2lbYrex"
   },
   "source": [
    "## Open QA with no context\n",
    "\n",
    "We now have all the pieces we need to begin building few-shot OpenQA systems. Our first system is the simplest and most naive: we simply feed the question text in as the prompt and hope that the model provides an answer as the first line of its generated text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "swF4V0ngYrex"
   },
   "outputs": [],
   "source": [
    "def evaluate_no_context(examples, gen_func=run_eleuther, batch_size=20):\n",
    "    prompts = [] \n",
    "    gens = []\n",
    "    for i in range(0, len(examples), batch_size):\n",
    "        ps = [ex.question for ex in examples[i: i+batch_size]]\n",
    "        gs = gen_func(ps)        \n",
    "        prompts += ps\n",
    "        gens += gs    \n",
    "    return evaluate(examples, prompts, gens)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aPuq5nzDYrex",
    "outputId": "0f93ea63-7cfd-474a-8d95-dea907328438"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0312800712469176\n",
      "CPU times: user 20min 16s, sys: 50.8 s, total: 21min 7s\n",
      "Wall time: 2min 39s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "nocontext_results = evaluate_no_context(bioasq_test)\n",
    "\n",
    "print(nocontext_results['macro_f1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "csAimDMGYrey"
   },
   "source": [
    "## Few-shot QA\n",
    "\n",
    "The above formulation is not especially fair to our model, since it doesn't convey anything about the intended structure of the prompt. We want the model to give us an answer to the input question, but we didn't specify that goal unambiguously. Perhaps we were looking for commentary on the question, or a count of the number of tokens it contains, or a passage containing the question string, or something else entirely.\n",
    "\n",
    "In few-shot QA, we construct a prompt that is intended to convey our intentions more clearly. The first part of the prompt gives some examples of what we want, and the final part provides the set-up for our actual question. In the current formulation, we assume access to the gold passage. For example, if our example of interest is\n",
    "\n",
    "```\n",
    "Title: CS224u\n",
    "\n",
    "Background: The course to take is NLU!\n",
    "\n",
    "Q: What is the course to take?\n",
    "```\n",
    "\n",
    "with gold answer ```NLU```, then we would create a prompt with, say, 2 additional examples preceding this, to yield a full prompt like this:\n",
    "\n",
    "```\n",
    "Title: Pragmatics\n",
    "\n",
    "Background: Pragmatics is the study of language use.\n",
    "\n",
    "Q: What is pragmatics?\n",
    "\n",
    "A: The study of language use\n",
    "\n",
    "Title: Bert\n",
    "\n",
    "Background: Bert is a Muppet who is lives with Ernie.\n",
    "\n",
    "Q: Who is Bert?\n",
    "\n",
    "A: Bert is a  Muppet\n",
    "\n",
    "Title: CS224u\n",
    "\n",
    "Background: The course to take is NLU!\n",
    "\n",
    "Q: What is the course to take?\n",
    "\n",
    "A:\n",
    "```\n",
    "This is essentially the formulation used in the GPT-3 paper for SQuAD. The context examples are drawn randomly from the SQuAD train set. We will adopt this same protocol for now. (You might revisit this in the context of your original system.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "lhsl9yvHYrey"
   },
   "outputs": [],
   "source": [
    "def build_few_shot_qa_prompt(ex, bioasq_train, n_context=2, joiner=\"\\n\\n\"):\n",
    "    segs = []\n",
    "    train_exs = random.sample(bioasq_train, k=n_context)    \n",
    "    for t in train_exs:\n",
    "        segs += [\n",
    "            # f\"Title: {t.title}\",\n",
    "            f\"Background: {t.context}\",\n",
    "            f\"Q: {t.question}\",\n",
    "            f\"A: {t.answers[0]}\"\n",
    "        ]\n",
    "    segs += [\n",
    "        # f\"Title: {ex.title}\",\n",
    "        f\"Background: {ex.context}\",\n",
    "        f\"Q: {ex.question}\",\n",
    "        f\"A:\"\n",
    "    ]\n",
    "    return joiner.join(segs)                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XzLghzI5Yrez"
   },
   "source": [
    "Here's the sort of output we get with `n_context=1`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VEuVae4xYrez",
    "outputId": "c64c39a3-1e16-4890-83c7-cad3b99355a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Background: Over the past decade, MM therapy is significantly improved by the introduction of novel therapeutics such as immunomodulatory agents (thalidomide, lenalidomide, and pomalidomide), proteasome inhibitors (bortezomib, carfilzomib, and ixazomib), monoclonal antibodies (daratumumab and elotuzumab), histone deacetylase (HDAC) inhibitors (Panobinostat). Due to the largely incurable nature of multiple myeloma, the development of newer agents is ongoing and includes new oral PIs (ixazomib), immunotherapies (e.g., CD38- or SLAMF7-targeted antibodies), and small molecules. Ixazomib (MLN9708-MLN2238), the second-generation proteasome inhibitor, selectivity and potency were similar to that of bortezomib, is currently being investigated in phase I studies. In the last few weeks, the FDA approved three new therapies for multiple myeloma: ixazomib, the first oral proteasome inhibitor; and daratumumab and elotuzumab, two monoclonal antibodies that target CD38 and SLAMF7, respectively. Ixazomib, the first oral proteasome inhib\n",
      "\n",
      "Q: Which enzyme is inhibited by ixazomib?\n",
      "\n",
      "A: proteasome\n",
      "\n",
      "Background: pre-exposure prophylaxis (PrEP) and the recent approval by the FDA of the supplemental indication for Truvada as PrEP tenofovir disoproxil fumarate 300 mg (TDF)/emtricitabine 200 mg (FTC) (Truvada, Gilead Sciences) as antiretroviral preexposure prophylaxis (PrEP) to reduce the risk for HIV acquisition In January 2011, following publication of evidence of safety and efficacy of daily oral tenofovir disoproxil fumarate 300 mg (TDF)/emtricitabine 200 mg (FTC) (Truvada, Gilead Sciences) as antiretroviral preexposure prophylaxis (PrEP) to reduce the risk for HIV acquisition among MSM in the iPrEx trial, CDC issued interim guidance to make available information and important initial cautions on the use of PrEP in this population. In this commentary, we review literature on sexual behavior change accompanying PrEP use, discuss risk compensation concerns and the &amp;quot;Truvada whore&amp;quot; stereotype as PrEP barriers, question the appropriateness of restricting PrEP access because of risk compensation, and cons\n",
      "\n",
      "Q: What is the indication for Truvada?\n",
      "\n",
      "A:\n"
     ]
    }
   ],
   "source": [
    "print(build_few_shot_qa_prompt(dev_exs[2], bioasq_train, n_context=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "VUwgM625Yrez"
   },
   "outputs": [],
   "source": [
    "def evaluate_few_shot_qa(examples, bioasq_train, gen_func=run_eleuther, batch_size=20, n_context=2):\n",
    "    prompts = []\n",
    "    gens = []\n",
    "    for i in range(0, len(examples), batch_size):\n",
    "        batch = examples[i: i+batch_size]\n",
    "        ps = [build_few_shot_qa_prompt(ex, bioasq_train, n_context=n_context) for ex in batch]        \n",
    "        gs = gen_func(ps)       \n",
    "        prompts += ps\n",
    "        gens += gs\n",
    "    return evaluate(examples, prompts, gens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UelblRFCYrez",
    "outputId": "39b49fbd-bc7e-42c8-8f8d-31c31652b966"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "few_shot_qa_results = evaluate_few_shot_qa(bioasq_test, bioasq_train, n_context=1)\n",
    "\n",
    "print(few_shot_qa_results['macro_f1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WfvVdsGrYre0"
   },
   "source": [
    "## ColBERT\n",
    "\n",
    "It's now just a short step to our core task, few-shot OpenQA. We just need to give up our beloved gold passage and instead try to retrieve the right passage or passages from a corpus. \n",
    "\n",
    "The first step is instantiating the ColBERT retriever and loading in an index. Our ColBERT retriever was initially trained on MS MARCO, and we have pre-indexed a collection of 100K documents that we know to be well-aligned with SQuAD and with the dataset used for the bake-off assessment. (See [the original system question](#Your-original-system-[3-points]) for tips on creating your own index.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "VCt6-Oe2zTio"
   },
   "outputs": [],
   "source": [
    "index_home = os.path.join(\"experiments\", \"notebook\", \"indexes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NFYxJPpuYre0"
   },
   "source": [
    "### ColBERT parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "5tnUU2UHYre0"
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(os.path.join(\"data\", \"openqa\", \"colbertv2.0.tar.gz\")):\n",
    "    !mkdir -p data/openqa\n",
    "    # ColBERTv2 checkpoint trained on MS MARCO Passage Ranking (388MB compressed)\n",
    "    !wget https://downloads.cs.stanford.edu/nlp/data/colbert/colbertv2/colbertv2.0.tar.gz -P data/openqa/\n",
    "    !tar -xvzf data/openqa/colbertv2.0.tar.gz -C data/openqa/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cjZ1hZJnYre0"
   },
   "source": [
    "If something went wrong with the above, you can just download the file https://downloads.cs.stanford.edu/nlp/data/colbert/colbertv2/colbertv2.0.tar.gz, unarchive it, and move the resulting `colbertv2.0` directory into the `data/openqa` directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QzRjL61eYre0"
   },
   "source": [
    "### ColBERT index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "jD0U5Outa9HU"
   },
   "outputs": [],
   "source": [
    "# if not os.path.exists(os.path.join(index_home, \"cs224u.collection.2bits.tgz\")):\n",
    "#     !wget https://web.stanford.edu/class/cs224u/data/cs224u.collection.2bits.tgz -P experiments/notebook/indexes\n",
    "#     !tar -xvzf experiments/notebook/indexes/cs224u.collection.2bits.tgz -C experiments/notebook/indexes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we use our created index for bioasq passages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "id": "XtEGC6MyYre0",
    "outputId": "c6553c0f-598f-4d37-c96b-0766aaef1dfe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Jun 03, 20:22:53] #> Loading collection...\n",
      "0M \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Loaded 2,068 passages'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_home = './experiments/bioasq/indexes'\n",
    "\n",
    "collection = os.path.join(index_home, \"bioasq.all.2bits\", \"bioasq_passage.tsv\")\n",
    "\n",
    "collection = Collection(path=collection)\n",
    "\n",
    "f'Loaded {len(collection):,} passages'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "BixmAYizYre0"
   },
   "outputs": [],
   "source": [
    "index_name = \"bioasq.all.2bits\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "83oO75Z1zTio"
   },
   "source": [
    "Now we create our `searcher`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N6wYpChzYre1",
    "outputId": "4436fe5f-55fa-43d0-ca4b-1d3a70c51d7e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Jun 03, 20:22:57] #> Loading collection...\n",
      "0M \n",
      "[Jun 03, 20:23:14] #> Building the emb2pid mapping..\n",
      "[Jun 03, 20:23:14] len(self.emb2pid) = 378124\n"
     ]
    }
   ],
   "source": [
    "with Run().context(RunConfig(experiment='bioasq')):\n",
    "    searcher = Searcher(index=index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2068"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(searcher.collection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dex5KPUTYre1"
   },
   "source": [
    "### Search\n",
    "\n",
    "Now that the index is loaded, you can do searches over it. The index is limited, but retrieval is very solid!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-v2jKaR8Yre1",
    "outputId": "6380a7fe-97bf-4f7e-cf30-195493c55670"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#> biomarker\n",
      "\n",
      "#> QueryTokenizer.tensorize(batch_text[0], batch_background[0], bsize) ==\n",
      "#> Input: . biomarker, \t\t True, \t\t None\n",
      "#> Output IDs: torch.Size([32]), tensor([  101,     1, 16012, 10665,  2121,   102,   103,   103,   103,   103,\n",
      "          103,   103,   103,   103,   103,   103,   103,   103,   103,   103,\n",
      "          103,   103,   103,   103,   103,   103,   103,   103,   103,   103,\n",
      "          103,   103])\n",
      "#> Output Mask: torch.Size([32]), tensor([1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])\n",
      "\n",
      "\t[1]\t21.6\t The target genes SEC22B, RAB10, and FLT1 may be potential biomarkers of AD.\n",
      "\t[2]\t21.5\t A total of 92 biomarkers were measured before a standardized meal as well as 30 and 120 minutes afterwards with the Proseek Multiplex CVD III kit PROSEEK Multiplex CVD and PROSEEK Multiplex INF A multiplex proximity extension assay allowed us to measure 157 cardiovascular disease (CVD) and inflammatory disease-related biomarkers in patients from the international, multicenter, and randomized trial; Plasma concentrations of MMP12 were measured at baseline in 3394 subjects with high-risk for cardiovascular disease (CVD) using the Olink ProSeek CVD I array. The samples were analyzed with a multiplex proximity extension assay in which 92 inflammation-related proteins were measured simultaneously (Proseek Multiplex Inflammation I; Olink Bioscience, Uppsala, Sweden). A total of 92 biomarkers were measured before a standardized meal as well as 30 and 120 min afterward with the Proseek Multiplex Neurology I kit. We used proximity extension immunoassay (PEA, Proseek Multiplex, Olink) to assess the serum levels of nine\n",
      "\t[3]\t21.4\t To date, there have been a limited number of useful biomarkers for the screening and monitoring of B-cell non-Hodgkin's lymphoma (B-NHL), which leads to the impetus to discover novel biomarkers for the disease. immune activation precedes non-Hodgkin lymphoma diagnosis by several years. Decreased B-cell activating factor levels may denote nascent chronic lymphocytic leukemia many years pre-diagnosis. Soluble interleukin-2 receptor-α, CXC chemokine ligand 13, soluble CD30, and soluble tumor necrosis factor receptor-2 were individually positively associated, and B-cell activating factor of the tumor necrosis factor family inversely associated, with all non-Hodgkin lymphoma and one or more subtypes. GALECTIN-3 AS A PROGNOSTIC BIOMARKER IN PATIENTS WITH NON-HODGKIN LYMPHOMA.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "query = \"biomarker\"\n",
    "\n",
    "print(f\"#> {query}\")\n",
    "\n",
    "# Find the top-3 passages for this query\n",
    "results = searcher.search(query, k=3) \n",
    "\n",
    "# Print out the top-k retrieved passages\n",
    "for passage_id, passage_rank, passage_score in zip(*results):\n",
    "    print(f\"\\t[{passage_rank}]\\t{passage_score:.1f}\\t {searcher.collection[passage_id]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w9n0_xwdYre1"
   },
   "source": [
    "### Retrieval evaluation\n",
    "\n",
    "For more rigorous evaluations of the retriever alone, we can use Sucess@`k` defined relative to the SQuAD passages and answers. We say that we have a \"success\" if a passage in the top `k` retrieved passages contains any of the answers substrings, and Sucess@`k` is the percentage of such success cases. This is very heuristic (perhaps the answer string happens to occur somewhere in a completely irrelevant passage), but it can still be good guidance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "fsNayHzdYre1"
   },
   "outputs": [],
   "source": [
    "def success_at_k(examples, k=20):\n",
    "    scores = []\n",
    "    for ex in examples: \n",
    "        scores.append(evaluate_retrieval_example(ex, k=5))\n",
    "    return sum(scores) / len(scores)\n",
    "        \n",
    "    \n",
    "def evaluate_retrieval_example(ex, k=20):    \n",
    "    results = searcher.search(ex.question, k=k)\n",
    "    for passage_id, passage_rank, passage_score in zip(*results):\n",
    "        passage = searcher.collection[passage_id]\n",
    "        score = has_answer([DPR_normalize(ans) for ans in ex.answers], passage)\n",
    "        if score:\n",
    "            return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k2U4v58dYre1"
   },
   "source": [
    "Here is Sucess@20 for the SQuAD dev set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "J2oEmspeYre1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7681159420289855\n",
      "CPU times: user 33.9 s, sys: 1.17 s, total: 35 s\n",
      "Wall time: 4.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if torch.cuda.is_available():\n",
    "    # This will take a few hours on a CPU:\n",
    "    print(success_at_k(bioasq_dev))\n",
    "else:\n",
    "    # This should be reasonably fast and yields the\n",
    "    # same kind of result:\n",
    "    print(success_at_k(bioasq_dev))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "teKobQM8Yre1"
   },
   "source": [
    "## Zero-shot OpenQA with ColBERT retrieval\n",
    "\n",
    "We're now in a position to define a system that does our full few-shot OpenQA task. To get this started, we define just a version that doesn't include any SQuaD-training examples in the prompt. So this is really zero-shot OpenQA. (The homework asks you to move to the true few-shot setting.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "jbn4BHr4Yre1"
   },
   "outputs": [],
   "source": [
    "def build_zero_shot_openqa_prompt(question, passage, joiner=\"\\n\\n\"):\n",
    "    ## since there is no title, passage itself is context\n",
    "    context = passage\n",
    "    \n",
    "    segs = [\n",
    "        # f\"Title: {title}\",\n",
    "        f\"Background: {context}\",\n",
    "        f\"Q: {question}\",\n",
    "        \"A:\"\n",
    "    ]\n",
    "    return joiner.join(segs)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "Xjz7l_yQYre2"
   },
   "outputs": [],
   "source": [
    "def evaluate_zero_shot_openqa(examples, joiner=\"\\n\\n\", gen_func=run_eleuther, batch_size=20):\n",
    "    prompts = []\n",
    "    gens = []\n",
    "    for i in range(0, len(examples), batch_size):\n",
    "        exs = examples[i: i+batch_size]\n",
    "        results = [searcher.search(ex.question, k=1) for ex in exs]\n",
    "        passages = [searcher.collection[r[0][0]] for r in results]\n",
    "        \n",
    "        ps = [build_zero_shot_openqa_prompt(ex.question, psg, joiner=joiner) \n",
    "              for ex, psg in zip(exs, passages)]\n",
    "        gs = gen_func(ps)       \n",
    "        prompts += ps\n",
    "        gens += gs\n",
    "    return evaluate(examples, prompts, gens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "21DfS0hHYre2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0904595812684048\n",
      "CPU times: user 40.4 s, sys: 9.62 s, total: 50 s\n",
      "Wall time: 6.26 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "zero_shot_openqa_results = evaluate_zero_shot_openqa(dev_exs)\n",
    "print(zero_shot_openqa_results['macro_f1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "outblNTaYre2"
   },
   "source": [
    "## Homework questions\n",
    "\n",
    "Please embed your homework responses in this notebook, and do not delete any cells from the notebook. (You are free to add as many cells as you like as part of your responses.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R55lgmnTzTip"
   },
   "source": [
    "### Few-shot OpenQA with no context [2 points]\n",
    "\n",
    "In the section [Open QA with no context](#Open-QA-with-no-context) above, we simply prompted our LM with a question string and looked at what came back. This is arguably unfair to the LM, since we didn't convey anything about our intentions.\n",
    "\n",
    "For a fairer assessment of what the LM alone can do, we should move to the few-shot setting by giving the model a few examples of what we have in mind. The idea here is to create prompts that look like this:\n",
    "\n",
    "   ```   \n",
    "   Q: What is pragmatics?\n",
    "\n",
    "   A: The study of language use\n",
    "\n",
    "   Q: Who is Bert?\n",
    "\n",
    "   A: Bert is one of the Muppets.\n",
    "\n",
    "   Q: What was Stanford University founded?\n",
    "   \n",
    "   A: \n",
    "   ```\n",
    "   \n",
    "This question asks you to write a function for creating such prompts, using SQuAD training examples, and a second function for evaluating this approach. The goal is to have a no context baseline for the other few-shot approaches we are considering.\n",
    "\n",
    "__Task 1___: Complete the function `build_few_shot_no_context_prompt` so that it builds prompts like the above. You can use `test_build_few_shot_no_context_prompt` to check that your function is returning prompts in the desired format.\n",
    "\n",
    "__Task 2__: Complete the function `evaluate_few_shot_no_context` so that you can evaluate this approach. You can use `test_evaluator` to check that your function is performing the desired kind of evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "jV7ySPA8zTip"
   },
   "outputs": [],
   "source": [
    "def build_few_shot_no_context_prompt(question, train_exs, joiner=\"\\n\\n\"):\n",
    "    \"\"\"No context few-shot OpenQA prompts.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    question : str   \n",
    "    train_exs : iterable of SQuAD train examples. These can be \n",
    "        obtained via a random sample \n",
    "        from `squad_train` as defined above.\n",
    "    joiner : str\n",
    "        The character to use to join pieces of the prompt into \n",
    "        a single str.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str, the prompt\n",
    "\n",
    "    \"\"\"\n",
    "    ##### YOUR CODE HERE\n",
    "    segs = []\n",
    "    # train_exs = random.sample(train_exs, k = len(train_exs))    \n",
    "    for t in train_exs:\n",
    "        segs += [\n",
    "            f\"Q: {t.question}\",\n",
    "            f\"A: {t.answers[0]}\"\n",
    "        ]\n",
    "    segs += [\n",
    "        f\"Q: {question}\",\n",
    "        f\"A:\"\n",
    "    ]\n",
    "    return joiner.join(segs)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "56kO5vE-zTiq"
   },
   "outputs": [],
   "source": [
    "def evaluate_few_shot_no_context(\n",
    "        examples,\n",
    "        squad_train,\n",
    "        batch_size=20,\n",
    "        n_context=2,\n",
    "        joiner=\"\\n\\n\",\n",
    "        gen_func=run_eleuther):\n",
    "    \"\"\"Evaluate a few-shot OpenQA with no context approach \n",
    "    defined by `build_few_shot_no_context_prompt` and `gen_func`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    examples : iterable of SQuAD train examples\n",
    "        Presumably a subset of `squad_dev` as defined above.\n",
    "    squad_train : iterable of SQuAD train examples\n",
    "    batch_size : int\n",
    "        Number of examples to send to `gen_func` at once.\n",
    "    joiner : str\n",
    "        Used by `build_few_shot_open_qa_prompt` to join segments\n",
    "        of the prompt into a single str.\n",
    "    gen_func : either `run_eleuther` or `run_gpt3`\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict as determined by `evaluate` above.\n",
    "\n",
    "    \"\"\"\n",
    "    # A list of strings that you build and feed into `gen_func`.\n",
    "    prompts = []\n",
    "\n",
    "    # A list of dicts that you get from `gen_func`.\n",
    "    gens = []\n",
    "\n",
    "    # Iterate through the examples in batches:\n",
    "    for i in range(0, len(examples), batch_size):\n",
    "        # Sample some SQuAD training examples to use with\n",
    "        # `build_few_shot_no_context_prompt` and `ex.question`,\n",
    "        # run the resulting prompt through `gen_func`, and\n",
    "        # add your prompts and results to `prompts` and `gens`.\n",
    "\n",
    "        ##### YOUR CODE HERE\n",
    "\n",
    "        # squad dev examples in batch\n",
    "        batch = examples[i: i+batch_size]\n",
    "\n",
    "        # build training to sample the Q/A pair from squad train\n",
    "        train_exs = random.sample(squad_train, k=n_context)\n",
    "\n",
    "        # build prompt using dev questions and Q/A pair from squad train\n",
    "        ps = [build_few_shot_no_context_prompt(ex.question, train_exs, joiner=joiner) for ex in batch]  \n",
    "\n",
    "        # feed that into run_eleuther\n",
    "        gs = gen_func(ps) \n",
    "\n",
    "        # append the results\n",
    "        prompts += ps\n",
    "        gens += gs\n",
    "\n",
    "    # Return value from a call to `evalaute`, with `examples`\n",
    "    # as provided by the user and the `prompts` and `gens`\n",
    "    # you built:\n",
    "    return evaluate(examples, prompts, gens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "i0oYyNiOzTiq"
   },
   "outputs": [],
   "source": [
    "def test_evaluator(func):\n",
    "    examples = [SquadExample(0, \"T1\", \"Q1\", \"C1\", [\"A1\"])]    \n",
    "    squad_train = [SquadExample(0, \"sT1\", \"sQ1\", \"sC1\", [\"sA1\"])] \n",
    "    \n",
    "    def gen_func(*prompts):\n",
    "        return [{\n",
    "            \"generated_answer\": \"Constant output\", \n",
    "            \"generated_answer_tokens\": [\"Constant\", \"output\"], \n",
    "            \"generated_answer_probs\": [0.1, 0.2]}]\n",
    "    \n",
    "    batch_size = 1    \n",
    "    n_context = 1    \n",
    "    joiner = \"\\n\"\n",
    "    result = func(\n",
    "        examples, \n",
    "        squad_train, \n",
    "        batch_size=1, \n",
    "        n_context=1, \n",
    "        joiner=joiner, \n",
    "        gen_func=gen_func)\n",
    "    expected_keys = {'em_per', 'examples', 'macro_f1'}\n",
    "    result_keys = set(result.keys())     \n",
    "    if expected_keys != result_keys:\n",
    "        print(f\"Unexpected keys in result. \"\n",
    "              f\"Expected: {expected_keys}; Got: {result_keys}\")\n",
    "        return\n",
    "    expected_ex_keys = {\n",
    "        'f1', 'id', 'em', 'generated_answer_tokens', 'generated_answer_probs',\n",
    "        'prediction', 'generated_answer', 'question', 'answers'}\n",
    "    result_ex_keys = set(result[\"examples\"][0].keys())\n",
    "    if expected_ex_keys != result_ex_keys:\n",
    "        print(f\"Unexpected keys in result['examples']. \"\n",
    "              f\"Expected: {expected_ex_keys}; Got: {result_ex_keys}\")\n",
    "        return\n",
    "    print(\"No errors detected in `evaluate_few_shot_open_qa`\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MnZhwidzzTiq",
    "outputId": "a7f30855-8f58-4b5f-8f92-4bdc34426f1e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No errors detected in `evaluate_few_shot_open_qa`\n"
     ]
    }
   ],
   "source": [
    "test_evaluator(evaluate_few_shot_no_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w2mx3Z4HYre2"
   },
   "source": [
    "### Few-shot OpenQA [2 points]\n",
    "\n",
    "In the section [Few-shot QA](Few-shot-QA) above, we used SQuAD training examples to build prompts that we hope will help the model infer our intended semantics for the prompts themselves. When we moved to the open formulation of the problem, in [Open QA with ColBERT retrieval](Open-QA-with-ColBERT-retrieval), we forced the model to deal with prompts that lack these context clues. This is a \"zero-shot\" formulation of the problem. The goal of this homework problem is to improve that system so that it truly supports few-shot OpenQA.\n",
    "\n",
    "__Task 1__: Complete the function `build_few_shot_open_qa_prompt` so that it builds prompts from a question, a passage, and a sample of SQuAD training examples. You can use `test_build_few_shot_open_qa_prompt` to check that your function is returning prompts in the desired format.\n",
    "\n",
    "__Task 2__: Complete the function `evaluate_few_shot_open_qa` so that you can evaluate this approach. You can use `test_evaluator` from above to check that your function is performing the desired kind of evaluation.\n",
    "\n",
    "We will be checking only that the tests pass. We will not be evaluating the quality of the results you obtain using this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "HUuZ5l3gYre2"
   },
   "outputs": [],
   "source": [
    "def build_few_shot_open_qa_prompt(question, passage, train_exs, joiner=\"\\n\\n\"):\n",
    "    \"\"\"Few-shot OpenQA prompts.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    question : str\n",
    "    passage : str\n",
    "        Presumably something retrieved via search.\n",
    "    train_exs : iterable of SQuAD train examples\n",
    "        These can be obtained via a random sample from \n",
    "        `squad_train` as defined above.\n",
    "    joiner : str\n",
    "        The character to use to join pieces of the prompt \n",
    "        into a single str.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str, the prompt\n",
    "\n",
    "    \"\"\"\n",
    "    ##### YOUR CODE HERE\n",
    "    passage_context = passage\n",
    "    \n",
    "    segs = []\n",
    "\n",
    "    for t in train_exs:\n",
    "        segs += [\n",
    "            # f\"Title: {t.title}\",\n",
    "            f\"Background: {t.context}\",\n",
    "            f\"Q: {t.question}\",\n",
    "            f\"A: {t.answers[0]}\"\n",
    "        ]\n",
    "    segs += [\n",
    "            # f\"Title: {passage_title}\",\n",
    "            f\"Background: {passage_context}\",\n",
    "            f\"Q: {question}\",\n",
    "            f\"A:\"\n",
    "    ]\n",
    "    return joiner.join(segs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "DRhoMeEGYre3"
   },
   "outputs": [],
   "source": [
    "def evaluate_few_shot_open_qa(\n",
    "        examples,\n",
    "        squad_train,\n",
    "        batch_size=20,\n",
    "        n_context=2,\n",
    "        joiner=\"\\n\\n\",\n",
    "        gen_func=run_eleuther):\n",
    "    \"\"\"Evaluate a few-shot OpenQA approach defined by \n",
    "    `build_few_shot_open_qa_prompt` and `gen_func`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    examples : iterable of SQuAD train examples\n",
    "        Presumably a subset of `squad_dev` as defined above.\n",
    "    squad_train : iterable of SQuAD train examples\n",
    "    batch_size : int\n",
    "        Number of examples to send to `gen_func` at once.\n",
    "    joiner : str\n",
    "        Used by `build_few_shot_open_qa_prompt` to join segments\n",
    "        of the prompt into a single str.\n",
    "    gen_func : either `run_eleuther` or `run_gpt3`\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict as determined by `evaluate` above.\n",
    "\n",
    "    \"\"\"\n",
    "    # A list of strings that you build and feed into `gen_func`.\n",
    "    prompts = []\n",
    "\n",
    "    # A list of dicts that you get from `gen_func`.\n",
    "    gens = []\n",
    "\n",
    "    # Iterate through the examples in batches:\n",
    "    for i in range(0, len(examples), batch_size):\n",
    "        # Use the `searcher` defined above to get passages\n",
    "        # using `ex.question` as the query, and use your\n",
    "        # `build_few_shot_open_qa_prompt` to build prompts.\n",
    "\n",
    "        ##### YOUR CODE HERE\n",
    "        \n",
    "        batch = examples[i: i+batch_size]\n",
    "\n",
    "        # sample training from squad_train\n",
    "        train_exs = random.sample(squad_train, k=n_context)\n",
    "\n",
    "        ## get a passage for each example in the dev batch\n",
    "        # get search results (passage index)\n",
    "        results = [searcher.search(ex.question, k=1) for ex in batch]\n",
    "\n",
    "        # from passage index to get the passage 'title | passage'\n",
    "        passages = [searcher.collection[r[0][0]] for r in results]\n",
    " \n",
    "        ps = []\n",
    "\n",
    "        # for every question, combine the find passage and generate the prompt\n",
    "        # append all prompt into a list\n",
    "        for ex, psg in zip(batch, passages):\n",
    "            ps.append(build_few_shot_open_qa_prompt(ex.question, psg, train_exs, joiner=joiner))  \n",
    "\n",
    "        # feed prompt to gen_func\n",
    "        gs = gen_func(ps)       \n",
    "\n",
    "        # add the prompt to prompt list\n",
    "        prompts += ps\n",
    "        # add generated txt to gen list\n",
    "        gens += gs\n",
    "\n",
    "\n",
    "    # Return value from a call to `evalaute`, with `examples`\n",
    "    # as provided by the user and the `prompts` and `gens`\n",
    "    # you built:\n",
    "    return evaluate(examples, prompts, gens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oXeQzplkYre3"
   },
   "source": [
    "### Answer scoring [2 points]\n",
    "\n",
    "We have so far been assuming that the top-ranked passage retrieved by ColBERT should be used in the prompt and that the single answer returned by the LM is our prediction. It may be possible to improve on this by scoring answers using the ColBERT scores and the probabilities returned by the LM. This question asks you to explore a basic approach to such scoring. The core scoring function:\n",
    "\n",
    "$$\n",
    "\\textbf{score}_{\\text{prompt-func}}(\\textrm{answer}, \\textrm{passage}, \\textrm{question}) = \n",
    "P(\\textrm{passage} \\mid \\textrm{question}) \\cdot \n",
    "P(\\textrm{answer} \\mid \\text{prompt-func}(\\textrm{question}, \\textrm{passage}) ) \n",
    "$$\n",
    "\n",
    "where we estimate the two conditional probabilities as follows:\n",
    "\n",
    "* $P(\\textrm{passage} \\mid \\textrm{question})$ is defined only for the top $k$ passages and defined by the softmax of the top $k$ scores returned by the retriever.\n",
    "\n",
    "* $P(\\textrm{answer} \\mid \\text{prompt-func}(\\textrm{question}, \\textrm{passage}))$ is simply the product of the per-token probabilities of the generated answer given the prompt determined by $\\text{prompt-func}(\\textrm{question}, \\textrm{passage})$. These values can be extracted from the return values of both `run_eleuther` and `run_gpt3` using the key `\"generated_answer_probs\"`. (Your prompt function might of course have other arguments not represented here.)\n",
    "\n",
    "__Your task__: Implement this scoring function for an individual example. The two required pieces are `get_passages_with_scores` and `answer_scoring`. Starter code for each is below, and each has a unit test you can run to check your work.\n",
    "\n",
    "(With this implemented, it is easy to create a new prediction function that uses the $\\textrm{answer}$ from the highest-scoring $\\textrm{answer}/\\textrm{passage}$ pair as the prediction for input $\\textrm{question}$. You are not required to implement such a prediction function, but you might do this as part of [your original system](#Your-original-system-[3-points]).)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "m7PhfMNsYre3"
   },
   "outputs": [],
   "source": [
    "def get_passages_with_scores(question, k=5):\n",
    "    \"\"\"Pseudo-probabilities from the retriever.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    question : str\n",
    "    k : int\n",
    "        Number of passages to retrieve.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    passages (list of str), passage_probs (np.array)\n",
    "\n",
    "    \"\"\"\n",
    "    # Use the `searcher` to get `k` passages for `questions`:\n",
    "    ##### YOUR CODE HERE\n",
    "    search_score = searcher.search(question, k = k)[2]\n",
    "    passage_index = searcher.search(question, k = k)[0]\n",
    "\n",
    "    # Softmax normalize the scores and convert the list to\n",
    "    # a NumPy array:\n",
    "    ##### YOUR CODE HERE\n",
    "    exp_score = np.exp(search_score)\n",
    "    sum_score = np.sum(exp_score) \n",
    "    passage_probs = np.array([score/sum_score for score in exp_score] )\n",
    "\n",
    "    # Get the passages as a list of texts:\n",
    "    ##### YOUR CODE HERE\n",
    "\n",
    "    passages = [searcher.collection[idx] for idx in passage_index]\n",
    "\n",
    "    return passages, passage_probs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "-vqCNOtMYre4"
   },
   "outputs": [],
   "source": [
    "def test_get_passages_with_scores(func):\n",
    "    question = \"What is linguistics?\"        \n",
    "    passages, passage_probs = get_passages_with_scores(question, k=2)    \n",
    "    if len(passages) != len(passage_probs):\n",
    "        print(\"`get_passages_with_scores` should return equal length \"\n",
    "              \"lists of passages and passage probabilities.\")\n",
    "        return\n",
    "    if len(passages) != 2:\n",
    "        print(f\"`get_passages_with_scores` should return `k` passages. Yours returns {len(passages)}\")\n",
    "        return\n",
    "    if not all(isinstance(psg, str) for psg in passages):\n",
    "        print(\"The first return argument should be a list of passage strings.\")\n",
    "        return\n",
    "    if not all(isinstance(p, (float, np.float32, np.float64)) for p in passage_probs): \n",
    "        print(\"The second return argument should be a list of floats.\")\n",
    "        return \n",
    "    print(\"No errors detected in `get_passages_with_scores`\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kfsb4pyHYre4",
    "outputId": "fd6997ff-23c7-4c3f-b9ec-0aae07577631"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No errors detected in `get_passages_with_scores`\n"
     ]
    }
   ],
   "source": [
    "test_get_passages_with_scores(get_passages_with_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "SwQVdCb6Yre4"
   },
   "outputs": [],
   "source": [
    "from types import GeneratorType\n",
    "def answer_scoring(passages, passage_probs, prompts, gen_func=run_eleuther):\n",
    "    \"\"\"Implements our basic scoring strategy.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    passages : list of str\n",
    "    passage_probs : list of float\n",
    "    prompts : list of str\n",
    "    gen_func : either `run_eleuther` or `run_gpt3`\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list of pairs (score, dict), sorted with the largest score first.\n",
    "    `dict` should be the return value of `gen_func` for an example.\n",
    "\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    for passage, passage_prob, prompt in zip(passages, passage_probs, prompts):\n",
    "        # Run `gen_func` on [prompt] (crucially, the singleton list here),\n",
    "        # and get the dictionary `gen` from the singleton list `gen_func`\n",
    "        # returns, and then use the values to score `gen` according to our\n",
    "        # scoring method.\n",
    "        #\n",
    "        # Be sure to use \"generated_answer_probs\" for the scores.\n",
    "        ##### YOUR CODE HERE\n",
    "\n",
    "        gen = gen_func([prompt])\n",
    "\n",
    "        # print(gen)\n",
    "        \n",
    "        answer_score = np.prod(gen[0]['generated_answer_probs'])\n",
    "\n",
    "        final_score = passage_prob*answer_score\n",
    "        \n",
    "        data.append((final_score, gen[0]))\n",
    "\n",
    "\n",
    "    # Return `data`, sorted with the highest scoring `(score, gen)`\n",
    "    # pair given first.\n",
    "    ##### YOUR CODE HERE\n",
    "    data.sort(key = lambda x: x[0], reverse=True)\n",
    "\n",
    "    return data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "JD7d8ucgYre4"
   },
   "outputs": [],
   "source": [
    "def test_answer_scoring(func):\n",
    "    passages = [\n",
    "        \"Pragmatics is the study of language use.\", \n",
    "        \"Phonology is the study of linguistic sound systems.\"]\n",
    "    passage_probs = [0.75, 0.25]\n",
    "    prompts = passages\n",
    "    \n",
    "    def gen_func(*prompts):\n",
    "        return [{\n",
    "            \"generated_answer\": \"Constant output\", \n",
    "            \"generated_answer_tokens\": [\"Constant\", \"output\"], \n",
    "            \"generated_answer_probs\": [0.1, 0.2]}]\n",
    "    \n",
    "    data = func(passages, passage_probs, prompts, gen_func=gen_func)\n",
    "    \n",
    "    if not all(len(x) == 2 for x in data):\n",
    "        print(\"`answer_scoring` should return a list of pairs (score, gen)\")\n",
    "        return \n",
    "    if not isinstance(data[0][0], (float, np.float32, np.float64)):\n",
    "        print(\"The first member of each pair in `data` should be a score (type `float`).\")\n",
    "        return    \n",
    "    if not isinstance(data[0][1], dict):\n",
    "        print(\"The second member of each pair in `data` should be a dict \" \n",
    "              \"created by running `gen_func` on a single example.\")\n",
    "        return    \n",
    "    if data[0][0] != max([x for x, y in data]):\n",
    "        print(\"`answer_scoring` should sort its data with the highest score first.\")\n",
    "        return \n",
    "    \n",
    "    print(\"No errors detected in `answer_scoring`\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QWwwDzvBYre4",
    "outputId": "c7829cf0-f8b8-498e-a3cf-733b465132fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No errors detected in `answer_scoring`\n"
     ]
    }
   ],
   "source": [
    "test_answer_scoring(answer_scoring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "OUcdfU9SYre4"
   },
   "outputs": [],
   "source": [
    "def answer_scoring_demo(question):\n",
    "    \"\"\"Example usage for answer_scoring. Here we extract the top-scoring\n",
    "    results, which can then be used in an evaluation.\"\"\"    \n",
    "    passages, passage_probs = get_passages_with_scores(question)\n",
    "    prompts = [build_zero_shot_openqa_prompt(question, psg) for psg in passages]\n",
    "    # for p in prompts:\n",
    "    #   print(p)\n",
    "    data = answer_scoring(passages, passage_probs, prompts)\n",
    "    # Top result:\n",
    "    return data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y1DGZOuVzTir"
   },
   "outputs": [],
   "source": [
    "answer_scoring_demo(\"How long is Moby Dick?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PmURWvoJYre4",
    "tags": []
   },
   "source": [
    "### Your original system [3 points]\n",
    "\n",
    "This question asks you to design your own few-shot OpenQA system. All of the code above can be used and modified for this, and the requirement is just that you try something new that goes beyond what we've done so far. \n",
    "\n",
    "Terms for the bake-off:\n",
    "\n",
    "* You can make free use of SQuAD and other publicly available data.\n",
    "* The LM must be an autoregressive language model. No trained QA components can be used. Our list of preallowed models are those available via the OpenAI API whose names begin with \"text\" and the Eluether models \"gpt-neo-125M\", \"gpt-neo-1.3B\", \"gpt-neo-2.7B\", and \"gpt-j-6B\". If you would like to use a model outside of this set, please check with the teaching team first.\n",
    "\n",
    "Here are some ideas for the original system:\n",
    "\n",
    "* We have so far sampled randomly from the SQuaD train set to create few-shot prompts. One might instead sample passages that have some connection to the target question.\n",
    "\n",
    "* We have used actual SQuAD training examples to build contexts. These might be different in meaningful ways from the passages in our corpus. An alternative is to use the SQuAD question–answer pairs to retrieve passages that contain the answer and use the resulting question–answer–passage triple when building prompts.\n",
    "\n",
    "* There are a lot of parameters to our LMs that we have so far ignored. Exploring different values might lead to better results. The `temperature` parameter is highly impactful for our task.\n",
    "\n",
    "* We have distributed a fixed index of 100K passages. These cover SQuAD plus our bake-off data, but there might still be value in creating a different/expanded index. There is starter code for indexing data with ColBERT [here](https://github.com/stanford-futuredata/ColBERT/blob/new_api/docs/intro.ipynb).\n",
    "\n",
    "* [Khattab et al. (2021a)](https://aclanthology.org/2021.tacl-1.55/) fine-tune the retriever through a handful of successive rounds, using weak supervision from the QA dataset. This is an ambitious direction that could quickly build to an original project, as the role of retriever training is under-explored so far in the context of few-shot OpenQA.\n",
    "\n",
    "* In our \"Answer scoring\" question, we don't normalize scores by answer length. Such normalization might be fairer to long answers and so seems worth adding.\n",
    "\n",
    "* Our \"Answer scoring\" question is inspired by the Retrieval Augmented Generation (RAG) model of [Lewis et al. 2020](https://arxiv.org/abs/2005.11401). Their model fully marginalizes over $k$ retrieved passages to create a proper model of $P(\\textrm{answer} \\mid \\textrm{question})$. Implementing this requires having the probabilities for the prompts. For GPT-3, these can be obtained with `echo=False`, which will lead you to have to make changes to the output processing of `run_gpt3`. For the Eleuther models, one needs to do another call to the model forward function. Here is some starter code that could be used to begin modifying `run_eleuther`:\n",
    "\n",
    "   ```\n",
    "    prompt_logits = eleuther_model(prompt_ids).logits                \n",
    "    prompt_probs = prompt_logits.softmax(-1)                                   \n",
    "    prompt_probs = torch.gather(prompt_probs, 2, prompt_ids[:, :, None]).squeeze(-1)\n",
    "    prompt_probs = [list(prompt_prob.numpy()) for p in prompt_probs]\n",
    "   ```\n",
    "\n",
    "__Original system instructions__:\n",
    "\n",
    "In the cell below, please provide a brief technical description of your original system, so that the teaching team can gain an understanding of what it does. This will help us to understand your code and analyze all the submissions to identify patterns and strategies. \n",
    "\n",
    "We also ask that you report the best macro F1 score your system got during development on `dev_exs` [as defined above](#SQuAD-dev-sample), just to help us understand how systems performed overall.\n",
    "\n",
    "Please review the descriptions in the following comment and follow the instructions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline System (ColBERT straight output + Eleuther)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ColBERT + Answer Scoring + Eleuther"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ColBERT Improvment One + Eleuther"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D9pfDIrPYre4"
   },
   "outputs": [],
   "source": [
    "######## This part is functional modules ############\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "#### enhanced squad training example searching\n",
    "\n",
    "def train_tf_idf(bioasq_train):\n",
    "    tfidfvectorizer = TfidfVectorizer(analyzer='word',stop_words= 'english', ngram_range=(1, 3))\n",
    "\n",
    "  # append all context\n",
    "    train_context = [x.context for x in bioasq_train]\n",
    "\n",
    "    tfidfvectorizer.fit_transform(train_context)\n",
    "\n",
    "    context_tfidf = tfidfvectorizer.transform(train_context)\n",
    "\n",
    "    return tfidfvectorizer, context_tfidf\n",
    "\n",
    "def sample_bioasq_train(tfidfvectorizer, context_tfidf, question, n_context):\n",
    "    '''\n",
    "    This is using tf-idf and consine similarity to sample \"related to question\" bioasq example to build the prompt\n",
    "    '''\n",
    "    question_tfidf = tfidfvectorizer.transform([question])\n",
    "\n",
    "    cosine_sim = cosine_similarity(context_tfidf, question_tfidf).flatten()\n",
    "\n",
    "    related_index = cosine_sim.argsort()[-n_context:][::-1]\n",
    "\n",
    "    train_exs = [bioasq_train[i] for i in related_index]\n",
    "\n",
    "    return train_exs\n",
    "\n",
    "### revised answer scoring by normalizing the score by length\n",
    "from types import GeneratorType\n",
    "## added temperature arg to allow change\n",
    "def answer_scoring_normalized(passages, passage_probs, prompts, temperature, gen_func=run_eleuther):\n",
    "    \"\"\"Implements our basic scoring strategy.\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  passages : list of str\n",
    "  passage_probs : list of float\n",
    "  prompts : list of str\n",
    "  gen_func : either `run_eleuther` or `run_gpt3`\n",
    "\n",
    "  Returns\n",
    "  -------\n",
    "  list of pairs (score, dict), sorted with the largest score first.\n",
    "  `dict` should be the return value of `gen_func` for an example.\n",
    "\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    length_sum = 0\n",
    "    gen_list = []\n",
    "\n",
    "    for passage, passage_prob, prompt in zip(passages, passage_probs, prompts):\n",
    "        gen = gen_func([prompt], temperature = temperature)\n",
    "\n",
    "        gen_list.append(gen)\n",
    "        # calculate the total length of answers\n",
    "        length_sum += len(gen[0]['generated_answer'].split(' '))\n",
    "\n",
    "    for passage_prob, gen in zip(passage_probs, gen_list):\n",
    "\n",
    "        answer_score = np.prod(gen[0]['generated_answer_probs'])\n",
    "\n",
    "        length_of_answer = len(gen[0]['generated_answer'].split(' '))\n",
    "\n",
    "        # give more weight to longer answers, as its product of per-token probabiliyy is underdog\n",
    "        weight = length_of_answer/length_sum\n",
    "\n",
    "        final_score = passage_prob*answer_score*weight\n",
    "\n",
    "        data.append((final_score, gen[0]))\n",
    "\n",
    "    data.sort(key = lambda x: x[0], reverse=True)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "######## This part is system development ############\n",
    "\n",
    "batch_size = 5\n",
    "joiner = '\\n\\n'\n",
    "n_context = 2\n",
    "\n",
    "# temperatures = [0.01, 0.025, 0.05, 0.075]\n",
    "temperatures = [0.025]\n",
    "# bioasq_dev\n",
    "# bioasq_train\n",
    "\n",
    "for temperature in temperatures:\n",
    "    prompts = []\n",
    "\n",
    "    gens = []\n",
    "\n",
    "    # use tf-idf to find \"related few shot in bioasq to build the prompt\n",
    "    # train tf-idf on all bioasq examples\n",
    "    tfidfvectorizer, context_tfidf = train_tf_idf(bioasq_train)\n",
    "\n",
    "    for i in range(0, len(bioasq_test), batch_size):\n",
    "        # Use the `searcher` defined above to get passages\n",
    "        # using `ex.question` as the query, and use your\n",
    "        # `build_few_shot_open_qa_prompt` to build prompts.\n",
    "\n",
    "        # get a batch from bioasq dev (to replace dev_exs)\n",
    "        batch = bioasq_test[i: i+batch_size]\n",
    "\n",
    "        # train_exs = random.sample(bioasq_train, k=n_context)\n",
    "\n",
    "        ## score for answer-passage pair\n",
    "        for ex in batch:\n",
    "\n",
    "          # use tf idf to sample training exs, instead of just random sampling bioasq training\n",
    "            train_exs = sample_bioasq_train(tfidfvectorizer, context_tfidf, ex.question, n_context)\n",
    "\n",
    "            passages, passage_probs = get_passages_with_scores(ex.question)\n",
    "\n",
    "            # re-initiating prompt\n",
    "            ps = []\n",
    "            # iterate through each passage in the top k (5) passages\n",
    "            for psg in passages:\n",
    "            # build the prompt based on question, that specific passge, and training examples\n",
    "            # say we have passage, then ps will be ['prompt1', 'prompt2', 'prompt3', 'prompt4', 'prompt5']\n",
    "                ps.append(build_few_shot_open_qa_prompt(ex.question, psg, train_exs, joiner=joiner)) \n",
    "\n",
    "          # calculate the answering score for the highest passage-answer pair                 \n",
    "          # data = answer_scoring(passages,       # only related to question, same length as ps\n",
    "          #                       passage_probs,  # only related to question, same length as ps\n",
    "          #                       ps,             # k prompts\n",
    "          #                       run_eleuther)\n",
    "\n",
    "            data = answer_scoring_normalized(passages,       # only related to question, same length as ps\n",
    "                                passage_probs,  # only related to question, same length as ps\n",
    "                                ps,             # k prompts\n",
    "                                temperature,\n",
    "                                run_eleuther)\n",
    "\n",
    "            # pick highest score answer-prompt pair (note: in)\n",
    "            highest_gs = [data[0][1]]\n",
    "            highest_ps = [data[0][1]['prompt']]\n",
    "\n",
    "            # add the prompt to prompt list\n",
    "            prompts += highest_ps\n",
    "\n",
    "            # add generated txt to gen list\n",
    "            gens += highest_gs\n",
    " \n",
    "    eva = evaluate(bioasq_test, prompts, gens)\n",
    "    print(f\"temperature {temperature} get {eva['macro_f1']}\")\n",
    "\n",
    "\n",
    "# ######## This part is final function wrapper for bakeoff ############\n",
    "\n",
    "# ## train tf-idf model first\n",
    "# tfidfvectorizer, context_tfidf = train_tf_idf(bioasq_train)\n",
    "\n",
    "# # make the system feed on one single question\n",
    "# def run_original_system(question, tfidfvectorizer=tfidfvectorizer, context_tfidf=context_tfidf):\n",
    "# '''\n",
    "# Wrapper for producing the bakeoff dictionary\n",
    "\n",
    "# args:\n",
    "#   questions: single question\n",
    "# '''\n",
    "# joiner = '\\n\\n'\n",
    "# n_context = 2\n",
    "\n",
    "# temperature = 0.025 # obtained from hyperparameter searching\n",
    "\n",
    "# gens = {}\n",
    "\n",
    "# # score for answer-passage pair\n",
    "# # use tf idf to sample training exs, instead of just random sampling bioasq training\n",
    "# train_exs = sample_bioasq_train(tfidfvectorizer, context_tfidf, question, n_context)\n",
    "\n",
    "# passages, passage_probs = get_passages_with_scores(question)\n",
    "\n",
    "# # re-initiating prompt\n",
    "# ps = []\n",
    "\n",
    "# # iterate through each passage in the top k (5) passages\n",
    "# for psg in passages:\n",
    "#   # build the prompt based on question, that specific passge, and training examples\n",
    "#   # say we have passage, then ps will be ['prompt1', 'prompt2', 'prompt3', 'prompt4', 'prompt5']\n",
    "#   ps.append(build_few_shot_open_qa_prompt(question, psg, train_exs, joiner=joiner)) \n",
    "\n",
    "# #answer scoring\n",
    "# data = answer_scoring_normalized(passages,       # only related to question, same length as ps\n",
    "#                             passage_probs,  # only related to question, same length as ps\n",
    "#                             ps,             # k prompts\n",
    "#                             temperature,\n",
    "#                             run_gpt3)\n",
    "\n",
    "\n",
    "# # pick highest score answer-prompt pair (note: in)\n",
    "# highest_gs = [data[0][1]]\n",
    "\n",
    "# return highest_gs\n",
    "\n",
    "\n",
    "# STOP COMMENT: Please do not remove this comment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SDX9uXGfzTis"
   },
   "source": [
    "If the above fails, you can just download https://web.stanford.edu/class/cs224u/data/cs224u-openqa-test-unlabeled.txt and place it in `data/openqa`.\n",
    "\n",
    "This file contains only questions. The starter code below will help you structure this. It writes a file \"cs224u-openqa-bakeoff-entry.json\" to the current directory. That file should be uploaded as-is. Please do not change its name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nWqnZmNeYG6S"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "dKB9zXRBYrel",
    "-mp1C-oyYreq",
    "Fca8-RXjYres",
    "XGK3hCs9Yrev",
    "DZpXMk-0Yrew",
    "NFYxJPpuYre0",
    "w9n0_xwdYre1",
    "teKobQM8Yre1",
    "R55lgmnTzTip",
    "w2mx3Z4HYre2"
   ],
   "machine_shape": "hm",
   "name": "hw_openqa.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "a99ac6d2deb03d0b7ced3594556c328848678d7cea021ae1b9990e15d3ad5c49"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "19838b224425453988f4ea47c50c048e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "29084cffd32b495ba5c29910a483b41a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_cb331d5edf364e19af8f45fd3726a9f8",
       "IPY_MODEL_dcb183bf0a85498d8bb2bd360c64255b",
       "IPY_MODEL_83ab942f9ce14cd6b82da28b36dd1377"
      ],
      "layout": "IPY_MODEL_19838b224425453988f4ea47c50c048e"
     }
    },
    "5054cd44686b4f649724445b581ad979": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "76b125c6b6cf426cbf3ad638e443cbd8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "83ab942f9ce14cd6b82da28b36dd1377": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_eda083d3daeb4c59b66beb142f12063a",
      "placeholder": "​",
      "style": "IPY_MODEL_adaf48a176ad439f89883761b3027232",
      "value": " 2/2 [00:00&lt;00:00,  7.29it/s]"
     }
    },
    "adaf48a176ad439f89883761b3027232": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "bdcd068db1ad4c55ba282295b91aaeec": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cb331d5edf364e19af8f45fd3726a9f8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_bdcd068db1ad4c55ba282295b91aaeec",
      "placeholder": "​",
      "style": "IPY_MODEL_f61e1dd551d74455b834735b4aa816df",
      "value": "100%"
     }
    },
    "dcb183bf0a85498d8bb2bd360c64255b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5054cd44686b4f649724445b581ad979",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_76b125c6b6cf426cbf3ad638e443cbd8",
      "value": 2
     }
    },
    "eda083d3daeb4c59b66beb142f12063a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f61e1dd551d74455b834735b4aa816df": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
